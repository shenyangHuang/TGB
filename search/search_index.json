{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Temporal Graph Benchmark","text":""},{"location":"#pip-install","title":"Pip Install","text":"<p>You can install TGB via pip <pre><code>pip install py-tgb\n</code></pre></p>"},{"location":"#links-and-datasets","title":"Links and Datasets","text":"<p>The project website can be found here.</p> <p>The API documentations can be found here.</p> <p>all dataset download links can be found at info.py</p> <p>TGB dataloader will also automatically download the dataset as well as the negative samples for the link property prediction datasets.</p>"},{"location":"#install-dependency","title":"Install dependency","text":"<p>Our implementation works with python &gt;= 3.9 and can be installed as follows</p> <ol> <li> <p>set up virtual environment (conda should work as well) <pre><code>python -m venv ~/tgb_env/\nsource ~/tgb_env/bin/activate\n</code></pre></p> </li> <li> <p>install external packages <pre><code>pip install pandas==1.5.3\npip install matplotlib==3.7.1\npip install clint==0.5.1\n</code></pre></p> </li> </ol> <p>install Pytorch and PyG dependencies (needed to run the examples) <pre><code>pip install torch==2.0.0 --index-url https://download.pytorch.org/whl/cu117\npip install torch_geometric==2.3.0\npip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cu117.html\n</code></pre></p> <ol> <li>install local dependencies under root directory <code>/TGB</code> <pre><code>pip install -e .\n</code></pre></li> </ol>"},{"location":"#instruction-for-tracking-new-documentation-and-running-mkdocs-locally","title":"Instruction for tracking new documentation and running mkdocs locally","text":"<ol> <li> <p>first run the mkdocs server locally in your terminal  <pre><code>mkdocs serve\n</code></pre></p> </li> <li> <p>go to the local hosted web address similar to <pre><code>[14:18:13] Browser connected: http://127.0.0.1:8000/\n</code></pre></p> </li> </ol> <p>Example: to track documentation of a new hi.py file in tgb/edgeregression/hi.py</p> <ol> <li> <p>create docs/api/tgb.hi.md and add the following <pre><code># `tgb.edgeregression`\n\n::: tgb.edgeregression.hi\n</code></pre></p> </li> <li> <p>edit mkdocs.yml  <pre><code>nav:\n  - Overview: index.md\n  - About: about.md\n  - API:\n    other *.md files \n    - tgb.edgeregression: api/tgb.hi.md\n</code></pre></p> </li> </ol>"},{"location":"#creating-new-branch","title":"Creating new branch","text":"<pre><code>git fetch origin\n\ngit checkout -b test origin/test\n</code></pre>"},{"location":"#dependencies-for-mkdocs-documentation","title":"dependencies for mkdocs (documentation)","text":"<pre><code>pip install mkdocs\npip install mkdocs-material\npip install mkdocstrings-python\npip install mkdocs-jupyter\npip install notebook\n</code></pre>"},{"location":"#full-dependency-list","title":"full dependency list","text":"<p>Our implementation works with python &gt;= 3.9 and has the following dependencies <pre><code>pytorch == 2.0.0\ntorch-geometric == 2.3.0\ntorch-scatter==2.1.1\ntorch-sparse==0.6.17\ntorch-spline-conv==1.2.2\npandas==1.5.3\nclint==0.5.1\n</code></pre></p>"},{"location":"about/","title":"Temporal Graph Benchmark (TGB)","text":""},{"location":"about/#overview","title":"Overview","text":"<p>The TGB repo provides an automated ML pipeline for learning on a diverse set of temporal graph datasets:</p> <ul> <li> <p>automatic download of datasets from url</p> </li> <li> <p>processing the raw files into ML ready format</p> </li> <li> <p>support datasets in <code>numpy</code>, <code>Pytorch</code> and <code>PyG TemporalData</code> formats</p> </li> <li> <p>evaluation code for each dataset </p> </li> </ul>"},{"location":"api/tgb.linkproppred/","title":"<code>tgb.linkproppred</code>","text":"<p>Evaluator Module for Dynamic Link Prediction</p> <p>Sample negative edges for evaluation of dynamic link prediction Load already generated negative edges from file, batch them based on the positive edge, and return the evaluation set</p> <p>Sample and Generate negative edges that are going to be used for evaluation of a dynamic graph learning model Negative samples are generated and saved to files ONLY once;      other times, they should be loaded from file with instances of the <code>negative_sampler.py</code>.</p> <p>Sample and Generate negative edges that are going to be used for evaluation of a dynamic graph learning model Negative samples are generated and saved to files ONLY once;      other times, they should be loaded from file with instances of the <code>negative_sampler.py</code>.</p> <p>Sample negative edges for evaluation of dynamic link prediction Load already generated negative edges from file, batch them based on the positive edge, and return the evaluation set</p> <p>Sample and Generate negative edges that are going to be used for evaluation of a dynamic graph learning model Negative samples are generated and saved to files ONLY once;      other times, they should be loaded from file with instances of the <code>negative_sampler.py</code>.</p> <p>Sample negative edges for evaluation of dynamic link prediction Load already generated negative edges from file, batch them based on the positive edge, and return the evaluation set</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset","title":"<code>LinkPropPredDataset</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>tgb/linkproppred/dataset.py</code> <pre><code>class LinkPropPredDataset(object):\n    def __init__(\n        self,\n        name: str,\n        root: Optional[str] = \"datasets\",\n        meta_dict: Optional[dict] = None,\n        preprocess: Optional[bool] = True,\n    ):\n        r\"\"\"Dataset class for link prediction dataset. Stores meta information about each dataset such as evaluation metrics etc.\n        also automatically pre-processes the dataset.\n        Args:\n            name: name of the dataset\n            root: root directory to store the dataset folder\n            meta_dict: dictionary containing meta information about the dataset, should contain key 'dir_name' which is the name of the dataset folder\n            preprocess: whether to pre-process the dataset\n        \"\"\"\n        self.name = name  ## original name\n        # check if dataset url exist\n        if self.name in DATA_URL_DICT:\n            self.url = DATA_URL_DICT[self.name]\n        else:\n            self.url = None\n            print(f\"Dataset {self.name} url not found, download not supported yet.\")\n\n\n        # check if the evaluatioin metric are specified\n        if self.name in DATA_EVAL_METRIC_DICT:\n            self.metric = DATA_EVAL_METRIC_DICT[self.name]\n        else:\n            self.metric = None\n            print(\n                f\"Dataset {self.name} default evaluation metric not found, it is not supported yet.\"\n            )\n\n\n        root = PROJ_DIR + root\n\n        if meta_dict is None:\n            self.dir_name = \"_\".join(name.split(\"-\"))  ## replace hyphen with underline\n            meta_dict = {\"dir_name\": self.dir_name}\n        else:\n            self.dir_name = meta_dict[\"dir_name\"]\n        self.root = osp.join(root, self.dir_name)\n        self.meta_dict = meta_dict\n        if \"fname\" not in self.meta_dict:\n            self.meta_dict[\"fname\"] = self.root + \"/\" + self.name + \"_edgelist.csv\"\n            self.meta_dict[\"nodefile\"] = None\n\n        if name == \"tgbl-flight\":\n            self.meta_dict[\"nodefile\"] = self.root + \"/\" + \"airport_node_feat.csv\"\n\n        if name == \"tkgl-wikidata\" or name == \"tkgl-smallpedia\":\n            self.meta_dict[\"staticfile\"] = self.root + \"/\" + self.name + \"_static_edgelist.csv\"\n\n        if \"thg\" in name:\n            self.meta_dict[\"nodeTypeFile\"] = self.root + \"/\" + self.name + \"_nodetype.csv\"\n        else:\n            self.meta_dict[\"nodeTypeFile\"] = None\n\n        self.meta_dict[\"val_ns\"] = self.root + \"/\" + self.name + \"_val_ns.pkl\"\n        self.meta_dict[\"test_ns\"] = self.root + \"/\" + self.name + \"_test_ns.pkl\"\n\n        #! version check\n        self.version_passed = True\n        self._version_check()\n\n        # initialize\n        self._node_feat = None\n        self._edge_feat = None\n        self._full_data = None\n        self._train_data = None\n        self._val_data = None\n        self._test_data = None\n\n        # for tkg and thg\n        self._edge_type = None\n\n        #tkgl-wikidata and tkgl-smallpedia only\n        self._static_data = None\n\n        # for thg only\n        self._node_type = None\n        self._node_id = None\n\n        self.download()\n        # check if the root directory exists, if not create it\n        if osp.isdir(self.root):\n            print(\"Dataset directory is \", self.root)\n        else:\n            # os.makedirs(self.root)\n            raise FileNotFoundError(f\"Directory not found at {self.root}\")\n\n        if preprocess:\n            self.pre_process()\n\n        self.min_dst_idx, self.max_dst_idx = int(self._full_data[\"destinations\"].min()), int(self._full_data[\"destinations\"].max())\n\n        if ('tkg' in self.name):\n            if self.name in DATA_NS_STRATEGY_DICT:\n                self.ns_sampler = TKGNegativeEdgeSampler(\n                    dataset_name=self.name,\n                    first_dst_id=self.min_dst_idx,\n                    last_dst_id=self.max_dst_idx,\n                    strategy=DATA_NS_STRATEGY_DICT[self.name],\n                    partial_path=self.root + \"/\" + self.name,\n                )\n            else:\n                raise ValueError(f\"Dataset {self.name} negative sampling strategy not found.\")\n        elif ('thg' in self.name):\n            #* need to find the smallest node id of all nodes (regardless of types)\n\n            min_node_idx = min(int(self._full_data[\"sources\"].min()), int(self._full_data[\"destinations\"].min()))\n            max_node_idx = max(int(self._full_data[\"sources\"].max()), int(self._full_data[\"destinations\"].max()))\n            self.ns_sampler = THGNegativeEdgeSampler(\n                dataset_name=self.name,\n                first_node_id=min_node_idx,\n                last_node_id=max_node_idx,\n                node_type=self._node_type,\n            )\n        else:\n            self.ns_sampler = NegativeEdgeSampler(\n                dataset_name=self.name,\n                first_dst_id=self.min_dst_idx,\n                last_dst_id=self.max_dst_idx,\n            )\n\n\n    def _version_check(self) -&gt; None:\n        r\"\"\"Implement Version checks for dataset files\n        updates the file names based on the current version number\n        prompt the user to download the new version via self.version_passed variable\n        \"\"\"\n        if (self.name in DATA_VERSION_DICT):\n            version = DATA_VERSION_DICT[self.name]\n        else:\n            print(f\"Dataset {self.name} version number not found.\")\n            self.version_passed = False\n            return None\n\n        if (version &gt; 1):\n            #* check if current version is outdated\n            self.meta_dict[\"fname\"] = self.root + \"/\" + self.name + \"_edgelist_v\" + str(int(version)) + \".csv\"\n            self.meta_dict[\"nodefile\"] = None\n            if self.name == \"tgbl-flight\":\n                self.meta_dict[\"nodefile\"] = self.root + \"/\" + \"airport_node_feat_v\" + str(int(version)) + \".csv\"\n            self.meta_dict[\"val_ns\"] = self.root + \"/\" + self.name + \"_val_ns_v\" + str(int(version)) + \".pkl\"\n            self.meta_dict[\"test_ns\"] = self.root + \"/\" + self.name + \"_test_ns_v\" + str(int(version)) + \".pkl\"\n\n            if (not osp.exists(self.meta_dict[\"fname\"])):\n                print(f\"Dataset {self.name} version {int(version)} not found.\")\n                print(f\"Please download the latest version of the dataset.\")\n                self.version_passed = False\n                return None\n\n\n    def download(self):\n        \"\"\"\n        downloads this dataset from url\n        check if files are already downloaded\n        \"\"\"\n        # check if the file already exists\n        if osp.exists(self.meta_dict[\"fname\"]):\n            print(\"raw file found, skipping download\")\n            return\n\n        inp = input(\n            \"Will you download the dataset(s) now? (y/N)\\n\"\n        ).lower()  # ask if the user wants to download the dataset\n\n        if inp == \"y\":\n            print(\n                f\"{BColors.WARNING}Download started, this might take a while . . . {BColors.ENDC}\"\n            )\n            print(f\"Dataset title: {self.name}\")\n\n            if self.url is None:\n                raise Exception(\"Dataset url not found, download not supported yet.\")\n            else:\n                r = requests.get(self.url, stream=True)\n                # download_dir = self.root + \"/\" + \"download\"\n                if osp.isdir(self.root):\n                    print(\"Dataset directory is \", self.root)\n                else:\n                    os.makedirs(self.root)\n\n                path_download = self.root + \"/\" + self.name + \".zip\"\n                with open(path_download, \"wb\") as f:\n                    total_length = int(r.headers.get(\"content-length\"))\n                    for chunk in progress.bar(\n                        r.iter_content(chunk_size=1024),\n                        expected_size=(total_length / 1024) + 1,\n                    ):\n                        if chunk:\n                            f.write(chunk)\n                            f.flush()\n                # for unzipping the file\n                with zipfile.ZipFile(path_download, \"r\") as zip_ref:\n                    zip_ref.extractall(self.root)\n                print(f\"{BColors.OKGREEN}Download completed {BColors.ENDC}\")\n                self.version_passed = True\n        else:\n            raise Exception(\n                BColors.FAIL + \"Data not found error, download \" + self.name + \" failed\"\n            )\n\n    def generate_processed_files(self) -&gt; pd.DataFrame:\n        r\"\"\"\n        turns raw data .csv file into a pandas data frame, stored on disc if not already\n        Returns:\n            df: pandas data frame\n        \"\"\"\n        node_feat = None\n        if not osp.exists(self.meta_dict[\"fname\"]):\n            raise FileNotFoundError(f\"File not found at {self.meta_dict['fname']}\")\n\n        if self.meta_dict[\"nodefile\"] is not None:\n            if not osp.exists(self.meta_dict[\"nodefile\"]):\n                raise FileNotFoundError(\n                    f\"File not found at {self.meta_dict['nodefile']}\"\n                )\n        #* for thg must have nodetypes \n        if self.meta_dict[\"nodeTypeFile\"] is not None:\n            if not osp.exists(self.meta_dict[\"nodeTypeFile\"]):\n                raise FileNotFoundError(\n                    f\"File not found at {self.meta_dict['nodeTypeFile']}\"\n                )\n\n\n        OUT_DF = self.root + \"/\" + \"ml_{}.pkl\".format(self.name)\n        OUT_EDGE_FEAT = self.root + \"/\" + \"ml_{}.pkl\".format(self.name + \"_edge\")\n        OUT_NODE_ID = self.root + \"/\" + \"ml_{}.pkl\".format(self.name + \"_nodeid\")\n        if self.meta_dict[\"nodefile\"] is not None:\n            OUT_NODE_FEAT = self.root + \"/\" + \"ml_{}.pkl\".format(self.name + \"_node\")\n        if self.meta_dict[\"nodeTypeFile\"] is not None:\n            OUT_NODE_TYPE = self.root + \"/\" + \"ml_{}.pkl\".format(self.name + \"_nodeType\")\n\n        if (osp.exists(OUT_DF)) and (self.version_passed is True):\n            print(\"loading processed file\")\n            df = pd.read_pickle(OUT_DF)\n            edge_feat = load_pkl(OUT_EDGE_FEAT)\n            if (self.name == \"tkgl-wikidata\") or (self.name == \"tkgl-smallpedia\"):\n                node_id = load_pkl(OUT_NODE_ID)\n                self._node_id = node_id\n            if self.meta_dict[\"nodefile\"] is not None:\n                node_feat = load_pkl(OUT_NODE_FEAT)\n            if self.meta_dict[\"nodeTypeFile\"] is not None:\n                node_type = load_pkl(OUT_NODE_TYPE)\n                self._node_type = node_type\n\n        else:\n            print(\"file not processed, generating processed file\")\n            if self.name == \"tgbl-flight\":\n                df, edge_feat, node_ids = csv_to_pd_data(self.meta_dict[\"fname\"])\n            elif self.name == \"tgbl-coin\":\n                df, edge_feat, node_ids = csv_to_pd_data_sc(self.meta_dict[\"fname\"])\n            elif self.name == \"tgbl-comment\":\n                df, edge_feat, node_ids = csv_to_pd_data_rc(self.meta_dict[\"fname\"])\n            elif self.name == \"tgbl-review\":\n                df, edge_feat, node_ids = csv_to_pd_data_sc(self.meta_dict[\"fname\"])\n            elif self.name == \"tgbl-wiki\":\n                df, edge_feat, node_ids = load_edgelist_wiki(self.meta_dict[\"fname\"])\n            elif self.name == \"tgbl-subreddit\":\n                df, edge_feat, node_ids = load_edgelist_wiki(self.meta_dict[\"fname\"])\n            elif self.name == \"tgbl-lastfm\":\n                df, edge_feat, node_ids = load_edgelist_wiki(self.meta_dict[\"fname\"])\n            elif self.name == \"tkgl-polecat\":\n                df, edge_feat, node_ids = csv_to_tkg_data(self.meta_dict[\"fname\"])\n            elif self.name == \"tkgl-icews\":\n                df, edge_feat, node_ids = csv_to_tkg_data(self.meta_dict[\"fname\"])\n            elif self.name == \"tkgl-yago\":\n                df, edge_feat, node_ids = csv_to_tkg_data(self.meta_dict[\"fname\"])\n            elif self.name == \"tkgl-wikidata\":\n                df, edge_feat, node_ids = csv_to_wikidata(self.meta_dict[\"fname\"])\n                save_pkl(node_ids, OUT_NODE_ID)\n                self._node_id = node_ids\n            elif self.name == \"tkgl-smallpedia\":\n                df, edge_feat, node_ids = csv_to_wikidata(self.meta_dict[\"fname\"])\n                save_pkl(node_ids, OUT_NODE_ID)\n                self._node_id = node_ids\n            elif self.name == \"thgl-myket\":\n                df, edge_feat, node_ids = csv_to_thg_data(self.meta_dict[\"fname\"])\n            elif self.name == \"thgl-github\":\n                df, edge_feat, node_ids = csv_to_thg_data(self.meta_dict[\"fname\"])\n            elif self.name == \"thgl-forum\":\n                df, edge_feat, node_ids = csv_to_forum_data(self.meta_dict[\"fname\"])\n            elif self.name == \"thgl-software\":\n                df, edge_feat, node_ids = csv_to_thg_data(self.meta_dict[\"fname\"])\n            else:\n                raise ValueError(f\"Dataset {self.name} not found.\")\n\n            save_pkl(edge_feat, OUT_EDGE_FEAT)\n            df.to_pickle(OUT_DF)\n            if self.meta_dict[\"nodefile\"] is not None:\n                node_feat = process_node_feat(self.meta_dict[\"nodefile\"], node_ids)\n                save_pkl(node_feat, OUT_NODE_FEAT)\n            if self.meta_dict[\"nodeTypeFile\"] is not None:\n                node_type = process_node_type(self.meta_dict[\"nodeTypeFile\"], node_ids)\n                save_pkl(node_type, OUT_NODE_TYPE)\n                #? do not return node_type, simply set it\n                self._node_type = node_type\n\n\n        return df, edge_feat, node_feat\n\n    def pre_process(self):\n        \"\"\"\n        Pre-process the dataset and generates the splits, must be run before dataset properties can be accessed\n        generates the edge data and different train, val, test splits\n        \"\"\"\n\n        # check if path to file is valid\n        df, edge_feat, node_feat = self.generate_processed_files()\n\n        #* design choice, only stores the original edges not the inverse relations on disc\n        if (\"tkgl\" in self.name):\n            df = add_inverse_quadruples(df)\n\n        sources = np.array(df[\"u\"])\n        destinations = np.array(df[\"i\"])\n        timestamps = np.array(df[\"ts\"])\n        edge_idxs = np.array(df[\"idx\"])\n        weights = np.array(df[\"w\"])\n        edge_label = np.ones(len(df))  # should be 1 for all pos edges\n        self._edge_feat = edge_feat\n        self._node_feat = node_feat\n\n        full_data = {\n            \"sources\": sources.astype(int),\n            \"destinations\": destinations.astype(int),\n            \"timestamps\": timestamps.astype(int),\n            \"edge_idxs\": edge_idxs,\n            \"edge_feat\": edge_feat,\n            \"w\": weights,\n            \"edge_label\": edge_label,\n        }\n\n        #* for tkg and thg\n        if (\"edge_type\" in df):\n            edge_type = np.array(df[\"edge_type\"]).astype(int)\n            self._edge_type = edge_type\n            full_data[\"edge_type\"] = edge_type\n\n        self._full_data = full_data\n\n        if (\"yago\" in self.name):\n            _train_mask, _val_mask, _test_mask = self.generate_splits(full_data, val_ratio=0.1, test_ratio=0.10) #99) #val_ratio=0.097, test_ratio=0.099)\n        else:\n            _train_mask, _val_mask, _test_mask = self.generate_splits(full_data, val_ratio=0.15, test_ratio=0.15)\n        self._train_mask = _train_mask\n        self._val_mask = _val_mask\n        self._test_mask = _test_mask\n\n    def generate_splits(\n        self,\n        full_data: Dict[str, Any],\n        val_ratio: float = 0.15,\n        test_ratio: float = 0.15,\n    ) -&gt; Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\n        r\"\"\"Generates train, validation, and test splits from the full dataset\n        Args:\n            full_data: dictionary containing the full dataset\n            val_ratio: ratio of validation data\n            test_ratio: ratio of test data\n        Returns:\n            train_data: dictionary containing the training dataset\n            val_data: dictionary containing the validation dataset\n            test_data: dictionary containing the test dataset\n        \"\"\"\n        val_time, test_time = list(\n            np.quantile(\n                full_data[\"timestamps\"],\n                [(1 - val_ratio - test_ratio), (1 - test_ratio)],\n            )\n        )\n        timestamps = full_data[\"timestamps\"]\n\n        train_mask = timestamps &lt;= val_time\n        val_mask = np.logical_and(timestamps &lt;= test_time, timestamps &gt; val_time)\n        test_mask = timestamps &gt; test_time\n\n        return train_mask, val_mask, test_mask\n\n    def preprocess_static_edges(self):\n        \"\"\"\n        Pre-process the static edges of the dataset\n        \"\"\"\n        if (\"staticfile\" in self.meta_dict):\n            OUT_DF = self.root + \"/\" + \"ml_{}.pkl\".format(self.name + \"_static\")\n            if (osp.exists(OUT_DF)) and (self.version_passed is True):\n                print(\"loading processed file\")\n                static_dict = load_pkl(OUT_DF)\n                self._static_data = static_dict\n            else:\n                print(\"file not processed, generating processed file\")\n                static_dict, node_ids =  csv_to_staticdata(self.meta_dict[\"staticfile\"], self._node_id)\n                save_pkl(static_dict, OUT_DF)\n                self._static_data = static_dict\n        else:\n            print (\"static edges are only for tkgl-wikidata and tkgl-smallpedia datasets\")\n\n\n    @property\n    def eval_metric(self) -&gt; str:\n        \"\"\"\n        the official evaluation metric for the dataset, loaded from info.py\n        Returns:\n            eval_metric: str, the evaluation metric\n        \"\"\"\n        return self.metric\n\n    @property\n    def negative_sampler(self) -&gt; NegativeEdgeSampler:\n        r\"\"\"\n        Returns the negative sampler of the dataset, will load negative samples from disc\n        Returns:\n            negative_sampler: NegativeEdgeSampler\n        \"\"\"\n        return self.ns_sampler\n\n\n    def load_val_ns(self) -&gt; None:\n        r\"\"\"\n        load the negative samples for the validation set\n        \"\"\"\n        self.ns_sampler.load_eval_set(\n            fname=self.meta_dict[\"val_ns\"], split_mode=\"val\"\n        )\n\n    def load_test_ns(self) -&gt; None:\n        r\"\"\"\n        load the negative samples for the test set\n        \"\"\"\n        self.ns_sampler.load_eval_set(\n            fname=self.meta_dict[\"test_ns\"], split_mode=\"test\"\n        )\n\n    @property\n    def num_nodes(self) -&gt; int:\n        r\"\"\"\n        Returns the total number of unique nodes in the dataset \n        Returns:\n            num_nodes: int, the number of unique nodes\n        \"\"\"\n        src = self._full_data[\"sources\"]\n        dst = self._full_data[\"destinations\"]\n        all_nodes = np.concatenate((src, dst), axis=0)\n        uniq_nodes = np.unique(all_nodes, axis=0)\n        return uniq_nodes.shape[0]\n\n\n    @property\n    def num_edges(self) -&gt; int:\n        r\"\"\"\n        Returns the total number of edges in the dataset\n        Returns:\n            num_edges: int, the number of edges\n        \"\"\"\n        src = self._full_data[\"sources\"]\n        return src.shape[0]\n\n\n    @property\n    def num_rels(self) -&gt; int:\n        r\"\"\"\n        Returns the number of relation types in the dataset\n        Returns:\n            num_rels: int, the number of relation types\n        \"\"\"\n        #* if it is a homogenous graph\n        if (\"edge_type\" not in self._full_data):\n            return 1\n        else:\n            return np.unique(self._full_data[\"edge_type\"]).shape[0]\n\n    @property\n    def node_feat(self) -&gt; Optional[np.ndarray]:\n        r\"\"\"\n        Returns the node features of the dataset with dim [N, feat_dim]\n        Returns:\n            node_feat: np.ndarray, [N, feat_dim] or None if there is no node feature\n        \"\"\"\n        return self._node_feat\n\n    @property\n    def node_type(self) -&gt; Optional[np.ndarray]:\n        r\"\"\"\n        Returns the node types of the dataset with dim [N], only for temporal heterogeneous graphs\n        Returns:\n            node_feat: np.ndarray, [N] or None if there is no node feature\n        \"\"\"\n        return self._node_type\n\n    @property\n    def edge_feat(self) -&gt; Optional[np.ndarray]:\n        r\"\"\"\n        Returns the edge features of the dataset with dim [E, feat_dim]\n        Returns:\n            edge_feat: np.ndarray, [E, feat_dim] or None if there is no edge feature\n        \"\"\"\n        return self._edge_feat\n\n    @property\n    def edge_type(self) -&gt; Optional[np.ndarray]:\n        r\"\"\"\n        Returns the edge types of the dataset with dim [E, 1], only for temporal knowledge graph and temporal heterogeneous graph\n        Returns:\n            edge_type: np.ndarray, [E, 1] or None if it is not a TKG or THG\n        \"\"\"\n        return self._edge_type\n\n    @property\n    def static_data(self) -&gt; Optional[np.ndarray]:\n        r\"\"\"\n        Returns the static edges related to this dataset, applies for tkgl-wikidata and tkgl-smallpedia, edges are (src, dst, rel_type)\n        Returns:\n            df: pd.DataFrame {\"head\": np.ndarray, \"tail\": np.ndarray, \"rel_type\": np.ndarray}\n        \"\"\"\n        if (self.name == \"tkgl-wikidata\") or (self.name == \"tkgl-smallpedia\"):\n            self.preprocess_static_edges()\n        return self._static_data\n\n    @property\n    def full_data(self) -&gt; Dict[str, Any]:\n        r\"\"\"\n        the full data of the dataset as a dictionary with keys: 'sources', 'destinations', 'timestamps', 'edge_idxs', 'edge_feat', 'w', 'edge_label',\n\n        Returns:\n            full_data: Dict[str, Any]\n        \"\"\"\n        if self._full_data is None:\n            raise ValueError(\n                \"dataset has not been processed yet, please call pre_process() first\"\n            )\n        return self._full_data\n\n    @property\n    def train_mask(self) -&gt; np.ndarray:\n        r\"\"\"\n        Returns the train mask of the dataset\n        Returns:\n            train_mask: training masks\n        \"\"\"\n        if self._train_mask is None:\n            raise ValueError(\"training split hasn't been loaded\")\n        return self._train_mask\n\n    @property\n    def val_mask(self) -&gt; np.ndarray:\n        r\"\"\"\n        Returns the validation mask of the dataset\n        Returns:\n            val_mask: Dict[str, Any]\n        \"\"\"\n        if self._val_mask is None:\n            raise ValueError(\"validation split hasn't been loaded\")\n        return self._val_mask\n\n    @property\n    def test_mask(self) -&gt; np.ndarray:\n        r\"\"\"\n        Returns the test mask of the dataset:\n        Returns:\n            test_mask: Dict[str, Any]\n        \"\"\"\n        if self._test_mask is None:\n            raise ValueError(\"test split hasn't been loaded\")\n        return self._test_mask\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.edge_feat","title":"<code>edge_feat: Optional[np.ndarray]</code>  <code>property</code>","text":"<p>Returns the edge features of the dataset with dim [E, feat_dim] Returns:     edge_feat: np.ndarray, [E, feat_dim] or None if there is no edge feature</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.edge_type","title":"<code>edge_type: Optional[np.ndarray]</code>  <code>property</code>","text":"<p>Returns the edge types of the dataset with dim [E, 1], only for temporal knowledge graph and temporal heterogeneous graph Returns:     edge_type: np.ndarray, [E, 1] or None if it is not a TKG or THG</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.eval_metric","title":"<code>eval_metric: str</code>  <code>property</code>","text":"<p>the official evaluation metric for the dataset, loaded from info.py Returns:     eval_metric: str, the evaluation metric</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.full_data","title":"<code>full_data: Dict[str, Any]</code>  <code>property</code>","text":"<p>the full data of the dataset as a dictionary with keys: 'sources', 'destinations', 'timestamps', 'edge_idxs', 'edge_feat', 'w', 'edge_label',</p> <p>Returns:</p> Name Type Description <code>full_data</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.negative_sampler","title":"<code>negative_sampler: NegativeEdgeSampler</code>  <code>property</code>","text":"<p>Returns the negative sampler of the dataset, will load negative samples from disc Returns:     negative_sampler: NegativeEdgeSampler</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.node_feat","title":"<code>node_feat: Optional[np.ndarray]</code>  <code>property</code>","text":"<p>Returns the node features of the dataset with dim [N, feat_dim] Returns:     node_feat: np.ndarray, [N, feat_dim] or None if there is no node feature</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.node_type","title":"<code>node_type: Optional[np.ndarray]</code>  <code>property</code>","text":"<p>Returns the node types of the dataset with dim [N], only for temporal heterogeneous graphs Returns:     node_feat: np.ndarray, [N] or None if there is no node feature</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.num_edges","title":"<code>num_edges: int</code>  <code>property</code>","text":"<p>Returns the total number of edges in the dataset Returns:     num_edges: int, the number of edges</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.num_nodes","title":"<code>num_nodes: int</code>  <code>property</code>","text":"<p>Returns the total number of unique nodes in the dataset  Returns:     num_nodes: int, the number of unique nodes</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.num_rels","title":"<code>num_rels: int</code>  <code>property</code>","text":"<p>Returns the number of relation types in the dataset Returns:     num_rels: int, the number of relation types</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.static_data","title":"<code>static_data: Optional[np.ndarray]</code>  <code>property</code>","text":"<p>Returns the static edges related to this dataset, applies for tkgl-wikidata and tkgl-smallpedia, edges are (src, dst, rel_type) Returns:     df: pd.DataFrame {\"head\": np.ndarray, \"tail\": np.ndarray, \"rel_type\": np.ndarray}</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.test_mask","title":"<code>test_mask: np.ndarray</code>  <code>property</code>","text":"<p>Returns the test mask of the dataset: Returns:     test_mask: Dict[str, Any]</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.train_mask","title":"<code>train_mask: np.ndarray</code>  <code>property</code>","text":"<p>Returns the train mask of the dataset Returns:     train_mask: training masks</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.val_mask","title":"<code>val_mask: np.ndarray</code>  <code>property</code>","text":"<p>Returns the validation mask of the dataset Returns:     val_mask: Dict[str, Any]</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.__init__","title":"<code>__init__(name, root='datasets', meta_dict=None, preprocess=True)</code>","text":"<p>Dataset class for link prediction dataset. Stores meta information about each dataset such as evaluation metrics etc. also automatically pre-processes the dataset. Args:     name: name of the dataset     root: root directory to store the dataset folder     meta_dict: dictionary containing meta information about the dataset, should contain key 'dir_name' which is the name of the dataset folder     preprocess: whether to pre-process the dataset</p> Source code in <code>tgb/linkproppred/dataset.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    root: Optional[str] = \"datasets\",\n    meta_dict: Optional[dict] = None,\n    preprocess: Optional[bool] = True,\n):\n    r\"\"\"Dataset class for link prediction dataset. Stores meta information about each dataset such as evaluation metrics etc.\n    also automatically pre-processes the dataset.\n    Args:\n        name: name of the dataset\n        root: root directory to store the dataset folder\n        meta_dict: dictionary containing meta information about the dataset, should contain key 'dir_name' which is the name of the dataset folder\n        preprocess: whether to pre-process the dataset\n    \"\"\"\n    self.name = name  ## original name\n    # check if dataset url exist\n    if self.name in DATA_URL_DICT:\n        self.url = DATA_URL_DICT[self.name]\n    else:\n        self.url = None\n        print(f\"Dataset {self.name} url not found, download not supported yet.\")\n\n\n    # check if the evaluatioin metric are specified\n    if self.name in DATA_EVAL_METRIC_DICT:\n        self.metric = DATA_EVAL_METRIC_DICT[self.name]\n    else:\n        self.metric = None\n        print(\n            f\"Dataset {self.name} default evaluation metric not found, it is not supported yet.\"\n        )\n\n\n    root = PROJ_DIR + root\n\n    if meta_dict is None:\n        self.dir_name = \"_\".join(name.split(\"-\"))  ## replace hyphen with underline\n        meta_dict = {\"dir_name\": self.dir_name}\n    else:\n        self.dir_name = meta_dict[\"dir_name\"]\n    self.root = osp.join(root, self.dir_name)\n    self.meta_dict = meta_dict\n    if \"fname\" not in self.meta_dict:\n        self.meta_dict[\"fname\"] = self.root + \"/\" + self.name + \"_edgelist.csv\"\n        self.meta_dict[\"nodefile\"] = None\n\n    if name == \"tgbl-flight\":\n        self.meta_dict[\"nodefile\"] = self.root + \"/\" + \"airport_node_feat.csv\"\n\n    if name == \"tkgl-wikidata\" or name == \"tkgl-smallpedia\":\n        self.meta_dict[\"staticfile\"] = self.root + \"/\" + self.name + \"_static_edgelist.csv\"\n\n    if \"thg\" in name:\n        self.meta_dict[\"nodeTypeFile\"] = self.root + \"/\" + self.name + \"_nodetype.csv\"\n    else:\n        self.meta_dict[\"nodeTypeFile\"] = None\n\n    self.meta_dict[\"val_ns\"] = self.root + \"/\" + self.name + \"_val_ns.pkl\"\n    self.meta_dict[\"test_ns\"] = self.root + \"/\" + self.name + \"_test_ns.pkl\"\n\n    #! version check\n    self.version_passed = True\n    self._version_check()\n\n    # initialize\n    self._node_feat = None\n    self._edge_feat = None\n    self._full_data = None\n    self._train_data = None\n    self._val_data = None\n    self._test_data = None\n\n    # for tkg and thg\n    self._edge_type = None\n\n    #tkgl-wikidata and tkgl-smallpedia only\n    self._static_data = None\n\n    # for thg only\n    self._node_type = None\n    self._node_id = None\n\n    self.download()\n    # check if the root directory exists, if not create it\n    if osp.isdir(self.root):\n        print(\"Dataset directory is \", self.root)\n    else:\n        # os.makedirs(self.root)\n        raise FileNotFoundError(f\"Directory not found at {self.root}\")\n\n    if preprocess:\n        self.pre_process()\n\n    self.min_dst_idx, self.max_dst_idx = int(self._full_data[\"destinations\"].min()), int(self._full_data[\"destinations\"].max())\n\n    if ('tkg' in self.name):\n        if self.name in DATA_NS_STRATEGY_DICT:\n            self.ns_sampler = TKGNegativeEdgeSampler(\n                dataset_name=self.name,\n                first_dst_id=self.min_dst_idx,\n                last_dst_id=self.max_dst_idx,\n                strategy=DATA_NS_STRATEGY_DICT[self.name],\n                partial_path=self.root + \"/\" + self.name,\n            )\n        else:\n            raise ValueError(f\"Dataset {self.name} negative sampling strategy not found.\")\n    elif ('thg' in self.name):\n        #* need to find the smallest node id of all nodes (regardless of types)\n\n        min_node_idx = min(int(self._full_data[\"sources\"].min()), int(self._full_data[\"destinations\"].min()))\n        max_node_idx = max(int(self._full_data[\"sources\"].max()), int(self._full_data[\"destinations\"].max()))\n        self.ns_sampler = THGNegativeEdgeSampler(\n            dataset_name=self.name,\n            first_node_id=min_node_idx,\n            last_node_id=max_node_idx,\n            node_type=self._node_type,\n        )\n    else:\n        self.ns_sampler = NegativeEdgeSampler(\n            dataset_name=self.name,\n            first_dst_id=self.min_dst_idx,\n            last_dst_id=self.max_dst_idx,\n        )\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.download","title":"<code>download()</code>","text":"<p>downloads this dataset from url check if files are already downloaded</p> Source code in <code>tgb/linkproppred/dataset.py</code> <pre><code>def download(self):\n    \"\"\"\n    downloads this dataset from url\n    check if files are already downloaded\n    \"\"\"\n    # check if the file already exists\n    if osp.exists(self.meta_dict[\"fname\"]):\n        print(\"raw file found, skipping download\")\n        return\n\n    inp = input(\n        \"Will you download the dataset(s) now? (y/N)\\n\"\n    ).lower()  # ask if the user wants to download the dataset\n\n    if inp == \"y\":\n        print(\n            f\"{BColors.WARNING}Download started, this might take a while . . . {BColors.ENDC}\"\n        )\n        print(f\"Dataset title: {self.name}\")\n\n        if self.url is None:\n            raise Exception(\"Dataset url not found, download not supported yet.\")\n        else:\n            r = requests.get(self.url, stream=True)\n            # download_dir = self.root + \"/\" + \"download\"\n            if osp.isdir(self.root):\n                print(\"Dataset directory is \", self.root)\n            else:\n                os.makedirs(self.root)\n\n            path_download = self.root + \"/\" + self.name + \".zip\"\n            with open(path_download, \"wb\") as f:\n                total_length = int(r.headers.get(\"content-length\"))\n                for chunk in progress.bar(\n                    r.iter_content(chunk_size=1024),\n                    expected_size=(total_length / 1024) + 1,\n                ):\n                    if chunk:\n                        f.write(chunk)\n                        f.flush()\n            # for unzipping the file\n            with zipfile.ZipFile(path_download, \"r\") as zip_ref:\n                zip_ref.extractall(self.root)\n            print(f\"{BColors.OKGREEN}Download completed {BColors.ENDC}\")\n            self.version_passed = True\n    else:\n        raise Exception(\n            BColors.FAIL + \"Data not found error, download \" + self.name + \" failed\"\n        )\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.generate_processed_files","title":"<code>generate_processed_files()</code>","text":"<p>turns raw data .csv file into a pandas data frame, stored on disc if not already Returns:     df: pandas data frame</p> Source code in <code>tgb/linkproppred/dataset.py</code> <pre><code>def generate_processed_files(self) -&gt; pd.DataFrame:\n    r\"\"\"\n    turns raw data .csv file into a pandas data frame, stored on disc if not already\n    Returns:\n        df: pandas data frame\n    \"\"\"\n    node_feat = None\n    if not osp.exists(self.meta_dict[\"fname\"]):\n        raise FileNotFoundError(f\"File not found at {self.meta_dict['fname']}\")\n\n    if self.meta_dict[\"nodefile\"] is not None:\n        if not osp.exists(self.meta_dict[\"nodefile\"]):\n            raise FileNotFoundError(\n                f\"File not found at {self.meta_dict['nodefile']}\"\n            )\n    #* for thg must have nodetypes \n    if self.meta_dict[\"nodeTypeFile\"] is not None:\n        if not osp.exists(self.meta_dict[\"nodeTypeFile\"]):\n            raise FileNotFoundError(\n                f\"File not found at {self.meta_dict['nodeTypeFile']}\"\n            )\n\n\n    OUT_DF = self.root + \"/\" + \"ml_{}.pkl\".format(self.name)\n    OUT_EDGE_FEAT = self.root + \"/\" + \"ml_{}.pkl\".format(self.name + \"_edge\")\n    OUT_NODE_ID = self.root + \"/\" + \"ml_{}.pkl\".format(self.name + \"_nodeid\")\n    if self.meta_dict[\"nodefile\"] is not None:\n        OUT_NODE_FEAT = self.root + \"/\" + \"ml_{}.pkl\".format(self.name + \"_node\")\n    if self.meta_dict[\"nodeTypeFile\"] is not None:\n        OUT_NODE_TYPE = self.root + \"/\" + \"ml_{}.pkl\".format(self.name + \"_nodeType\")\n\n    if (osp.exists(OUT_DF)) and (self.version_passed is True):\n        print(\"loading processed file\")\n        df = pd.read_pickle(OUT_DF)\n        edge_feat = load_pkl(OUT_EDGE_FEAT)\n        if (self.name == \"tkgl-wikidata\") or (self.name == \"tkgl-smallpedia\"):\n            node_id = load_pkl(OUT_NODE_ID)\n            self._node_id = node_id\n        if self.meta_dict[\"nodefile\"] is not None:\n            node_feat = load_pkl(OUT_NODE_FEAT)\n        if self.meta_dict[\"nodeTypeFile\"] is not None:\n            node_type = load_pkl(OUT_NODE_TYPE)\n            self._node_type = node_type\n\n    else:\n        print(\"file not processed, generating processed file\")\n        if self.name == \"tgbl-flight\":\n            df, edge_feat, node_ids = csv_to_pd_data(self.meta_dict[\"fname\"])\n        elif self.name == \"tgbl-coin\":\n            df, edge_feat, node_ids = csv_to_pd_data_sc(self.meta_dict[\"fname\"])\n        elif self.name == \"tgbl-comment\":\n            df, edge_feat, node_ids = csv_to_pd_data_rc(self.meta_dict[\"fname\"])\n        elif self.name == \"tgbl-review\":\n            df, edge_feat, node_ids = csv_to_pd_data_sc(self.meta_dict[\"fname\"])\n        elif self.name == \"tgbl-wiki\":\n            df, edge_feat, node_ids = load_edgelist_wiki(self.meta_dict[\"fname\"])\n        elif self.name == \"tgbl-subreddit\":\n            df, edge_feat, node_ids = load_edgelist_wiki(self.meta_dict[\"fname\"])\n        elif self.name == \"tgbl-lastfm\":\n            df, edge_feat, node_ids = load_edgelist_wiki(self.meta_dict[\"fname\"])\n        elif self.name == \"tkgl-polecat\":\n            df, edge_feat, node_ids = csv_to_tkg_data(self.meta_dict[\"fname\"])\n        elif self.name == \"tkgl-icews\":\n            df, edge_feat, node_ids = csv_to_tkg_data(self.meta_dict[\"fname\"])\n        elif self.name == \"tkgl-yago\":\n            df, edge_feat, node_ids = csv_to_tkg_data(self.meta_dict[\"fname\"])\n        elif self.name == \"tkgl-wikidata\":\n            df, edge_feat, node_ids = csv_to_wikidata(self.meta_dict[\"fname\"])\n            save_pkl(node_ids, OUT_NODE_ID)\n            self._node_id = node_ids\n        elif self.name == \"tkgl-smallpedia\":\n            df, edge_feat, node_ids = csv_to_wikidata(self.meta_dict[\"fname\"])\n            save_pkl(node_ids, OUT_NODE_ID)\n            self._node_id = node_ids\n        elif self.name == \"thgl-myket\":\n            df, edge_feat, node_ids = csv_to_thg_data(self.meta_dict[\"fname\"])\n        elif self.name == \"thgl-github\":\n            df, edge_feat, node_ids = csv_to_thg_data(self.meta_dict[\"fname\"])\n        elif self.name == \"thgl-forum\":\n            df, edge_feat, node_ids = csv_to_forum_data(self.meta_dict[\"fname\"])\n        elif self.name == \"thgl-software\":\n            df, edge_feat, node_ids = csv_to_thg_data(self.meta_dict[\"fname\"])\n        else:\n            raise ValueError(f\"Dataset {self.name} not found.\")\n\n        save_pkl(edge_feat, OUT_EDGE_FEAT)\n        df.to_pickle(OUT_DF)\n        if self.meta_dict[\"nodefile\"] is not None:\n            node_feat = process_node_feat(self.meta_dict[\"nodefile\"], node_ids)\n            save_pkl(node_feat, OUT_NODE_FEAT)\n        if self.meta_dict[\"nodeTypeFile\"] is not None:\n            node_type = process_node_type(self.meta_dict[\"nodeTypeFile\"], node_ids)\n            save_pkl(node_type, OUT_NODE_TYPE)\n            #? do not return node_type, simply set it\n            self._node_type = node_type\n\n\n    return df, edge_feat, node_feat\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.generate_splits","title":"<code>generate_splits(full_data, val_ratio=0.15, test_ratio=0.15)</code>","text":"<p>Generates train, validation, and test splits from the full dataset Args:     full_data: dictionary containing the full dataset     val_ratio: ratio of validation data     test_ratio: ratio of test data Returns:     train_data: dictionary containing the training dataset     val_data: dictionary containing the validation dataset     test_data: dictionary containing the test dataset</p> Source code in <code>tgb/linkproppred/dataset.py</code> <pre><code>def generate_splits(\n    self,\n    full_data: Dict[str, Any],\n    val_ratio: float = 0.15,\n    test_ratio: float = 0.15,\n) -&gt; Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\n    r\"\"\"Generates train, validation, and test splits from the full dataset\n    Args:\n        full_data: dictionary containing the full dataset\n        val_ratio: ratio of validation data\n        test_ratio: ratio of test data\n    Returns:\n        train_data: dictionary containing the training dataset\n        val_data: dictionary containing the validation dataset\n        test_data: dictionary containing the test dataset\n    \"\"\"\n    val_time, test_time = list(\n        np.quantile(\n            full_data[\"timestamps\"],\n            [(1 - val_ratio - test_ratio), (1 - test_ratio)],\n        )\n    )\n    timestamps = full_data[\"timestamps\"]\n\n    train_mask = timestamps &lt;= val_time\n    val_mask = np.logical_and(timestamps &lt;= test_time, timestamps &gt; val_time)\n    test_mask = timestamps &gt; test_time\n\n    return train_mask, val_mask, test_mask\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.load_test_ns","title":"<code>load_test_ns()</code>","text":"<p>load the negative samples for the test set</p> Source code in <code>tgb/linkproppred/dataset.py</code> <pre><code>def load_test_ns(self) -&gt; None:\n    r\"\"\"\n    load the negative samples for the test set\n    \"\"\"\n    self.ns_sampler.load_eval_set(\n        fname=self.meta_dict[\"test_ns\"], split_mode=\"test\"\n    )\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.load_val_ns","title":"<code>load_val_ns()</code>","text":"<p>load the negative samples for the validation set</p> Source code in <code>tgb/linkproppred/dataset.py</code> <pre><code>def load_val_ns(self) -&gt; None:\n    r\"\"\"\n    load the negative samples for the validation set\n    \"\"\"\n    self.ns_sampler.load_eval_set(\n        fname=self.meta_dict[\"val_ns\"], split_mode=\"val\"\n    )\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.pre_process","title":"<code>pre_process()</code>","text":"<p>Pre-process the dataset and generates the splits, must be run before dataset properties can be accessed generates the edge data and different train, val, test splits</p> Source code in <code>tgb/linkproppred/dataset.py</code> <pre><code>def pre_process(self):\n    \"\"\"\n    Pre-process the dataset and generates the splits, must be run before dataset properties can be accessed\n    generates the edge data and different train, val, test splits\n    \"\"\"\n\n    # check if path to file is valid\n    df, edge_feat, node_feat = self.generate_processed_files()\n\n    #* design choice, only stores the original edges not the inverse relations on disc\n    if (\"tkgl\" in self.name):\n        df = add_inverse_quadruples(df)\n\n    sources = np.array(df[\"u\"])\n    destinations = np.array(df[\"i\"])\n    timestamps = np.array(df[\"ts\"])\n    edge_idxs = np.array(df[\"idx\"])\n    weights = np.array(df[\"w\"])\n    edge_label = np.ones(len(df))  # should be 1 for all pos edges\n    self._edge_feat = edge_feat\n    self._node_feat = node_feat\n\n    full_data = {\n        \"sources\": sources.astype(int),\n        \"destinations\": destinations.astype(int),\n        \"timestamps\": timestamps.astype(int),\n        \"edge_idxs\": edge_idxs,\n        \"edge_feat\": edge_feat,\n        \"w\": weights,\n        \"edge_label\": edge_label,\n    }\n\n    #* for tkg and thg\n    if (\"edge_type\" in df):\n        edge_type = np.array(df[\"edge_type\"]).astype(int)\n        self._edge_type = edge_type\n        full_data[\"edge_type\"] = edge_type\n\n    self._full_data = full_data\n\n    if (\"yago\" in self.name):\n        _train_mask, _val_mask, _test_mask = self.generate_splits(full_data, val_ratio=0.1, test_ratio=0.10) #99) #val_ratio=0.097, test_ratio=0.099)\n    else:\n        _train_mask, _val_mask, _test_mask = self.generate_splits(full_data, val_ratio=0.15, test_ratio=0.15)\n    self._train_mask = _train_mask\n    self._val_mask = _val_mask\n    self._test_mask = _test_mask\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset.LinkPropPredDataset.preprocess_static_edges","title":"<code>preprocess_static_edges()</code>","text":"<p>Pre-process the static edges of the dataset</p> Source code in <code>tgb/linkproppred/dataset.py</code> <pre><code>def preprocess_static_edges(self):\n    \"\"\"\n    Pre-process the static edges of the dataset\n    \"\"\"\n    if (\"staticfile\" in self.meta_dict):\n        OUT_DF = self.root + \"/\" + \"ml_{}.pkl\".format(self.name + \"_static\")\n        if (osp.exists(OUT_DF)) and (self.version_passed is True):\n            print(\"loading processed file\")\n            static_dict = load_pkl(OUT_DF)\n            self._static_data = static_dict\n        else:\n            print(\"file not processed, generating processed file\")\n            static_dict, node_ids =  csv_to_staticdata(self.meta_dict[\"staticfile\"], self._node_id)\n            save_pkl(static_dict, OUT_DF)\n            self._static_data = static_dict\n    else:\n        print (\"static edges are only for tkgl-wikidata and tkgl-smallpedia datasets\")\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset","title":"<code>PyGLinkPropPredDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> Source code in <code>tgb/linkproppred/dataset_pyg.py</code> <pre><code>class PyGLinkPropPredDataset(Dataset):\n    def __init__(\n        self,\n        name: str,\n        root: str,\n        transform: Optional[Callable] = None,\n        pre_transform: Optional[Callable] = None,\n    ):\n        r\"\"\"\n        PyG wrapper for the LinkPropPredDataset\n        can return pytorch tensors for src,dst,t,msg,label\n        can return Temporal Data object\n        Parameters:\n            name: name of the dataset, passed to `LinkPropPredDataset`\n            root (string): Root directory where the dataset should be saved, passed to `LinkPropPredDataset`\n            transform (callable, optional): A function/transform that takes in an, not used in this case\n            pre_transform (callable, optional): A function/transform that takes in, not used in this case\n        \"\"\"\n        self.name = name\n        self.root = root\n        self.dataset = LinkPropPredDataset(name=name, root=root)\n        self._train_mask = torch.from_numpy(self.dataset.train_mask)\n        self._val_mask = torch.from_numpy(self.dataset.val_mask)\n        self._test_mask = torch.from_numpy(self.dataset.test_mask)\n        super().__init__(root, transform, pre_transform)\n        self._node_feat = self.dataset.node_feat\n        self._edge_type = None\n        self._static_data = None\n\n        if self._node_feat is None:\n            self._node_feat = None\n        else:\n            self._node_feat = torch.from_numpy(self._node_feat).float()\n\n        self._node_type = self.dataset.node_type\n        if self.node_type is not None:\n            self._node_type = torch.from_numpy(self.dataset.node_type).long()\n\n        self.process_data()\n\n        self._ns_sampler = self.dataset.negative_sampler\n\n    @property\n    def eval_metric(self) -&gt; str:\n        \"\"\"\n        the official evaluation metric for the dataset, loaded from info.py\n        Returns:\n            eval_metric: str, the evaluation metric\n        \"\"\"\n        return self.dataset.eval_metric\n\n    @property\n    def negative_sampler(self) -&gt; NegativeEdgeSampler:\n        r\"\"\"\n        Returns the negative sampler of the dataset, will load negative samples from disc\n        Returns:\n            negative_sampler: NegativeEdgeSampler\n        \"\"\"\n        return self._ns_sampler\n\n    @property\n    def num_nodes(self) -&gt; int:\n        r\"\"\"\n        Returns the total number of unique nodes in the dataset \n        Returns:\n            num_nodes: int, the number of unique nodes\n        \"\"\"\n        return self.dataset.num_nodes\n\n    @property\n    def num_rels(self) -&gt; int:\n        r\"\"\"\n        Returns the total number of unique relations in the dataset \n        Returns:\n            num_rels: int, the number of unique relations\n        \"\"\"\n        return self.dataset.num_rels\n\n    @property\n    def num_edges(self) -&gt; int:\n        r\"\"\"\n        Returns the total number of edges in the dataset \n        Returns:\n            num_edges: int, the number of edges\n        \"\"\"\n        return self.dataset.num_edges\n\n    def load_val_ns(self) -&gt; None:\n        r\"\"\"\n        load the negative samples for the validation set\n        \"\"\"\n        self.dataset.load_val_ns()\n\n    def load_test_ns(self) -&gt; None:\n        r\"\"\"\n        load the negative samples for the test set\n        \"\"\"\n        self.dataset.load_test_ns()\n\n    @property\n    def train_mask(self) -&gt; torch.Tensor:\n        r\"\"\"\n        Returns the train mask of the dataset\n        Returns:\n            train_mask: the mask for edges in the training set\n        \"\"\"\n        if self._train_mask is None:\n            raise ValueError(\"training split hasn't been loaded\")\n        return self._train_mask\n\n    @property\n    def val_mask(self) -&gt; torch.Tensor:\n        r\"\"\"\n        Returns the validation mask of the dataset\n        Returns:\n            val_mask: the mask for edges in the validation set\n        \"\"\"\n        if self._val_mask is None:\n            raise ValueError(\"validation split hasn't been loaded\")\n        return self._val_mask\n\n    @property\n    def test_mask(self) -&gt; torch.Tensor:\n        r\"\"\"\n        Returns the test mask of the dataset:\n        Returns:\n            test_mask: the mask for edges in the test set\n        \"\"\"\n        if self._test_mask is None:\n            raise ValueError(\"test split hasn't been loaded\")\n        return self._test_mask\n\n    @property\n    def node_feat(self) -&gt; torch.Tensor:\n        r\"\"\"\n        Returns the node features of the dataset\n        Returns:\n            node_feat: the node features\n        \"\"\"\n        return self._node_feat\n\n    @property\n    def node_type(self) -&gt; torch.Tensor:\n        r\"\"\"\n        Returns the node types of the dataset\n        Returns:\n            node_type: the node types [N]\n        \"\"\"\n        return self._node_type\n\n    @property\n    def src(self) -&gt; torch.Tensor:\n        r\"\"\"\n        Returns the source nodes of the dataset\n        Returns:\n            src: the idx of the source nodes\n        \"\"\"\n        return self._src\n\n    @property\n    def dst(self) -&gt; torch.Tensor:\n        r\"\"\"\n        Returns the destination nodes of the dataset\n        Returns:\n            dst: the idx of the destination nodes\n        \"\"\"\n        return self._dst\n\n    @property\n    def ts(self) -&gt; torch.Tensor:\n        r\"\"\"\n        Returns the timestamps of the dataset\n        Returns:\n            ts: the timestamps of the edges\n        \"\"\"\n        return self._ts\n\n    @property\n    def static_data(self) -&gt; torch.Tensor:\n        r\"\"\"\n        Returns the static data of the dataset for tkgl-wikidata and tkgl-smallpedia\n        Returns:\n            static_data: the static data of the dataset\n        \"\"\"\n        if (self._static_data is None):\n            static_dict = {}\n            static_dict[\"head\"] = torch.from_numpy(self.dataset.static_data[\"head\"]).long()\n            static_dict[\"tail\"] = torch.from_numpy(self.dataset.static_data[\"tail\"]).long()\n            static_dict[\"edge_type\"] = torch.from_numpy(self.dataset.static_data[\"edge_type\"]).long()\n            self._static_data = static_dict\n            return self._static_data\n        else:\n            return self._static_data \n\n    @property\n    def edge_type(self) -&gt; torch.Tensor:\n        r\"\"\"\n        Returns the edge types for each edge\n        Returns:\n            edge_type: edge type tensor (int)\n        \"\"\"\n        return self._edge_type\n\n    @property\n    def edge_feat(self) -&gt; torch.Tensor:\n        r\"\"\"\n        Returns the edge features of the dataset\n        Returns:\n            edge_feat: the edge features\n        \"\"\"\n        return self._edge_feat\n\n    @property\n    def edge_label(self) -&gt; torch.Tensor:\n        r\"\"\"\n        Returns the edge labels of the dataset\n        Returns:\n            edge_label: the labels of the edges\n        \"\"\"\n        return self._edge_label\n\n    def process_data(self) -&gt; None:\n        r\"\"\"\n        convert the numpy arrays from dataset to pytorch tensors\n        \"\"\"\n        src = torch.from_numpy(self.dataset.full_data[\"sources\"])\n        dst = torch.from_numpy(self.dataset.full_data[\"destinations\"])\n        ts = torch.from_numpy(self.dataset.full_data[\"timestamps\"])\n        msg = torch.from_numpy(\n            self.dataset.full_data[\"edge_feat\"]\n        )  # use edge features here if available\n        edge_label = torch.from_numpy(\n            self.dataset.full_data[\"edge_label\"]\n        )  # this is the label indicating if an edge is a true edge, always 1 for true edges\n\n\n        # * first check typing for all tensors\n        # source tensor must be of type int64\n        # warnings.warn(\"sources tensor is not of type int64 or int32, forcing conversion\")\n        if src.dtype != torch.int64:\n            src = src.long()\n\n        # destination tensor must be of type int64\n        if dst.dtype != torch.int64:\n            dst = dst.long()\n\n        # timestamp tensor must be of type int64\n        if ts.dtype != torch.int64:\n            ts = ts.long()\n\n        # message tensor must be of type float32\n        if msg.dtype != torch.float32:\n            msg = msg.float()\n\n        #* for tkg\n        if (\"edge_type\" in self.dataset.full_data):\n            edge_type = torch.from_numpy(self.dataset.full_data[\"edge_type\"])\n            if edge_type.dtype != torch.int64:\n                edge_type = edge_type.long()\n            self._edge_type = edge_type\n\n        self._src = src\n        self._dst = dst\n        self._ts = ts\n        self._edge_label = edge_label\n        self._edge_feat = msg\n\n    def get_TemporalData(self) -&gt; TemporalData:\n        \"\"\"\n        return the TemporalData object for the entire dataset\n        \"\"\"\n        if (self._edge_type is not None):\n            data = TemporalData(\n                src=self._src,\n                dst=self._dst,\n                t=self._ts,\n                msg=self._edge_feat,\n                y=self._edge_label,\n                edge_type=self._edge_type\n            )\n        else:\n            data = TemporalData(\n                src=self._src,\n                dst=self._dst,\n                t=self._ts,\n                msg=self._edge_feat,\n                y=self._edge_label,\n            )\n        return data\n\n    def len(self) -&gt; int:\n        \"\"\"\n        size of the dataset\n        Returns:\n            size: int\n        \"\"\"\n        return self._src.shape[0]\n\n    def get(self, idx: int) -&gt; TemporalData:\n        \"\"\"\n        construct temporal data object for a single edge\n        Parameters:\n            idx: index of the edge\n        Returns:\n            data: TemporalData object\n        \"\"\"\n        if (self._edge_type is not None):\n            data = TemporalData(\n                src=self._src[idx],\n                dst=self._dst[idx],\n                t=self._ts[idx],\n                msg=self._edge_feat[idx],\n                y=self._edge_label[idx],\n                edge_type=self._edge_type[idx]\n            )\n        else:\n            data = TemporalData(\n                src=self._src[idx],\n                dst=self._dst[idx],\n                t=self._ts[idx],\n                msg=self._edge_feat[idx],\n                y=self._edge_label[idx],\n            )\n        return data\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.name.capitalize()}()\"\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.dst","title":"<code>dst: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the destination nodes of the dataset Returns:     dst: the idx of the destination nodes</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.edge_feat","title":"<code>edge_feat: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the edge features of the dataset Returns:     edge_feat: the edge features</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.edge_label","title":"<code>edge_label: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the edge labels of the dataset Returns:     edge_label: the labels of the edges</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.edge_type","title":"<code>edge_type: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the edge types for each edge Returns:     edge_type: edge type tensor (int)</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.eval_metric","title":"<code>eval_metric: str</code>  <code>property</code>","text":"<p>the official evaluation metric for the dataset, loaded from info.py Returns:     eval_metric: str, the evaluation metric</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.negative_sampler","title":"<code>negative_sampler: NegativeEdgeSampler</code>  <code>property</code>","text":"<p>Returns the negative sampler of the dataset, will load negative samples from disc Returns:     negative_sampler: NegativeEdgeSampler</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.node_feat","title":"<code>node_feat: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the node features of the dataset Returns:     node_feat: the node features</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.node_type","title":"<code>node_type: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the node types of the dataset Returns:     node_type: the node types [N]</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.num_edges","title":"<code>num_edges: int</code>  <code>property</code>","text":"<p>Returns the total number of edges in the dataset  Returns:     num_edges: int, the number of edges</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.num_nodes","title":"<code>num_nodes: int</code>  <code>property</code>","text":"<p>Returns the total number of unique nodes in the dataset  Returns:     num_nodes: int, the number of unique nodes</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.num_rels","title":"<code>num_rels: int</code>  <code>property</code>","text":"<p>Returns the total number of unique relations in the dataset  Returns:     num_rels: int, the number of unique relations</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.src","title":"<code>src: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the source nodes of the dataset Returns:     src: the idx of the source nodes</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.static_data","title":"<code>static_data: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the static data of the dataset for tkgl-wikidata and tkgl-smallpedia Returns:     static_data: the static data of the dataset</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.test_mask","title":"<code>test_mask: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the test mask of the dataset: Returns:     test_mask: the mask for edges in the test set</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.train_mask","title":"<code>train_mask: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the train mask of the dataset Returns:     train_mask: the mask for edges in the training set</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.ts","title":"<code>ts: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the timestamps of the dataset Returns:     ts: the timestamps of the edges</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.val_mask","title":"<code>val_mask: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the validation mask of the dataset Returns:     val_mask: the mask for edges in the validation set</p>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.__init__","title":"<code>__init__(name, root, transform=None, pre_transform=None)</code>","text":"<p>PyG wrapper for the LinkPropPredDataset can return pytorch tensors for src,dst,t,msg,label can return Temporal Data object Parameters:     name: name of the dataset, passed to <code>LinkPropPredDataset</code>     root (string): Root directory where the dataset should be saved, passed to <code>LinkPropPredDataset</code>     transform (callable, optional): A function/transform that takes in an, not used in this case     pre_transform (callable, optional): A function/transform that takes in, not used in this case</p> Source code in <code>tgb/linkproppred/dataset_pyg.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    root: str,\n    transform: Optional[Callable] = None,\n    pre_transform: Optional[Callable] = None,\n):\n    r\"\"\"\n    PyG wrapper for the LinkPropPredDataset\n    can return pytorch tensors for src,dst,t,msg,label\n    can return Temporal Data object\n    Parameters:\n        name: name of the dataset, passed to `LinkPropPredDataset`\n        root (string): Root directory where the dataset should be saved, passed to `LinkPropPredDataset`\n        transform (callable, optional): A function/transform that takes in an, not used in this case\n        pre_transform (callable, optional): A function/transform that takes in, not used in this case\n    \"\"\"\n    self.name = name\n    self.root = root\n    self.dataset = LinkPropPredDataset(name=name, root=root)\n    self._train_mask = torch.from_numpy(self.dataset.train_mask)\n    self._val_mask = torch.from_numpy(self.dataset.val_mask)\n    self._test_mask = torch.from_numpy(self.dataset.test_mask)\n    super().__init__(root, transform, pre_transform)\n    self._node_feat = self.dataset.node_feat\n    self._edge_type = None\n    self._static_data = None\n\n    if self._node_feat is None:\n        self._node_feat = None\n    else:\n        self._node_feat = torch.from_numpy(self._node_feat).float()\n\n    self._node_type = self.dataset.node_type\n    if self.node_type is not None:\n        self._node_type = torch.from_numpy(self.dataset.node_type).long()\n\n    self.process_data()\n\n    self._ns_sampler = self.dataset.negative_sampler\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.get","title":"<code>get(idx)</code>","text":"<p>construct temporal data object for a single edge Parameters:     idx: index of the edge Returns:     data: TemporalData object</p> Source code in <code>tgb/linkproppred/dataset_pyg.py</code> <pre><code>def get(self, idx: int) -&gt; TemporalData:\n    \"\"\"\n    construct temporal data object for a single edge\n    Parameters:\n        idx: index of the edge\n    Returns:\n        data: TemporalData object\n    \"\"\"\n    if (self._edge_type is not None):\n        data = TemporalData(\n            src=self._src[idx],\n            dst=self._dst[idx],\n            t=self._ts[idx],\n            msg=self._edge_feat[idx],\n            y=self._edge_label[idx],\n            edge_type=self._edge_type[idx]\n        )\n    else:\n        data = TemporalData(\n            src=self._src[idx],\n            dst=self._dst[idx],\n            t=self._ts[idx],\n            msg=self._edge_feat[idx],\n            y=self._edge_label[idx],\n        )\n    return data\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.get_TemporalData","title":"<code>get_TemporalData()</code>","text":"<p>return the TemporalData object for the entire dataset</p> Source code in <code>tgb/linkproppred/dataset_pyg.py</code> <pre><code>def get_TemporalData(self) -&gt; TemporalData:\n    \"\"\"\n    return the TemporalData object for the entire dataset\n    \"\"\"\n    if (self._edge_type is not None):\n        data = TemporalData(\n            src=self._src,\n            dst=self._dst,\n            t=self._ts,\n            msg=self._edge_feat,\n            y=self._edge_label,\n            edge_type=self._edge_type\n        )\n    else:\n        data = TemporalData(\n            src=self._src,\n            dst=self._dst,\n            t=self._ts,\n            msg=self._edge_feat,\n            y=self._edge_label,\n        )\n    return data\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.len","title":"<code>len()</code>","text":"<p>size of the dataset Returns:     size: int</p> Source code in <code>tgb/linkproppred/dataset_pyg.py</code> <pre><code>def len(self) -&gt; int:\n    \"\"\"\n    size of the dataset\n    Returns:\n        size: int\n    \"\"\"\n    return self._src.shape[0]\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.load_test_ns","title":"<code>load_test_ns()</code>","text":"<p>load the negative samples for the test set</p> Source code in <code>tgb/linkproppred/dataset_pyg.py</code> <pre><code>def load_test_ns(self) -&gt; None:\n    r\"\"\"\n    load the negative samples for the test set\n    \"\"\"\n    self.dataset.load_test_ns()\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.load_val_ns","title":"<code>load_val_ns()</code>","text":"<p>load the negative samples for the validation set</p> Source code in <code>tgb/linkproppred/dataset_pyg.py</code> <pre><code>def load_val_ns(self) -&gt; None:\n    r\"\"\"\n    load the negative samples for the validation set\n    \"\"\"\n    self.dataset.load_val_ns()\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset.process_data","title":"<code>process_data()</code>","text":"<p>convert the numpy arrays from dataset to pytorch tensors</p> Source code in <code>tgb/linkproppred/dataset_pyg.py</code> <pre><code>def process_data(self) -&gt; None:\n    r\"\"\"\n    convert the numpy arrays from dataset to pytorch tensors\n    \"\"\"\n    src = torch.from_numpy(self.dataset.full_data[\"sources\"])\n    dst = torch.from_numpy(self.dataset.full_data[\"destinations\"])\n    ts = torch.from_numpy(self.dataset.full_data[\"timestamps\"])\n    msg = torch.from_numpy(\n        self.dataset.full_data[\"edge_feat\"]\n    )  # use edge features here if available\n    edge_label = torch.from_numpy(\n        self.dataset.full_data[\"edge_label\"]\n    )  # this is the label indicating if an edge is a true edge, always 1 for true edges\n\n\n    # * first check typing for all tensors\n    # source tensor must be of type int64\n    # warnings.warn(\"sources tensor is not of type int64 or int32, forcing conversion\")\n    if src.dtype != torch.int64:\n        src = src.long()\n\n    # destination tensor must be of type int64\n    if dst.dtype != torch.int64:\n        dst = dst.long()\n\n    # timestamp tensor must be of type int64\n    if ts.dtype != torch.int64:\n        ts = ts.long()\n\n    # message tensor must be of type float32\n    if msg.dtype != torch.float32:\n        msg = msg.float()\n\n    #* for tkg\n    if (\"edge_type\" in self.dataset.full_data):\n        edge_type = torch.from_numpy(self.dataset.full_data[\"edge_type\"])\n        if edge_type.dtype != torch.int64:\n            edge_type = edge_type.long()\n        self._edge_type = edge_type\n\n    self._src = src\n    self._dst = dst\n    self._ts = ts\n    self._edge_label = edge_label\n    self._edge_feat = msg\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.evaluate.Evaluator","title":"<code>Evaluator</code>","text":"<p>               Bases: <code>object</code></p> <p>Evaluator for Link Property Prediction</p> Source code in <code>tgb/linkproppred/evaluate.py</code> <pre><code>class Evaluator(object):\n    r\"\"\"Evaluator for Link Property Prediction \"\"\"\n\n    def __init__(self, name: str, k_value: int = 10):\n        r\"\"\"\n        Parameters:\n            name: name of the dataset\n            k_value: the desired 'k' value for calculating metric@k\n        \"\"\"\n        self.name = name\n        self.k_value = k_value  # for computing `hits@k`\n        self.valid_metric_list = ['hits@', 'mrr']\n        if self.name not in DATA_EVAL_METRIC_DICT:\n            raise NotImplementedError(\"Dataset not supported\")\n\n    def _parse_and_check_input(self, input_dict):\n        r\"\"\"\n        Check whether the input has the appropriate format\n        Parametrers:\n            input_dict: a dictionary containing \"y_pred_pos\", \"y_pred_neg\", and \"eval_metric\"\n            note: \"eval_metric\" should be a list including one or more of the followin metrics: [\"hits@\", \"mrr\"]\n        Returns:\n            y_pred_pos: positive predicted scores\n            y_pred_neg: negative predicted scores\n        \"\"\"\n\n        if \"eval_metric\" not in input_dict:\n            raise RuntimeError(\"Missing key of eval_metric!\")\n\n        for eval_metric in input_dict[\"eval_metric\"]:\n            if eval_metric in self.valid_metric_list:\n                if \"y_pred_pos\" not in input_dict:\n                    raise RuntimeError(\"Missing key of y_true\")\n                if \"y_pred_neg\" not in input_dict:\n                    raise RuntimeError(\"Missing key of y_pred\")\n\n                y_pred_pos, y_pred_neg = input_dict[\"y_pred_pos\"], input_dict[\"y_pred_neg\"]\n\n                # converting to numpy on cpu\n                if torch is not None and isinstance(y_pred_pos, torch.Tensor):\n                    y_pred_pos = y_pred_pos.detach().cpu().numpy()\n                if torch is not None and isinstance(y_pred_neg, torch.Tensor):\n                    y_pred_neg = y_pred_neg.detach().cpu().numpy()\n\n                # check type and shape\n                if not isinstance(y_pred_pos, np.ndarray) or not isinstance(y_pred_neg, np.ndarray):\n                    raise RuntimeError(\n                        \"Arguments to Evaluator need to be either numpy ndarray or torch tensor!\"\n                    )\n            else:\n                print(\n                    \"ERROR: The evaluation metric should be in:\", self.valid_metric_list\n                )\n                raise ValueError(\"Unsupported eval metric %s \" % (eval_metric))\n        self.eval_metric = input_dict[\"eval_metric\"]\n\n        return y_pred_pos, y_pred_neg\n\n    def _eval_hits_and_mrr(self, y_pred_pos, y_pred_neg, type_info, k_value):\n        r\"\"\"\n        compute hist@k and mrr\n        reference:\n            - https://github.com/snap-stanford/ogb/blob/d5c11d91c9e1c22ed090a2e0bbda3fe357de66e7/ogb/linkproppred/evaluate.py#L214\n\n        Parameters:\n            y_pred_pos: positive predicted scores\n            y_pred_neg: negative predicted scores\n            type_info: type of the predicted scores; could be 'torch' or 'numpy'\n            k_value: the desired 'k' value for calculating metric@k\n\n        Returns:\n            a dictionary containing the computed performance metrics\n        \"\"\"\n        if type_info == 'torch':\n            # calculate ranks\n            y_pred_pos = y_pred_pos.view(-1, 1)\n            # optimistic rank: \"how many negatives have a larger score than the positive?\"\n            # ~&gt; the positive is ranked first among those with equal score\n            optimistic_rank = (y_pred_neg &gt; y_pred_pos).sum(dim=1)\n            # pessimistic rank: \"how many negatives have at least the positive score?\"\n            # ~&gt; the positive is ranked last among those with equal score\n            pessimistic_rank = (y_pred_neg &gt;= y_pred_pos).sum(dim=1)\n            ranking_list = 0.5 * (optimistic_rank + pessimistic_rank) + 1\n            hitsK_list = (ranking_list &lt;= k_value).to(torch.float)\n            mrr_list = 1./ranking_list.to(torch.float)\n\n            return {\n                    f'hits@{k_value}': hitsK_list.mean(),\n                    'mrr': mrr_list.mean()\n                    }\n\n        else:\n            y_pred_pos = y_pred_pos.reshape(-1, 1)\n            optimistic_rank = (y_pred_neg &gt; y_pred_pos).sum(axis=1)\n            pessimistic_rank = (y_pred_neg &gt;= y_pred_pos).sum(axis=1)\n            ranking_list = 0.5 * (optimistic_rank + pessimistic_rank) + 1\n            hitsK_list = (ranking_list &lt;= k_value).astype(np.float32)\n            mrr_list = 1./ranking_list.astype(np.float32)\n\n            return {\n                    f'hits@{k_value}': hitsK_list.mean(),\n                    'mrr': mrr_list.mean()\n                    }\n\n    def eval(self, \n             input_dict: dict, \n             verbose: bool = False) -&gt; dict:\n        r\"\"\"\n        evaluate the link prediction task\n        this method is callable through an instance of this object to compute the metric\n\n        Parameters:\n            input_dict: a dictionary containing \"y_pred_pos\", \"y_pred_neg\", and \"eval_metric\"\n                        the performance metric is calculated for the provided scores\n            verbose: whether to print out the computed metric\n\n        Returns:\n            perf_dict: a dictionary containing the computed performance metric\n        \"\"\"\n        y_pred_pos, y_pred_neg = self._parse_and_check_input(input_dict)  # convert the predictions to numpy\n        perf_dict = self._eval_hits_and_mrr(y_pred_pos, y_pred_neg, type_info='numpy', k_value=self.k_value)\n\n        return perf_dict\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.evaluate.Evaluator.__init__","title":"<code>__init__(name, k_value=10)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of the dataset</p> required <code>k_value</code> <code>int</code> <p>the desired 'k' value for calculating metric@k</p> <code>10</code> Source code in <code>tgb/linkproppred/evaluate.py</code> <pre><code>def __init__(self, name: str, k_value: int = 10):\n    r\"\"\"\n    Parameters:\n        name: name of the dataset\n        k_value: the desired 'k' value for calculating metric@k\n    \"\"\"\n    self.name = name\n    self.k_value = k_value  # for computing `hits@k`\n    self.valid_metric_list = ['hits@', 'mrr']\n    if self.name not in DATA_EVAL_METRIC_DICT:\n        raise NotImplementedError(\"Dataset not supported\")\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.evaluate.Evaluator.eval","title":"<code>eval(input_dict, verbose=False)</code>","text":"<p>evaluate the link prediction task this method is callable through an instance of this object to compute the metric</p> <p>Parameters:</p> Name Type Description Default <code>input_dict</code> <code>dict</code> <p>a dictionary containing \"y_pred_pos\", \"y_pred_neg\", and \"eval_metric\"         the performance metric is calculated for the provided scores</p> required <code>verbose</code> <code>bool</code> <p>whether to print out the computed metric</p> <code>False</code> <p>Returns:</p> Name Type Description <code>perf_dict</code> <code>dict</code> <p>a dictionary containing the computed performance metric</p> Source code in <code>tgb/linkproppred/evaluate.py</code> <pre><code>def eval(self, \n         input_dict: dict, \n         verbose: bool = False) -&gt; dict:\n    r\"\"\"\n    evaluate the link prediction task\n    this method is callable through an instance of this object to compute the metric\n\n    Parameters:\n        input_dict: a dictionary containing \"y_pred_pos\", \"y_pred_neg\", and \"eval_metric\"\n                    the performance metric is calculated for the provided scores\n        verbose: whether to print out the computed metric\n\n    Returns:\n        perf_dict: a dictionary containing the computed performance metric\n    \"\"\"\n    y_pred_pos, y_pred_neg = self._parse_and_check_input(input_dict)  # convert the predictions to numpy\n    perf_dict = self._eval_hits_and_mrr(y_pred_pos, y_pred_neg, type_info='numpy', k_value=self.k_value)\n\n    return perf_dict\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.negative_sampler.NegativeEdgeSampler","title":"<code>NegativeEdgeSampler</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>tgb/linkproppred/negative_sampler.py</code> <pre><code>class NegativeEdgeSampler(object):\n    def __init__(\n        self,\n        dataset_name: str,\n        first_dst_id: int = 0,\n        last_dst_id: int = 0,\n        strategy: str = \"hist_rnd\",\n    ) -&gt; None:\n        r\"\"\"\n        Negative Edge Sampler\n            Loads and query the negative batches based on the positive batches provided.\n        constructor for the negative edge sampler class\n\n        Parameters:\n            dataset_name: name of the dataset\n            first_dst_id: identity of the first destination node\n            last_dst_id: indentity of the last destination node\n            strategy: will always load the pre-generated negatives\n\n        Returns:\n            None\n        \"\"\"\n        self.dataset_name = dataset_name\n        assert strategy in [\n            \"rnd\",\n            \"hist_rnd\",\n        ], \"The supported strategies are `rnd` or `hist_rnd`!\"\n        self.strategy = strategy\n        self.eval_set = {}\n\n    def load_eval_set(\n        self,\n        fname: str,\n        split_mode: str = \"val\",\n    ) -&gt; None:\n        r\"\"\"\n        Load the evaluation set from disk, can be either val or test set ns samples\n        Parameters:\n            fname: the file name of the evaluation ns on disk\n            split_mode: the split mode of the evaluation set, can be either `val` or `test`\n\n        Returns:\n            None\n        \"\"\"\n        assert split_mode in [\n            \"val\",\n            \"test\",\n        ], \"Invalid split-mode! It should be `val`, `test`\"\n        if not os.path.exists(fname):\n            raise FileNotFoundError(f\"File not found at {fname}\")\n        self.eval_set[split_mode] = load_pkl(fname)\n\n    def reset_eval_set(self, \n                       split_mode: str = \"test\",\n                       ) -&gt; None:\n        r\"\"\"\n        Reset evaluation set\n\n        Parameters:\n            split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n\n        Returns:\n            None\n        \"\"\"\n        assert split_mode in [\n            \"val\",\n            \"test\",\n        ], \"Invalid split-mode! It should be `val`, `test`!\"\n        self.eval_set[split_mode] = None\n\n    def query_batch(self, \n                    pos_src: Tensor, \n                    pos_dst: Tensor, \n                    pos_timestamp: Tensor, \n                    edge_type: Tensor = None,\n                    split_mode: str = \"test\") -&gt; list:\n        r\"\"\"\n        For each positive edge in the `pos_batch`, return a list of negative edges\n        `split_mode` specifies whether the valiation or test evaluation set should be retrieved.\n        modify now to include edge type argument\n\n        Parameters:\n            pos_src: list of positive source nodes\n            pos_dst: list of positive destination nodes\n            pos_timestamp: list of timestamps of the positive edges\n            split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n\n        Returns:\n            neg_samples: a list of list; each internal list contains the set of negative edges that\n                        should be evaluated against each positive edge.\n        \"\"\"\n        assert split_mode in [\n            \"val\",\n            \"test\",\n        ], \"Invalid split-mode! It should be `val`, `test`!\"\n        if self.eval_set[split_mode] == None:\n            raise ValueError(\n                f\"Evaluation set is None! You should load the {split_mode} evaluation set first!\"\n            )\n\n        # check the argument types...\n        if torch is not None and isinstance(pos_src, torch.Tensor):\n            pos_src = pos_src.detach().cpu().numpy()\n        if torch is not None and isinstance(pos_dst, torch.Tensor):\n            pos_dst = pos_dst.detach().cpu().numpy()\n        if torch is not None and isinstance(pos_timestamp, torch.Tensor):\n            pos_timestamp = pos_timestamp.detach().cpu().numpy()\n        if torch is not None and isinstance(edge_type, torch.Tensor):\n            edge_type = edge_type.detach().cpu().numpy()\n\n        if not isinstance(pos_src, np.ndarray) or not isinstance(pos_dst, np.ndarray) or not(pos_timestamp, np.ndarray):\n            raise RuntimeError(\n                \"pos_src, pos_dst, and pos_timestamp need to be either numpy ndarray or torch tensor!\"\n                )\n\n        neg_samples = []\n        if (edge_type is None):\n            for pos_s, pos_d, pos_t in zip(pos_src, pos_dst, pos_timestamp):\n                if (pos_s, pos_d, pos_t) not in self.eval_set[split_mode]:\n                    raise ValueError(\n                        f\"The edge ({pos_s}, {pos_d}, {pos_t}) is not in the '{split_mode}' evaluation set! Please check the implementation.\"\n                    )\n                else:\n                    neg_samples.append(\n                        [\n                            int(neg_dst)\n                            for neg_dst in self.eval_set[split_mode][(pos_s, pos_d, pos_t)]\n                        ]\n                    )\n        else:\n            for pos_s, pos_d, pos_t, e_type in zip(pos_src, pos_dst, pos_timestamp, edge_type):\n                if (pos_s, pos_d, pos_t, e_type) not in self.eval_set[split_mode]:\n                    raise ValueError(\n                        f\"The edge ({pos_s}, {pos_d}, {pos_t}, {e_type}) is not in the '{split_mode}' evaluation set! Please check the implementation.\"\n                    )\n                else:\n                    neg_samples.append(\n                        [\n                            int(neg_dst)\n                            for neg_dst in self.eval_set[split_mode][(pos_s, pos_d, pos_t, e_type)]\n                        ]\n                    )\n\n        return neg_samples\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.negative_sampler.NegativeEdgeSampler.__init__","title":"<code>__init__(dataset_name, first_dst_id=0, last_dst_id=0, strategy='hist_rnd')</code>","text":"<p>Negative Edge Sampler     Loads and query the negative batches based on the positive batches provided. constructor for the negative edge sampler class</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>name of the dataset</p> required <code>first_dst_id</code> <code>int</code> <p>identity of the first destination node</p> <code>0</code> <code>last_dst_id</code> <code>int</code> <p>indentity of the last destination node</p> <code>0</code> <code>strategy</code> <code>str</code> <p>will always load the pre-generated negatives</p> <code>'hist_rnd'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>tgb/linkproppred/negative_sampler.py</code> <pre><code>def __init__(\n    self,\n    dataset_name: str,\n    first_dst_id: int = 0,\n    last_dst_id: int = 0,\n    strategy: str = \"hist_rnd\",\n) -&gt; None:\n    r\"\"\"\n    Negative Edge Sampler\n        Loads and query the negative batches based on the positive batches provided.\n    constructor for the negative edge sampler class\n\n    Parameters:\n        dataset_name: name of the dataset\n        first_dst_id: identity of the first destination node\n        last_dst_id: indentity of the last destination node\n        strategy: will always load the pre-generated negatives\n\n    Returns:\n        None\n    \"\"\"\n    self.dataset_name = dataset_name\n    assert strategy in [\n        \"rnd\",\n        \"hist_rnd\",\n    ], \"The supported strategies are `rnd` or `hist_rnd`!\"\n    self.strategy = strategy\n    self.eval_set = {}\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.negative_sampler.NegativeEdgeSampler.load_eval_set","title":"<code>load_eval_set(fname, split_mode='val')</code>","text":"<p>Load the evaluation set from disk, can be either val or test set ns samples Parameters:     fname: the file name of the evaluation ns on disk     split_mode: the split mode of the evaluation set, can be either <code>val</code> or <code>test</code></p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>tgb/linkproppred/negative_sampler.py</code> <pre><code>def load_eval_set(\n    self,\n    fname: str,\n    split_mode: str = \"val\",\n) -&gt; None:\n    r\"\"\"\n    Load the evaluation set from disk, can be either val or test set ns samples\n    Parameters:\n        fname: the file name of the evaluation ns on disk\n        split_mode: the split mode of the evaluation set, can be either `val` or `test`\n\n    Returns:\n        None\n    \"\"\"\n    assert split_mode in [\n        \"val\",\n        \"test\",\n    ], \"Invalid split-mode! It should be `val`, `test`\"\n    if not os.path.exists(fname):\n        raise FileNotFoundError(f\"File not found at {fname}\")\n    self.eval_set[split_mode] = load_pkl(fname)\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.negative_sampler.NegativeEdgeSampler.query_batch","title":"<code>query_batch(pos_src, pos_dst, pos_timestamp, edge_type=None, split_mode='test')</code>","text":"<p>For each positive edge in the <code>pos_batch</code>, return a list of negative edges <code>split_mode</code> specifies whether the valiation or test evaluation set should be retrieved. modify now to include edge type argument</p> <p>Parameters:</p> Name Type Description Default <code>pos_src</code> <code>Tensor</code> <p>list of positive source nodes</p> required <code>pos_dst</code> <code>Tensor</code> <p>list of positive destination nodes</p> required <code>pos_timestamp</code> <code>Tensor</code> <p>list of timestamps of the positive edges</p> required <code>split_mode</code> <code>str</code> <p>specifies whether to generate negative edges for 'validation' or 'test' splits</p> <code>'test'</code> <p>Returns:</p> Name Type Description <code>neg_samples</code> <code>list</code> <p>a list of list; each internal list contains the set of negative edges that         should be evaluated against each positive edge.</p> Source code in <code>tgb/linkproppred/negative_sampler.py</code> <pre><code>def query_batch(self, \n                pos_src: Tensor, \n                pos_dst: Tensor, \n                pos_timestamp: Tensor, \n                edge_type: Tensor = None,\n                split_mode: str = \"test\") -&gt; list:\n    r\"\"\"\n    For each positive edge in the `pos_batch`, return a list of negative edges\n    `split_mode` specifies whether the valiation or test evaluation set should be retrieved.\n    modify now to include edge type argument\n\n    Parameters:\n        pos_src: list of positive source nodes\n        pos_dst: list of positive destination nodes\n        pos_timestamp: list of timestamps of the positive edges\n        split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n\n    Returns:\n        neg_samples: a list of list; each internal list contains the set of negative edges that\n                    should be evaluated against each positive edge.\n    \"\"\"\n    assert split_mode in [\n        \"val\",\n        \"test\",\n    ], \"Invalid split-mode! It should be `val`, `test`!\"\n    if self.eval_set[split_mode] == None:\n        raise ValueError(\n            f\"Evaluation set is None! You should load the {split_mode} evaluation set first!\"\n        )\n\n    # check the argument types...\n    if torch is not None and isinstance(pos_src, torch.Tensor):\n        pos_src = pos_src.detach().cpu().numpy()\n    if torch is not None and isinstance(pos_dst, torch.Tensor):\n        pos_dst = pos_dst.detach().cpu().numpy()\n    if torch is not None and isinstance(pos_timestamp, torch.Tensor):\n        pos_timestamp = pos_timestamp.detach().cpu().numpy()\n    if torch is not None and isinstance(edge_type, torch.Tensor):\n        edge_type = edge_type.detach().cpu().numpy()\n\n    if not isinstance(pos_src, np.ndarray) or not isinstance(pos_dst, np.ndarray) or not(pos_timestamp, np.ndarray):\n        raise RuntimeError(\n            \"pos_src, pos_dst, and pos_timestamp need to be either numpy ndarray or torch tensor!\"\n            )\n\n    neg_samples = []\n    if (edge_type is None):\n        for pos_s, pos_d, pos_t in zip(pos_src, pos_dst, pos_timestamp):\n            if (pos_s, pos_d, pos_t) not in self.eval_set[split_mode]:\n                raise ValueError(\n                    f\"The edge ({pos_s}, {pos_d}, {pos_t}) is not in the '{split_mode}' evaluation set! Please check the implementation.\"\n                )\n            else:\n                neg_samples.append(\n                    [\n                        int(neg_dst)\n                        for neg_dst in self.eval_set[split_mode][(pos_s, pos_d, pos_t)]\n                    ]\n                )\n    else:\n        for pos_s, pos_d, pos_t, e_type in zip(pos_src, pos_dst, pos_timestamp, edge_type):\n            if (pos_s, pos_d, pos_t, e_type) not in self.eval_set[split_mode]:\n                raise ValueError(\n                    f\"The edge ({pos_s}, {pos_d}, {pos_t}, {e_type}) is not in the '{split_mode}' evaluation set! Please check the implementation.\"\n                )\n            else:\n                neg_samples.append(\n                    [\n                        int(neg_dst)\n                        for neg_dst in self.eval_set[split_mode][(pos_s, pos_d, pos_t, e_type)]\n                    ]\n                )\n\n    return neg_samples\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.negative_sampler.NegativeEdgeSampler.reset_eval_set","title":"<code>reset_eval_set(split_mode='test')</code>","text":"<p>Reset evaluation set</p> <p>Parameters:</p> Name Type Description Default <code>split_mode</code> <code>str</code> <p>specifies whether to generate negative edges for 'validation' or 'test' splits</p> <code>'test'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>tgb/linkproppred/negative_sampler.py</code> <pre><code>def reset_eval_set(self, \n                   split_mode: str = \"test\",\n                   ) -&gt; None:\n    r\"\"\"\n    Reset evaluation set\n\n    Parameters:\n        split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n\n    Returns:\n        None\n    \"\"\"\n    assert split_mode in [\n        \"val\",\n        \"test\",\n    ], \"Invalid split-mode! It should be `val`, `test`!\"\n    self.eval_set[split_mode] = None\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.negative_generator.NegativeEdgeGenerator","title":"<code>NegativeEdgeGenerator</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>tgb/linkproppred/negative_generator.py</code> <pre><code>class NegativeEdgeGenerator(object):\n    def __init__(\n        self,\n        dataset_name: str,\n        first_dst_id: int,\n        last_dst_id: int,\n        num_neg_e: int = 100,  # number of negative edges sampled per positive edges --&gt; make it constant =&gt; 1000\n        strategy: str = \"rnd\",\n        rnd_seed: int = 123,\n        hist_ratio: float = 0.5,\n        historical_data: TemporalData = None,\n    ) -&gt; None:\n        r\"\"\"\n        Negative Edge Sampler class\n        this is a class for generating negative samples for a specific datasets\n        the set of the positive samples are provided, the negative samples are generated with specific strategies \n        and are saved for consistent evaluation across different methods\n        negative edges are sampled with 'oen_vs_many' strategy.\n        it is assumed that the destination nodes are indexed sequentially with 'first_dst_id' \n        and 'last_dst_id' being the first and last index, respectively.\n\n        Parameters:\n            dataset_name: name of the dataset\n            first_dst_id: identity of the first destination node\n            last_dst_id: indentity of the last destination node\n            num_neg_e: number of negative edges being generated per each positive edge\n            strategy: how to generate negative edges; can be 'rnd' or 'hist_rnd'\n            rnd_seed: random seed for consistency\n            hist_ratio: if the startegy is 'hist_rnd', how much of the negatives are historical\n            historical_data: previous records of the positive edges\n\n        Returns:\n            None\n        \"\"\"\n        self.rnd_seed = rnd_seed\n        np.random.seed(self.rnd_seed)\n        self.dataset_name = dataset_name\n\n        self.first_dst_id = first_dst_id\n        self.last_dst_id = last_dst_id\n        self.num_neg_e = num_neg_e\n        assert strategy in [\n            \"rnd\",\n            \"hist_rnd\",\n        ], \"The supported strategies are `rnd` or `hist_rnd`!\"\n        self.strategy = strategy\n        if self.strategy == \"hist_rnd\":\n            assert (\n                historical_data != None\n            ), \"Train data should be passed when `hist_rnd` strategy is selected.\"\n            self.hist_ratio = hist_ratio\n            self.historical_data = historical_data\n\n    def generate_negative_samples(self, \n                                  data: TemporalData, \n                                  split_mode: str, \n                                  partial_path: str,\n                                  ) -&gt; None:\n        r\"\"\"\n        Generate negative samples\n\n        Parameters:\n            data: an object containing positive edges information\n            split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n            partial_path: in which directory save the generated negatives\n        \"\"\"\n        # file name for saving or loading...\n        filename = (\n            partial_path\n            + \"/\"\n            + self.dataset_name\n            + \"_\"\n            + split_mode\n            + \"_\"\n            + \"ns\"\n            + \".pkl\"\n        )\n\n        if self.strategy == \"rnd\":\n            self.generate_negative_samples_rnd(data, split_mode, filename)\n        elif self.strategy == \"hist_rnd\":\n            self.generate_negative_samples_hist_rnd(\n                self.historical_data, data, split_mode, filename\n            )\n        else:\n            raise ValueError(\"Unsupported negative sample generation strategy!\")\n\n    def generate_negative_samples_rnd(self, \n                                      data: TemporalData, \n                                      split_mode: str, \n                                      filename: str,\n                                      ) -&gt; None:\n        r\"\"\"\n        Generate negative samples based on the `HIST-RND` strategy:\n            - for each positive edge, sample a batch of negative edges from all possible edges with the same source node\n            - filter actual positive edges\n\n        Parameters:\n            data: an object containing positive edges information\n            split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n            filename: name of the file containing the generated negative edges\n        \"\"\"\n        print(\n            f\"INFO: Negative Sampling Strategy: {self.strategy}, Data Split: {split_mode}\"\n        )\n        assert split_mode in [\n            \"val\",\n            \"test\",\n        ], \"Invalid split-mode! It should be `val` or `test`!\"\n\n        if os.path.exists(filename):\n            print(\n                f\"INFO: negative samples for '{split_mode}' evaluation are already generated!\"\n            )\n        else:\n            print(f\"INFO: Generating negative samples for '{split_mode}' evaluation!\")\n            # retrieve the information from the batch\n            pos_src, pos_dst, pos_timestamp = (\n                data.src.cpu().numpy(),\n                data.dst.cpu().numpy(),\n                data.t.cpu().numpy(),\n            )\n\n            # all possible destinations\n            all_dst = np.arange(self.first_dst_id, self.last_dst_id + 1)\n\n            evaluation_set = {}\n            # generate a list of negative destinations for each positive edge\n            pos_edge_tqdm = tqdm(\n                zip(pos_src, pos_dst, pos_timestamp), total=len(pos_src)\n            )\n            for (\n                pos_s,\n                pos_d,\n                pos_t,\n            ) in pos_edge_tqdm:\n                t_mask = pos_timestamp == pos_t\n                src_mask = pos_src == pos_s\n                fn_mask = np.logical_and(t_mask, src_mask)\n                pos_e_dst_same_src = pos_dst[fn_mask]\n                filtered_all_dst = np.setdiff1d(all_dst, pos_e_dst_same_src)\n\n                '''\n                when num_neg_e is larger than all possible destinations simple return all possible destinations\n                '''\n                if (self.num_neg_e &gt; len(filtered_all_dst)):\n                    neg_d_arr = filtered_all_dst\n                else:\n                    neg_d_arr = np.random.choice(\n                    filtered_all_dst, self.num_neg_e, replace=False) #never replace negatives\n\n                evaluation_set[(pos_s, pos_d, pos_t)] = neg_d_arr\n\n            # save the generated evaluation set to disk\n            save_pkl(evaluation_set, filename)\n\n    def generate_historical_edge_set(self, \n                                     historical_data: TemporalData,\n                                     ) -&gt; tuple:\n        r\"\"\"\n        Generate the set of edges seen durign training or validation\n\n        ONLY `train_data` should be passed as historical data; i.e., the HISTORICAL negative edges should be selected from training data only.\n\n        Parameters:\n            historical_data: contains the positive edges observed previously\n\n        Returns:\n            historical_edges: distict historical positive edges\n            hist_edge_set_per_node: historical edges observed for each node\n        \"\"\"\n        sources = historical_data.src.cpu().numpy()\n        destinations = historical_data.dst.cpu().numpy()\n        historical_edges = {}\n        hist_e_per_node = {}\n        for src, dst in zip(sources, destinations):\n            # edge-centric\n            if (src, dst) not in historical_edges:\n                historical_edges[(src, dst)] = 1\n\n            # node-centric\n            if src not in hist_e_per_node:\n                hist_e_per_node[src] = [dst]\n            else:\n                hist_e_per_node[src].append(dst)\n\n        hist_edge_set_per_node = {}\n        for src, dst_list in hist_e_per_node.items():\n            hist_edge_set_per_node[src] = np.array(list(set(dst_list)))\n\n        return historical_edges, hist_edge_set_per_node\n\n    def generate_negative_samples_hist_rnd(\n        self, \n        historical_data : TemporalData, \n        data: TemporalData, \n        split_mode: str, \n        filename: str,\n    ) -&gt; None:\n        r\"\"\"\n        Generate negative samples based on the `HIST-RND` strategy:\n            - up to 50% of the negative samples are selected from the set of edges seen during the training with the same source node.\n            - the rest of the negative edges are randomly sampled with the fixed source node.\n\n        Parameters:\n            historical_data: contains the history of the observed positive edges including \n                            distinct positive edges and edges observed for each positive node\n            data: an object containing positive edges information\n            split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n            filename: name of the file to save generated negative edges\n\n        Returns:\n            None\n        \"\"\"\n        print(\n            f\"INFO: Negative Sampling Strategy: {self.strategy}, Data Split: {split_mode}\"\n        )\n        assert split_mode in [\n            \"val\",\n            \"test\",\n        ], \"Invalid split-mode! It should be `val` or `test`!\"\n\n        if os.path.exists(filename):\n            print(\n                f\"INFO: negative samples for '{split_mode}' evaluation are already generated!\"\n            )\n        else:\n            print(f\"INFO: Generating negative samples for '{split_mode}' evaluation!\")\n            # retrieve the information from the batch\n            pos_src, pos_dst, pos_timestamp = (\n                data.src.cpu().numpy(),\n                data.dst.cpu().numpy(),\n                data.t.cpu().numpy(),\n            )\n\n            pos_ts_edge_dict = {} #{ts: {src: [dsts]}}\n            pos_edge_tqdm = tqdm(\n                zip(pos_src, pos_dst, pos_timestamp), total=len(pos_src)\n            )\n            for (\n                pos_s,\n                pos_d,\n                pos_t,\n            ) in pos_edge_tqdm:\n                if (pos_t not in pos_ts_edge_dict):\n                    pos_ts_edge_dict[pos_t] = {pos_s: [pos_d]}\n                else:\n                    if (pos_s not in pos_ts_edge_dict[pos_t]):\n                        pos_ts_edge_dict[pos_t][pos_s] = [pos_d]\n                    else:\n                        pos_ts_edge_dict[pos_t][pos_s].append(pos_d)\n\n            # all possible destinations\n            all_dst = np.arange(self.first_dst_id, self.last_dst_id + 1)\n\n            # get seen edge history\n            (\n                historical_edges,\n                hist_edge_set_per_node,\n            ) = self.generate_historical_edge_set(historical_data)\n\n            # sample historical edges\n            max_num_hist_neg_e = int(self.num_neg_e * self.hist_ratio)\n\n            evaluation_set = {}\n            # generate a list of negative destinations for each positive edge\n            pos_edge_tqdm = tqdm(\n                zip(pos_src, pos_dst, pos_timestamp), total=len(pos_src)\n            )\n            for (\n                pos_s,\n                pos_d,\n                pos_t,\n            ) in pos_edge_tqdm:\n                pos_e_dst_same_src = np.array(pos_ts_edge_dict[pos_t][pos_s])\n\n                # sample historical edges\n                num_hist_neg_e = 0\n                neg_hist_dsts = np.array([])\n                seen_dst = []\n                if pos_s in hist_edge_set_per_node:\n                    seen_dst = hist_edge_set_per_node[pos_s]\n                    if len(seen_dst) &gt;= 1:\n                        filtered_all_seen_dst = np.setdiff1d(seen_dst, pos_e_dst_same_src)\n                        #filtered_all_seen_dst = seen_dst #! no collision check\n                        num_hist_neg_e = (\n                            max_num_hist_neg_e\n                            if max_num_hist_neg_e &lt;= len(filtered_all_seen_dst)\n                            else len(filtered_all_seen_dst)\n                        )\n                        neg_hist_dsts = np.random.choice(\n                            filtered_all_seen_dst, num_hist_neg_e, replace=False\n                        )\n\n                # sample random edges\n                if (len(seen_dst) &gt;= 1):\n                    invalid_dst = np.concatenate((np.array(pos_e_dst_same_src), seen_dst))\n                else:\n                    invalid_dst = np.array(pos_e_dst_same_src)\n                filtered_all_rnd_dst = np.setdiff1d(all_dst, invalid_dst)\n\n                num_rnd_neg_e = self.num_neg_e - num_hist_neg_e\n                '''\n                when num_neg_e is larger than all possible destinations simple return all possible destinations\n                '''\n                if (num_rnd_neg_e &gt; len(filtered_all_rnd_dst)):\n                    neg_rnd_dsts = filtered_all_rnd_dst\n                else:\n                    neg_rnd_dsts = np.random.choice(\n                    filtered_all_rnd_dst, num_rnd_neg_e, replace=False\n                )\n                # concatenate the two sets: historical and random\n                neg_dst_arr = np.concatenate((neg_hist_dsts, neg_rnd_dsts))\n                evaluation_set[(pos_s, pos_d, pos_t)] = neg_dst_arr\n\n            # save the generated evaluation set to disk\n            save_pkl(evaluation_set, filename)\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.negative_generator.NegativeEdgeGenerator.__init__","title":"<code>__init__(dataset_name, first_dst_id, last_dst_id, num_neg_e=100, strategy='rnd', rnd_seed=123, hist_ratio=0.5, historical_data=None)</code>","text":"<p>Negative Edge Sampler class this is a class for generating negative samples for a specific datasets the set of the positive samples are provided, the negative samples are generated with specific strategies  and are saved for consistent evaluation across different methods negative edges are sampled with 'oen_vs_many' strategy. it is assumed that the destination nodes are indexed sequentially with 'first_dst_id'  and 'last_dst_id' being the first and last index, respectively.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>name of the dataset</p> required <code>first_dst_id</code> <code>int</code> <p>identity of the first destination node</p> required <code>last_dst_id</code> <code>int</code> <p>indentity of the last destination node</p> required <code>num_neg_e</code> <code>int</code> <p>number of negative edges being generated per each positive edge</p> <code>100</code> <code>strategy</code> <code>str</code> <p>how to generate negative edges; can be 'rnd' or 'hist_rnd'</p> <code>'rnd'</code> <code>rnd_seed</code> <code>int</code> <p>random seed for consistency</p> <code>123</code> <code>hist_ratio</code> <code>float</code> <p>if the startegy is 'hist_rnd', how much of the negatives are historical</p> <code>0.5</code> <code>historical_data</code> <code>TemporalData</code> <p>previous records of the positive edges</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>tgb/linkproppred/negative_generator.py</code> <pre><code>def __init__(\n    self,\n    dataset_name: str,\n    first_dst_id: int,\n    last_dst_id: int,\n    num_neg_e: int = 100,  # number of negative edges sampled per positive edges --&gt; make it constant =&gt; 1000\n    strategy: str = \"rnd\",\n    rnd_seed: int = 123,\n    hist_ratio: float = 0.5,\n    historical_data: TemporalData = None,\n) -&gt; None:\n    r\"\"\"\n    Negative Edge Sampler class\n    this is a class for generating negative samples for a specific datasets\n    the set of the positive samples are provided, the negative samples are generated with specific strategies \n    and are saved for consistent evaluation across different methods\n    negative edges are sampled with 'oen_vs_many' strategy.\n    it is assumed that the destination nodes are indexed sequentially with 'first_dst_id' \n    and 'last_dst_id' being the first and last index, respectively.\n\n    Parameters:\n        dataset_name: name of the dataset\n        first_dst_id: identity of the first destination node\n        last_dst_id: indentity of the last destination node\n        num_neg_e: number of negative edges being generated per each positive edge\n        strategy: how to generate negative edges; can be 'rnd' or 'hist_rnd'\n        rnd_seed: random seed for consistency\n        hist_ratio: if the startegy is 'hist_rnd', how much of the negatives are historical\n        historical_data: previous records of the positive edges\n\n    Returns:\n        None\n    \"\"\"\n    self.rnd_seed = rnd_seed\n    np.random.seed(self.rnd_seed)\n    self.dataset_name = dataset_name\n\n    self.first_dst_id = first_dst_id\n    self.last_dst_id = last_dst_id\n    self.num_neg_e = num_neg_e\n    assert strategy in [\n        \"rnd\",\n        \"hist_rnd\",\n    ], \"The supported strategies are `rnd` or `hist_rnd`!\"\n    self.strategy = strategy\n    if self.strategy == \"hist_rnd\":\n        assert (\n            historical_data != None\n        ), \"Train data should be passed when `hist_rnd` strategy is selected.\"\n        self.hist_ratio = hist_ratio\n        self.historical_data = historical_data\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.negative_generator.NegativeEdgeGenerator.generate_historical_edge_set","title":"<code>generate_historical_edge_set(historical_data)</code>","text":"<p>Generate the set of edges seen durign training or validation</p> <p>ONLY <code>train_data</code> should be passed as historical data; i.e., the HISTORICAL negative edges should be selected from training data only.</p> <p>Parameters:</p> Name Type Description Default <code>historical_data</code> <code>TemporalData</code> <p>contains the positive edges observed previously</p> required <p>Returns:</p> Name Type Description <code>historical_edges</code> <code>tuple</code> <p>distict historical positive edges</p> <code>hist_edge_set_per_node</code> <code>tuple</code> <p>historical edges observed for each node</p> Source code in <code>tgb/linkproppred/negative_generator.py</code> <pre><code>def generate_historical_edge_set(self, \n                                 historical_data: TemporalData,\n                                 ) -&gt; tuple:\n    r\"\"\"\n    Generate the set of edges seen durign training or validation\n\n    ONLY `train_data` should be passed as historical data; i.e., the HISTORICAL negative edges should be selected from training data only.\n\n    Parameters:\n        historical_data: contains the positive edges observed previously\n\n    Returns:\n        historical_edges: distict historical positive edges\n        hist_edge_set_per_node: historical edges observed for each node\n    \"\"\"\n    sources = historical_data.src.cpu().numpy()\n    destinations = historical_data.dst.cpu().numpy()\n    historical_edges = {}\n    hist_e_per_node = {}\n    for src, dst in zip(sources, destinations):\n        # edge-centric\n        if (src, dst) not in historical_edges:\n            historical_edges[(src, dst)] = 1\n\n        # node-centric\n        if src not in hist_e_per_node:\n            hist_e_per_node[src] = [dst]\n        else:\n            hist_e_per_node[src].append(dst)\n\n    hist_edge_set_per_node = {}\n    for src, dst_list in hist_e_per_node.items():\n        hist_edge_set_per_node[src] = np.array(list(set(dst_list)))\n\n    return historical_edges, hist_edge_set_per_node\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.negative_generator.NegativeEdgeGenerator.generate_negative_samples","title":"<code>generate_negative_samples(data, split_mode, partial_path)</code>","text":"<p>Generate negative samples</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TemporalData</code> <p>an object containing positive edges information</p> required <code>split_mode</code> <code>str</code> <p>specifies whether to generate negative edges for 'validation' or 'test' splits</p> required <code>partial_path</code> <code>str</code> <p>in which directory save the generated negatives</p> required Source code in <code>tgb/linkproppred/negative_generator.py</code> <pre><code>def generate_negative_samples(self, \n                              data: TemporalData, \n                              split_mode: str, \n                              partial_path: str,\n                              ) -&gt; None:\n    r\"\"\"\n    Generate negative samples\n\n    Parameters:\n        data: an object containing positive edges information\n        split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n        partial_path: in which directory save the generated negatives\n    \"\"\"\n    # file name for saving or loading...\n    filename = (\n        partial_path\n        + \"/\"\n        + self.dataset_name\n        + \"_\"\n        + split_mode\n        + \"_\"\n        + \"ns\"\n        + \".pkl\"\n    )\n\n    if self.strategy == \"rnd\":\n        self.generate_negative_samples_rnd(data, split_mode, filename)\n    elif self.strategy == \"hist_rnd\":\n        self.generate_negative_samples_hist_rnd(\n            self.historical_data, data, split_mode, filename\n        )\n    else:\n        raise ValueError(\"Unsupported negative sample generation strategy!\")\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.negative_generator.NegativeEdgeGenerator.generate_negative_samples_hist_rnd","title":"<code>generate_negative_samples_hist_rnd(historical_data, data, split_mode, filename)</code>","text":"<p>Generate negative samples based on the <code>HIST-RND</code> strategy:     - up to 50% of the negative samples are selected from the set of edges seen during the training with the same source node.     - the rest of the negative edges are randomly sampled with the fixed source node.</p> <p>Parameters:</p> Name Type Description Default <code>historical_data</code> <code>TemporalData</code> <p>contains the history of the observed positive edges including              distinct positive edges and edges observed for each positive node</p> required <code>data</code> <code>TemporalData</code> <p>an object containing positive edges information</p> required <code>split_mode</code> <code>str</code> <p>specifies whether to generate negative edges for 'validation' or 'test' splits</p> required <code>filename</code> <code>str</code> <p>name of the file to save generated negative edges</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>tgb/linkproppred/negative_generator.py</code> <pre><code>def generate_negative_samples_hist_rnd(\n    self, \n    historical_data : TemporalData, \n    data: TemporalData, \n    split_mode: str, \n    filename: str,\n) -&gt; None:\n    r\"\"\"\n    Generate negative samples based on the `HIST-RND` strategy:\n        - up to 50% of the negative samples are selected from the set of edges seen during the training with the same source node.\n        - the rest of the negative edges are randomly sampled with the fixed source node.\n\n    Parameters:\n        historical_data: contains the history of the observed positive edges including \n                        distinct positive edges and edges observed for each positive node\n        data: an object containing positive edges information\n        split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n        filename: name of the file to save generated negative edges\n\n    Returns:\n        None\n    \"\"\"\n    print(\n        f\"INFO: Negative Sampling Strategy: {self.strategy}, Data Split: {split_mode}\"\n    )\n    assert split_mode in [\n        \"val\",\n        \"test\",\n    ], \"Invalid split-mode! It should be `val` or `test`!\"\n\n    if os.path.exists(filename):\n        print(\n            f\"INFO: negative samples for '{split_mode}' evaluation are already generated!\"\n        )\n    else:\n        print(f\"INFO: Generating negative samples for '{split_mode}' evaluation!\")\n        # retrieve the information from the batch\n        pos_src, pos_dst, pos_timestamp = (\n            data.src.cpu().numpy(),\n            data.dst.cpu().numpy(),\n            data.t.cpu().numpy(),\n        )\n\n        pos_ts_edge_dict = {} #{ts: {src: [dsts]}}\n        pos_edge_tqdm = tqdm(\n            zip(pos_src, pos_dst, pos_timestamp), total=len(pos_src)\n        )\n        for (\n            pos_s,\n            pos_d,\n            pos_t,\n        ) in pos_edge_tqdm:\n            if (pos_t not in pos_ts_edge_dict):\n                pos_ts_edge_dict[pos_t] = {pos_s: [pos_d]}\n            else:\n                if (pos_s not in pos_ts_edge_dict[pos_t]):\n                    pos_ts_edge_dict[pos_t][pos_s] = [pos_d]\n                else:\n                    pos_ts_edge_dict[pos_t][pos_s].append(pos_d)\n\n        # all possible destinations\n        all_dst = np.arange(self.first_dst_id, self.last_dst_id + 1)\n\n        # get seen edge history\n        (\n            historical_edges,\n            hist_edge_set_per_node,\n        ) = self.generate_historical_edge_set(historical_data)\n\n        # sample historical edges\n        max_num_hist_neg_e = int(self.num_neg_e * self.hist_ratio)\n\n        evaluation_set = {}\n        # generate a list of negative destinations for each positive edge\n        pos_edge_tqdm = tqdm(\n            zip(pos_src, pos_dst, pos_timestamp), total=len(pos_src)\n        )\n        for (\n            pos_s,\n            pos_d,\n            pos_t,\n        ) in pos_edge_tqdm:\n            pos_e_dst_same_src = np.array(pos_ts_edge_dict[pos_t][pos_s])\n\n            # sample historical edges\n            num_hist_neg_e = 0\n            neg_hist_dsts = np.array([])\n            seen_dst = []\n            if pos_s in hist_edge_set_per_node:\n                seen_dst = hist_edge_set_per_node[pos_s]\n                if len(seen_dst) &gt;= 1:\n                    filtered_all_seen_dst = np.setdiff1d(seen_dst, pos_e_dst_same_src)\n                    #filtered_all_seen_dst = seen_dst #! no collision check\n                    num_hist_neg_e = (\n                        max_num_hist_neg_e\n                        if max_num_hist_neg_e &lt;= len(filtered_all_seen_dst)\n                        else len(filtered_all_seen_dst)\n                    )\n                    neg_hist_dsts = np.random.choice(\n                        filtered_all_seen_dst, num_hist_neg_e, replace=False\n                    )\n\n            # sample random edges\n            if (len(seen_dst) &gt;= 1):\n                invalid_dst = np.concatenate((np.array(pos_e_dst_same_src), seen_dst))\n            else:\n                invalid_dst = np.array(pos_e_dst_same_src)\n            filtered_all_rnd_dst = np.setdiff1d(all_dst, invalid_dst)\n\n            num_rnd_neg_e = self.num_neg_e - num_hist_neg_e\n            '''\n            when num_neg_e is larger than all possible destinations simple return all possible destinations\n            '''\n            if (num_rnd_neg_e &gt; len(filtered_all_rnd_dst)):\n                neg_rnd_dsts = filtered_all_rnd_dst\n            else:\n                neg_rnd_dsts = np.random.choice(\n                filtered_all_rnd_dst, num_rnd_neg_e, replace=False\n            )\n            # concatenate the two sets: historical and random\n            neg_dst_arr = np.concatenate((neg_hist_dsts, neg_rnd_dsts))\n            evaluation_set[(pos_s, pos_d, pos_t)] = neg_dst_arr\n\n        # save the generated evaluation set to disk\n        save_pkl(evaluation_set, filename)\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.negative_generator.NegativeEdgeGenerator.generate_negative_samples_rnd","title":"<code>generate_negative_samples_rnd(data, split_mode, filename)</code>","text":"<p>Generate negative samples based on the <code>HIST-RND</code> strategy:     - for each positive edge, sample a batch of negative edges from all possible edges with the same source node     - filter actual positive edges</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TemporalData</code> <p>an object containing positive edges information</p> required <code>split_mode</code> <code>str</code> <p>specifies whether to generate negative edges for 'validation' or 'test' splits</p> required <code>filename</code> <code>str</code> <p>name of the file containing the generated negative edges</p> required Source code in <code>tgb/linkproppred/negative_generator.py</code> <pre><code>def generate_negative_samples_rnd(self, \n                                  data: TemporalData, \n                                  split_mode: str, \n                                  filename: str,\n                                  ) -&gt; None:\n    r\"\"\"\n    Generate negative samples based on the `HIST-RND` strategy:\n        - for each positive edge, sample a batch of negative edges from all possible edges with the same source node\n        - filter actual positive edges\n\n    Parameters:\n        data: an object containing positive edges information\n        split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n        filename: name of the file containing the generated negative edges\n    \"\"\"\n    print(\n        f\"INFO: Negative Sampling Strategy: {self.strategy}, Data Split: {split_mode}\"\n    )\n    assert split_mode in [\n        \"val\",\n        \"test\",\n    ], \"Invalid split-mode! It should be `val` or `test`!\"\n\n    if os.path.exists(filename):\n        print(\n            f\"INFO: negative samples for '{split_mode}' evaluation are already generated!\"\n        )\n    else:\n        print(f\"INFO: Generating negative samples for '{split_mode}' evaluation!\")\n        # retrieve the information from the batch\n        pos_src, pos_dst, pos_timestamp = (\n            data.src.cpu().numpy(),\n            data.dst.cpu().numpy(),\n            data.t.cpu().numpy(),\n        )\n\n        # all possible destinations\n        all_dst = np.arange(self.first_dst_id, self.last_dst_id + 1)\n\n        evaluation_set = {}\n        # generate a list of negative destinations for each positive edge\n        pos_edge_tqdm = tqdm(\n            zip(pos_src, pos_dst, pos_timestamp), total=len(pos_src)\n        )\n        for (\n            pos_s,\n            pos_d,\n            pos_t,\n        ) in pos_edge_tqdm:\n            t_mask = pos_timestamp == pos_t\n            src_mask = pos_src == pos_s\n            fn_mask = np.logical_and(t_mask, src_mask)\n            pos_e_dst_same_src = pos_dst[fn_mask]\n            filtered_all_dst = np.setdiff1d(all_dst, pos_e_dst_same_src)\n\n            '''\n            when num_neg_e is larger than all possible destinations simple return all possible destinations\n            '''\n            if (self.num_neg_e &gt; len(filtered_all_dst)):\n                neg_d_arr = filtered_all_dst\n            else:\n                neg_d_arr = np.random.choice(\n                filtered_all_dst, self.num_neg_e, replace=False) #never replace negatives\n\n            evaluation_set[(pos_s, pos_d, pos_t)] = neg_d_arr\n\n        # save the generated evaluation set to disk\n        save_pkl(evaluation_set, filename)\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.tkg_negative_generator.TKGNegativeEdgeGenerator","title":"<code>TKGNegativeEdgeGenerator</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>tgb/linkproppred/tkg_negative_generator.py</code> <pre><code>class TKGNegativeEdgeGenerator(object):\n    def __init__(\n        self,\n        dataset_name: str,\n        first_dst_id: int,\n        last_dst_id: int,\n        strategy: str = \"time-filtered\",\n        num_neg_e: int = -1,  # -1 means generate all possible negatives\n        rnd_seed: int = 1,\n        partial_path: str = None,\n        edge_data: TemporalData = None,\n    ) -&gt; None:\n        r\"\"\"\n        Negative Edge Generator class for Temporal Knowledge Graphs\n        constructor for the negative edge generator class\n\n        Parameters:\n            dataset_name: name of the dataset\n            first_dst_id: identity of the first destination node\n            last_dst_id: indentity of the last destination node\n            num_neg_e: number of negative edges being generated per each positive edge\n            strategy: specifies which strategy should be used for generating the negatives\n            rnd_seed: random seed for reproducibility\n            edge_data: the positive edges to generate the negatives for, assuming sorted temporally\n\n        Returns:\n            None\n        \"\"\"\n        self.rnd_seed = rnd_seed\n        np.random.seed(self.rnd_seed)\n        self.dataset_name = dataset_name\n        self.first_dst_id = first_dst_id\n        self.last_dst_id = last_dst_id      \n        self.num_neg_e = num_neg_e  #-1 means generate all \n        assert strategy in [\n            \"time-filtered\",\n            \"dst-time-filtered\",\n            \"random\"\n        ], \"The supported strategies are `time-filtered`, dst-time-filtered, random\"\n        self.strategy = strategy\n        self.dst_dict = None\n        if self.strategy == \"dst-time-filtered\":\n            if partial_path is None:\n                raise ValueError(\n                    \"The partial path to the directory where the dst_dict is stored is required\")\n            else:\n                self.dst_dict_name = (\n                    partial_path\n                    + \"/\"\n                    + self.dataset_name\n                    + \"_\"\n                    + \"dst_dict\"\n                    + \".pkl\"\n                )\n                self.dst_dict = self.generate_dst_dict(edge_data=edge_data, dst_name=self.dst_dict_name)\n        self.edge_data = edge_data\n\n    def generate_dst_dict(self, edge_data: TemporalData, dst_name: str) -&gt; dict:\n        r\"\"\"\n        Generate a dictionary of destination nodes for each type of edge\n\n        Parameters:\n            edge_data: an object containing positive edges information\n            dst_name: name of the file to save the generated dictionary of destination nodes\n\n        Returns:\n            dst_dict: a dictionary of destination nodes for each type of edge\n        \"\"\"\n\n        min_dst_idx, max_dst_idx = int(edge_data.dst.min()), int(edge_data.dst.max())\n\n        pos_src, pos_dst, pos_timestamp, edge_type = (\n            edge_data.src.cpu().numpy(),\n            edge_data.dst.cpu().numpy(),\n            edge_data.t.cpu().numpy(),\n            edge_data.edge_type.cpu().numpy(),\n        )\n\n\n\n        dst_track_dict = {} # {edge_type: {dst_1, dst_2, ..} }\n\n        # generate a list of negative destinations for each positive edge\n        pos_edge_tqdm = tqdm(\n            zip(pos_src, pos_dst, pos_timestamp, edge_type), total=len(pos_src)\n        )\n\n        for (\n            pos_s,\n            pos_d,\n            pos_t,\n            edge_type,\n            ) in pos_edge_tqdm:\n            if edge_type not in dst_track_dict:\n                dst_track_dict[edge_type] = {pos_d:1}\n            else:\n                dst_track_dict[edge_type][pos_d] = 1\n        dst_dict = {}\n        edge_type_size = []\n        for key in dst_track_dict:\n            dst = np.array(list(dst_track_dict[key].keys()))\n            edge_type_size.append(len(dst))\n            dst_dict[key] = dst\n        print ('destination candidates generated for all edge types ', len(dst_dict))\n        return dst_dict\n\n    def generate_negative_samples(self, \n                                  pos_edges: TemporalData,\n                                  split_mode: str, \n                                  partial_path: str,\n                                  ) -&gt; None:\n        r\"\"\"\n        Generate negative samples\n\n        Parameters:\n            pos_edges: positive edges to generate the negatives for\n            split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n            partial_path: in which directory save the generated negatives\n        \"\"\"\n        # file name for saving or loading...\n        filename = (\n            partial_path\n            + \"/\"\n            + self.dataset_name\n            + \"_\"\n            + split_mode\n            + \"_\"\n            + \"ns\"\n            + \".pkl\"\n        )\n\n        if self.strategy == \"time-filtered\":\n            self.generate_negative_samples_ftr(pos_edges, split_mode, filename)\n        elif self.strategy == \"dst-time-filtered\":\n            self.generate_negative_samples_dst(pos_edges, split_mode, filename)\n        elif self.strategy == \"random\":\n            self.generate_negative_samples_random(pos_edges, split_mode, filename)\n        else:\n            raise ValueError(\"Unsupported negative sample generation strategy!\")\n\n    def generate_negative_samples_ftr(self, \n                                      data: TemporalData, \n                                      split_mode: str, \n                                      filename: str,\n                                      ) -&gt; None:\n        r\"\"\"\n        now we consider (s, d, t, edge_type) as a unique edge\n        Generate negative samples based on the random strategy:\n            - for each positive edge, sample a batch of negative edges from all possible edges with the same source node\n            - filter actual positive edges at the same timestamp with the same edge type\n\n        Parameters:\n            data: an object containing positive edges information\n            split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n            filename: name of the file containing the generated negative edges\n        \"\"\"\n        print(\n            f\"INFO: Negative Sampling Strategy: {self.strategy}, Data Split: {split_mode}\"\n        )\n        assert split_mode in [\n            \"val\",\n            \"test\",\n        ], \"Invalid split-mode! It should be `val` or `test`!\"\n\n        if os.path.exists(filename):\n            print(\n                f\"INFO: negative samples for '{split_mode}' evaluation are already generated!\"\n            )\n        else:\n            print(f\"INFO: Generating negative samples for '{split_mode}' evaluation!\")\n            # retrieve the information from the batch\n            pos_src, pos_dst, pos_timestamp, edge_type = (\n                data.src.cpu().numpy(),\n                data.dst.cpu().numpy(),\n                data.t.cpu().numpy(),\n                data.edge_type.cpu().numpy(),\n            )\n            # generate a list of negative destinations for each positive edge\n            pos_edge_tqdm = tqdm(\n                zip(pos_src, pos_dst, pos_timestamp, edge_type), total=len(pos_src)\n            )\n\n            edge_t_dict = {} # {(t, u, edge_type): {v_1, v_2, ..} }\n            #! iterate once to put all edges into a dictionary for reference\n            for (\n                pos_s,\n                pos_d,\n                pos_t,\n                edge_type,\n            ) in pos_edge_tqdm:\n                if (pos_t, pos_s, edge_type) not in edge_t_dict:\n                    edge_t_dict[(pos_t, pos_s, edge_type)] = {pos_d:1}\n                else:\n                    edge_t_dict[(pos_t, pos_s, edge_type)][pos_d] = 1\n\n            conflict_dict = {}\n            for key in edge_t_dict:\n                conflict_dict[key] = np.array(list(edge_t_dict[key].keys()))\n\n            print (\"conflict sets for ns samples for \", len(conflict_dict), \" positive edges are generated\")\n            # save the generated evaluation set to disk\n            save_pkl(conflict_dict, filename)\n\n\n    def generate_negative_samples_dst(self, \n                                      data: TemporalData, \n                                      split_mode: str, \n                                      filename: str,\n                                      ) -&gt; None:\n        r\"\"\"\n        now we consider (s, d, t, edge_type) as a unique edge\n        Generate negative samples based on the random strategy:\n            - for each positive edge, sample a batch of negative edges from all possible edges with the same source node\n            - filter actual positive edges at the same timestamp with the same edge type\n\n        Parameters:\n            data: an object containing positive edges information\n            split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n            filename: name of the file containing the generated negative edges\n        \"\"\"\n        print(\n            f\"INFO: Negative Sampling Strategy: {self.strategy}, Data Split: {split_mode}\"\n        )\n        assert split_mode in [\n            \"val\",\n            \"test\",\n        ], \"Invalid split-mode! It should be `val` or `test`!\"\n\n        if os.path.exists(filename):\n            print(\n                f\"INFO: negative samples for '{split_mode}' evaluation are already generated!\"\n            )\n        else:\n            if self.dst_dict is None:\n                raise ValueError(\"The dst_dict is not generated!\")\n\n            print(f\"INFO: Generating negative samples for '{split_mode}' evaluation!\")\n            # retrieve the information from the batch\n            pos_src, pos_dst, pos_timestamp, edge_type = (\n                data.src.cpu().numpy(),\n                data.dst.cpu().numpy(),\n                data.t.cpu().numpy(),\n                data.edge_type.cpu().numpy(),\n            )\n            # generate a list of negative destinations for each positive edge\n            pos_edge_tqdm = tqdm(\n                zip(pos_src, pos_dst, pos_timestamp, edge_type), total=len(pos_src)\n            )\n\n            edge_t_dict = {} # {(t, u, edge_type): {v_1, v_2, ..} }\n            out_dict = {}\n            #! iterate once to put all edges into a dictionary for reference\n            for (\n                pos_s,\n                pos_d,\n                pos_t,\n                edge_type,\n            ) in pos_edge_tqdm:\n                if (pos_t, pos_s, edge_type) not in edge_t_dict:\n                    edge_t_dict[(pos_t, pos_s, edge_type)] = {pos_d:1}\n                else:\n                    edge_t_dict[(pos_t, pos_s, edge_type)][pos_d] = 1\n\n\n            pos_src, pos_dst, pos_timestamp, edge_type = (\n                data.src.cpu().numpy(),\n                data.dst.cpu().numpy(),\n                data.t.cpu().numpy(),\n                data.edge_type.cpu().numpy(),\n            )\n\n            new_pos_edge_tqdm = tqdm(\n                zip(pos_src, pos_dst, pos_timestamp, edge_type), total=len(pos_src)\n            )\n\n            min_dst_idx, max_dst_idx = int(self.edge_data.dst.min()), int(self.edge_data.dst.max())\n\n\n            for (\n                pos_s,\n                pos_d,\n                pos_t,\n                edge_type,\n            ) in new_pos_edge_tqdm:\n                #* generate based on # of ns samples\n                conflict_set = np.array(list(edge_t_dict[(pos_t, pos_s, edge_type)].keys()))\n                dst_set = self.dst_dict[edge_type]  #dst_set contains conflict set\n                sample_num = self.num_neg_e\n                filtered_dst_set = np.setdiff1d(dst_set, conflict_set) #more efficient\n                dst_sampled = None\n                all_dst = np.arange(min_dst_idx, max_dst_idx+1)\n                if len(filtered_dst_set) &lt; (sample_num):\n                    #* with collision check\n                    filtered_sample_set = np.setdiff1d(all_dst, filtered_dst_set)\n                    dst_sampled = np.random.choice(filtered_sample_set, sample_num, replace=False)\n                    # #* remove the conflict set from dst set\n                    dst_sampled[0:len(filtered_dst_set)] = filtered_dst_set[:]\n                else:\n                    # dst_sampled = rng.choice(max_dst_idx+1, sample_num, replace=False)\n                    dst_sampled = np.random.choice(filtered_dst_set, sample_num, replace=False)\n\n\n                if (dst_sampled.shape[0] &gt; sample_num):\n                    print (\"I am the bug that Julia worries about\")\n                    print (\"dst_sampled shape is \", dst_sampled.shape)\n                out_dict[(pos_t, pos_s, edge_type)] = dst_sampled\n\n            print (\"negative samples for \", len(out_dict), \" positive edges are generated\")\n            # save the generated evaluation set to disk\n            save_pkl(out_dict, filename)\n\n\n    def generate_negative_samples_random(self, \n                                      data: TemporalData, \n                                      split_mode: str, \n                                      filename: str,\n                                      ) -&gt; None:\n        r\"\"\"\n        generate random negative edges for ablation study\n\n        Parameters:\n            data: an object containing positive edges information\n            split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n            filename: name of the file containing the generated negative edges\n        \"\"\"\n        print(\n            f\"INFO: Negative Sampling Strategy: {self.strategy}, Data Split: {split_mode}\"\n        )\n        assert split_mode in [\n            \"val\",\n            \"test\",\n        ], \"Invalid split-mode! It should be `val` or `test`!\"\n\n        if os.path.exists(filename):\n            print(\n                f\"INFO: negative samples for '{split_mode}' evaluation are already generated!\"\n            )\n        else:\n            print(f\"INFO: Generating negative samples for '{split_mode}' evaluation!\")\n            # retrieve the information from the batch\n            pos_src, pos_dst, pos_timestamp, edge_type = (\n                data.src.cpu().numpy(),\n                data.dst.cpu().numpy(),\n                data.t.cpu().numpy(),\n                data.edge_type.cpu().numpy(),\n            )\n            first_dst_id = self.edge_data.dst.min()\n            last_dst_id = self.edge_data.dst.max()\n            all_dst = np.arange(first_dst_id, last_dst_id + 1)\n            evaluation_set = {}\n            # generate a list of negative destinations for each positive edge\n            pos_edge_tqdm = tqdm(\n                zip(pos_src, pos_dst, pos_timestamp, edge_type), total=len(pos_src)\n            )\n\n            for (\n                pos_s,\n                pos_d,\n                pos_t,\n                edge_type,\n            ) in pos_edge_tqdm:\n                t_mask = pos_timestamp == pos_t\n                src_mask = pos_src == pos_s\n                fn_mask = np.logical_and(t_mask, src_mask)\n                pos_e_dst_same_src = pos_dst[fn_mask]\n                filtered_all_dst = np.setdiff1d(all_dst, pos_e_dst_same_src)\n                if (self.num_neg_e &gt; len(filtered_all_dst)):\n                    neg_d_arr = filtered_all_dst\n                else:\n                    neg_d_arr = np.random.choice(\n                    filtered_all_dst, self.num_neg_e, replace=False) #never replace negatives\n                evaluation_set[(pos_t, pos_s, edge_type)] = neg_d_arr\n            save_pkl(evaluation_set, filename)\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.tkg_negative_generator.TKGNegativeEdgeGenerator.__init__","title":"<code>__init__(dataset_name, first_dst_id, last_dst_id, strategy='time-filtered', num_neg_e=-1, rnd_seed=1, partial_path=None, edge_data=None)</code>","text":"<p>Negative Edge Generator class for Temporal Knowledge Graphs constructor for the negative edge generator class</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>name of the dataset</p> required <code>first_dst_id</code> <code>int</code> <p>identity of the first destination node</p> required <code>last_dst_id</code> <code>int</code> <p>indentity of the last destination node</p> required <code>num_neg_e</code> <code>int</code> <p>number of negative edges being generated per each positive edge</p> <code>-1</code> <code>strategy</code> <code>str</code> <p>specifies which strategy should be used for generating the negatives</p> <code>'time-filtered'</code> <code>rnd_seed</code> <code>int</code> <p>random seed for reproducibility</p> <code>1</code> <code>edge_data</code> <code>TemporalData</code> <p>the positive edges to generate the negatives for, assuming sorted temporally</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>tgb/linkproppred/tkg_negative_generator.py</code> <pre><code>def __init__(\n    self,\n    dataset_name: str,\n    first_dst_id: int,\n    last_dst_id: int,\n    strategy: str = \"time-filtered\",\n    num_neg_e: int = -1,  # -1 means generate all possible negatives\n    rnd_seed: int = 1,\n    partial_path: str = None,\n    edge_data: TemporalData = None,\n) -&gt; None:\n    r\"\"\"\n    Negative Edge Generator class for Temporal Knowledge Graphs\n    constructor for the negative edge generator class\n\n    Parameters:\n        dataset_name: name of the dataset\n        first_dst_id: identity of the first destination node\n        last_dst_id: indentity of the last destination node\n        num_neg_e: number of negative edges being generated per each positive edge\n        strategy: specifies which strategy should be used for generating the negatives\n        rnd_seed: random seed for reproducibility\n        edge_data: the positive edges to generate the negatives for, assuming sorted temporally\n\n    Returns:\n        None\n    \"\"\"\n    self.rnd_seed = rnd_seed\n    np.random.seed(self.rnd_seed)\n    self.dataset_name = dataset_name\n    self.first_dst_id = first_dst_id\n    self.last_dst_id = last_dst_id      \n    self.num_neg_e = num_neg_e  #-1 means generate all \n    assert strategy in [\n        \"time-filtered\",\n        \"dst-time-filtered\",\n        \"random\"\n    ], \"The supported strategies are `time-filtered`, dst-time-filtered, random\"\n    self.strategy = strategy\n    self.dst_dict = None\n    if self.strategy == \"dst-time-filtered\":\n        if partial_path is None:\n            raise ValueError(\n                \"The partial path to the directory where the dst_dict is stored is required\")\n        else:\n            self.dst_dict_name = (\n                partial_path\n                + \"/\"\n                + self.dataset_name\n                + \"_\"\n                + \"dst_dict\"\n                + \".pkl\"\n            )\n            self.dst_dict = self.generate_dst_dict(edge_data=edge_data, dst_name=self.dst_dict_name)\n    self.edge_data = edge_data\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.tkg_negative_generator.TKGNegativeEdgeGenerator.generate_dst_dict","title":"<code>generate_dst_dict(edge_data, dst_name)</code>","text":"<p>Generate a dictionary of destination nodes for each type of edge</p> <p>Parameters:</p> Name Type Description Default <code>edge_data</code> <code>TemporalData</code> <p>an object containing positive edges information</p> required <code>dst_name</code> <code>str</code> <p>name of the file to save the generated dictionary of destination nodes</p> required <p>Returns:</p> Name Type Description <code>dst_dict</code> <code>dict</code> <p>a dictionary of destination nodes for each type of edge</p> Source code in <code>tgb/linkproppred/tkg_negative_generator.py</code> <pre><code>def generate_dst_dict(self, edge_data: TemporalData, dst_name: str) -&gt; dict:\n    r\"\"\"\n    Generate a dictionary of destination nodes for each type of edge\n\n    Parameters:\n        edge_data: an object containing positive edges information\n        dst_name: name of the file to save the generated dictionary of destination nodes\n\n    Returns:\n        dst_dict: a dictionary of destination nodes for each type of edge\n    \"\"\"\n\n    min_dst_idx, max_dst_idx = int(edge_data.dst.min()), int(edge_data.dst.max())\n\n    pos_src, pos_dst, pos_timestamp, edge_type = (\n        edge_data.src.cpu().numpy(),\n        edge_data.dst.cpu().numpy(),\n        edge_data.t.cpu().numpy(),\n        edge_data.edge_type.cpu().numpy(),\n    )\n\n\n\n    dst_track_dict = {} # {edge_type: {dst_1, dst_2, ..} }\n\n    # generate a list of negative destinations for each positive edge\n    pos_edge_tqdm = tqdm(\n        zip(pos_src, pos_dst, pos_timestamp, edge_type), total=len(pos_src)\n    )\n\n    for (\n        pos_s,\n        pos_d,\n        pos_t,\n        edge_type,\n        ) in pos_edge_tqdm:\n        if edge_type not in dst_track_dict:\n            dst_track_dict[edge_type] = {pos_d:1}\n        else:\n            dst_track_dict[edge_type][pos_d] = 1\n    dst_dict = {}\n    edge_type_size = []\n    for key in dst_track_dict:\n        dst = np.array(list(dst_track_dict[key].keys()))\n        edge_type_size.append(len(dst))\n        dst_dict[key] = dst\n    print ('destination candidates generated for all edge types ', len(dst_dict))\n    return dst_dict\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.tkg_negative_generator.TKGNegativeEdgeGenerator.generate_negative_samples","title":"<code>generate_negative_samples(pos_edges, split_mode, partial_path)</code>","text":"<p>Generate negative samples</p> <p>Parameters:</p> Name Type Description Default <code>pos_edges</code> <code>TemporalData</code> <p>positive edges to generate the negatives for</p> required <code>split_mode</code> <code>str</code> <p>specifies whether to generate negative edges for 'validation' or 'test' splits</p> required <code>partial_path</code> <code>str</code> <p>in which directory save the generated negatives</p> required Source code in <code>tgb/linkproppred/tkg_negative_generator.py</code> <pre><code>def generate_negative_samples(self, \n                              pos_edges: TemporalData,\n                              split_mode: str, \n                              partial_path: str,\n                              ) -&gt; None:\n    r\"\"\"\n    Generate negative samples\n\n    Parameters:\n        pos_edges: positive edges to generate the negatives for\n        split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n        partial_path: in which directory save the generated negatives\n    \"\"\"\n    # file name for saving or loading...\n    filename = (\n        partial_path\n        + \"/\"\n        + self.dataset_name\n        + \"_\"\n        + split_mode\n        + \"_\"\n        + \"ns\"\n        + \".pkl\"\n    )\n\n    if self.strategy == \"time-filtered\":\n        self.generate_negative_samples_ftr(pos_edges, split_mode, filename)\n    elif self.strategy == \"dst-time-filtered\":\n        self.generate_negative_samples_dst(pos_edges, split_mode, filename)\n    elif self.strategy == \"random\":\n        self.generate_negative_samples_random(pos_edges, split_mode, filename)\n    else:\n        raise ValueError(\"Unsupported negative sample generation strategy!\")\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.tkg_negative_generator.TKGNegativeEdgeGenerator.generate_negative_samples_dst","title":"<code>generate_negative_samples_dst(data, split_mode, filename)</code>","text":"<p>now we consider (s, d, t, edge_type) as a unique edge Generate negative samples based on the random strategy:     - for each positive edge, sample a batch of negative edges from all possible edges with the same source node     - filter actual positive edges at the same timestamp with the same edge type</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TemporalData</code> <p>an object containing positive edges information</p> required <code>split_mode</code> <code>str</code> <p>specifies whether to generate negative edges for 'validation' or 'test' splits</p> required <code>filename</code> <code>str</code> <p>name of the file containing the generated negative edges</p> required Source code in <code>tgb/linkproppred/tkg_negative_generator.py</code> <pre><code>def generate_negative_samples_dst(self, \n                                  data: TemporalData, \n                                  split_mode: str, \n                                  filename: str,\n                                  ) -&gt; None:\n    r\"\"\"\n    now we consider (s, d, t, edge_type) as a unique edge\n    Generate negative samples based on the random strategy:\n        - for each positive edge, sample a batch of negative edges from all possible edges with the same source node\n        - filter actual positive edges at the same timestamp with the same edge type\n\n    Parameters:\n        data: an object containing positive edges information\n        split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n        filename: name of the file containing the generated negative edges\n    \"\"\"\n    print(\n        f\"INFO: Negative Sampling Strategy: {self.strategy}, Data Split: {split_mode}\"\n    )\n    assert split_mode in [\n        \"val\",\n        \"test\",\n    ], \"Invalid split-mode! It should be `val` or `test`!\"\n\n    if os.path.exists(filename):\n        print(\n            f\"INFO: negative samples for '{split_mode}' evaluation are already generated!\"\n        )\n    else:\n        if self.dst_dict is None:\n            raise ValueError(\"The dst_dict is not generated!\")\n\n        print(f\"INFO: Generating negative samples for '{split_mode}' evaluation!\")\n        # retrieve the information from the batch\n        pos_src, pos_dst, pos_timestamp, edge_type = (\n            data.src.cpu().numpy(),\n            data.dst.cpu().numpy(),\n            data.t.cpu().numpy(),\n            data.edge_type.cpu().numpy(),\n        )\n        # generate a list of negative destinations for each positive edge\n        pos_edge_tqdm = tqdm(\n            zip(pos_src, pos_dst, pos_timestamp, edge_type), total=len(pos_src)\n        )\n\n        edge_t_dict = {} # {(t, u, edge_type): {v_1, v_2, ..} }\n        out_dict = {}\n        #! iterate once to put all edges into a dictionary for reference\n        for (\n            pos_s,\n            pos_d,\n            pos_t,\n            edge_type,\n        ) in pos_edge_tqdm:\n            if (pos_t, pos_s, edge_type) not in edge_t_dict:\n                edge_t_dict[(pos_t, pos_s, edge_type)] = {pos_d:1}\n            else:\n                edge_t_dict[(pos_t, pos_s, edge_type)][pos_d] = 1\n\n\n        pos_src, pos_dst, pos_timestamp, edge_type = (\n            data.src.cpu().numpy(),\n            data.dst.cpu().numpy(),\n            data.t.cpu().numpy(),\n            data.edge_type.cpu().numpy(),\n        )\n\n        new_pos_edge_tqdm = tqdm(\n            zip(pos_src, pos_dst, pos_timestamp, edge_type), total=len(pos_src)\n        )\n\n        min_dst_idx, max_dst_idx = int(self.edge_data.dst.min()), int(self.edge_data.dst.max())\n\n\n        for (\n            pos_s,\n            pos_d,\n            pos_t,\n            edge_type,\n        ) in new_pos_edge_tqdm:\n            #* generate based on # of ns samples\n            conflict_set = np.array(list(edge_t_dict[(pos_t, pos_s, edge_type)].keys()))\n            dst_set = self.dst_dict[edge_type]  #dst_set contains conflict set\n            sample_num = self.num_neg_e\n            filtered_dst_set = np.setdiff1d(dst_set, conflict_set) #more efficient\n            dst_sampled = None\n            all_dst = np.arange(min_dst_idx, max_dst_idx+1)\n            if len(filtered_dst_set) &lt; (sample_num):\n                #* with collision check\n                filtered_sample_set = np.setdiff1d(all_dst, filtered_dst_set)\n                dst_sampled = np.random.choice(filtered_sample_set, sample_num, replace=False)\n                # #* remove the conflict set from dst set\n                dst_sampled[0:len(filtered_dst_set)] = filtered_dst_set[:]\n            else:\n                # dst_sampled = rng.choice(max_dst_idx+1, sample_num, replace=False)\n                dst_sampled = np.random.choice(filtered_dst_set, sample_num, replace=False)\n\n\n            if (dst_sampled.shape[0] &gt; sample_num):\n                print (\"I am the bug that Julia worries about\")\n                print (\"dst_sampled shape is \", dst_sampled.shape)\n            out_dict[(pos_t, pos_s, edge_type)] = dst_sampled\n\n        print (\"negative samples for \", len(out_dict), \" positive edges are generated\")\n        # save the generated evaluation set to disk\n        save_pkl(out_dict, filename)\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.tkg_negative_generator.TKGNegativeEdgeGenerator.generate_negative_samples_ftr","title":"<code>generate_negative_samples_ftr(data, split_mode, filename)</code>","text":"<p>now we consider (s, d, t, edge_type) as a unique edge Generate negative samples based on the random strategy:     - for each positive edge, sample a batch of negative edges from all possible edges with the same source node     - filter actual positive edges at the same timestamp with the same edge type</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TemporalData</code> <p>an object containing positive edges information</p> required <code>split_mode</code> <code>str</code> <p>specifies whether to generate negative edges for 'validation' or 'test' splits</p> required <code>filename</code> <code>str</code> <p>name of the file containing the generated negative edges</p> required Source code in <code>tgb/linkproppred/tkg_negative_generator.py</code> <pre><code>def generate_negative_samples_ftr(self, \n                                  data: TemporalData, \n                                  split_mode: str, \n                                  filename: str,\n                                  ) -&gt; None:\n    r\"\"\"\n    now we consider (s, d, t, edge_type) as a unique edge\n    Generate negative samples based on the random strategy:\n        - for each positive edge, sample a batch of negative edges from all possible edges with the same source node\n        - filter actual positive edges at the same timestamp with the same edge type\n\n    Parameters:\n        data: an object containing positive edges information\n        split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n        filename: name of the file containing the generated negative edges\n    \"\"\"\n    print(\n        f\"INFO: Negative Sampling Strategy: {self.strategy}, Data Split: {split_mode}\"\n    )\n    assert split_mode in [\n        \"val\",\n        \"test\",\n    ], \"Invalid split-mode! It should be `val` or `test`!\"\n\n    if os.path.exists(filename):\n        print(\n            f\"INFO: negative samples for '{split_mode}' evaluation are already generated!\"\n        )\n    else:\n        print(f\"INFO: Generating negative samples for '{split_mode}' evaluation!\")\n        # retrieve the information from the batch\n        pos_src, pos_dst, pos_timestamp, edge_type = (\n            data.src.cpu().numpy(),\n            data.dst.cpu().numpy(),\n            data.t.cpu().numpy(),\n            data.edge_type.cpu().numpy(),\n        )\n        # generate a list of negative destinations for each positive edge\n        pos_edge_tqdm = tqdm(\n            zip(pos_src, pos_dst, pos_timestamp, edge_type), total=len(pos_src)\n        )\n\n        edge_t_dict = {} # {(t, u, edge_type): {v_1, v_2, ..} }\n        #! iterate once to put all edges into a dictionary for reference\n        for (\n            pos_s,\n            pos_d,\n            pos_t,\n            edge_type,\n        ) in pos_edge_tqdm:\n            if (pos_t, pos_s, edge_type) not in edge_t_dict:\n                edge_t_dict[(pos_t, pos_s, edge_type)] = {pos_d:1}\n            else:\n                edge_t_dict[(pos_t, pos_s, edge_type)][pos_d] = 1\n\n        conflict_dict = {}\n        for key in edge_t_dict:\n            conflict_dict[key] = np.array(list(edge_t_dict[key].keys()))\n\n        print (\"conflict sets for ns samples for \", len(conflict_dict), \" positive edges are generated\")\n        # save the generated evaluation set to disk\n        save_pkl(conflict_dict, filename)\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.tkg_negative_generator.TKGNegativeEdgeGenerator.generate_negative_samples_random","title":"<code>generate_negative_samples_random(data, split_mode, filename)</code>","text":"<p>generate random negative edges for ablation study</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TemporalData</code> <p>an object containing positive edges information</p> required <code>split_mode</code> <code>str</code> <p>specifies whether to generate negative edges for 'validation' or 'test' splits</p> required <code>filename</code> <code>str</code> <p>name of the file containing the generated negative edges</p> required Source code in <code>tgb/linkproppred/tkg_negative_generator.py</code> <pre><code>def generate_negative_samples_random(self, \n                                  data: TemporalData, \n                                  split_mode: str, \n                                  filename: str,\n                                  ) -&gt; None:\n    r\"\"\"\n    generate random negative edges for ablation study\n\n    Parameters:\n        data: an object containing positive edges information\n        split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n        filename: name of the file containing the generated negative edges\n    \"\"\"\n    print(\n        f\"INFO: Negative Sampling Strategy: {self.strategy}, Data Split: {split_mode}\"\n    )\n    assert split_mode in [\n        \"val\",\n        \"test\",\n    ], \"Invalid split-mode! It should be `val` or `test`!\"\n\n    if os.path.exists(filename):\n        print(\n            f\"INFO: negative samples for '{split_mode}' evaluation are already generated!\"\n        )\n    else:\n        print(f\"INFO: Generating negative samples for '{split_mode}' evaluation!\")\n        # retrieve the information from the batch\n        pos_src, pos_dst, pos_timestamp, edge_type = (\n            data.src.cpu().numpy(),\n            data.dst.cpu().numpy(),\n            data.t.cpu().numpy(),\n            data.edge_type.cpu().numpy(),\n        )\n        first_dst_id = self.edge_data.dst.min()\n        last_dst_id = self.edge_data.dst.max()\n        all_dst = np.arange(first_dst_id, last_dst_id + 1)\n        evaluation_set = {}\n        # generate a list of negative destinations for each positive edge\n        pos_edge_tqdm = tqdm(\n            zip(pos_src, pos_dst, pos_timestamp, edge_type), total=len(pos_src)\n        )\n\n        for (\n            pos_s,\n            pos_d,\n            pos_t,\n            edge_type,\n        ) in pos_edge_tqdm:\n            t_mask = pos_timestamp == pos_t\n            src_mask = pos_src == pos_s\n            fn_mask = np.logical_and(t_mask, src_mask)\n            pos_e_dst_same_src = pos_dst[fn_mask]\n            filtered_all_dst = np.setdiff1d(all_dst, pos_e_dst_same_src)\n            if (self.num_neg_e &gt; len(filtered_all_dst)):\n                neg_d_arr = filtered_all_dst\n            else:\n                neg_d_arr = np.random.choice(\n                filtered_all_dst, self.num_neg_e, replace=False) #never replace negatives\n            evaluation_set[(pos_t, pos_s, edge_type)] = neg_d_arr\n        save_pkl(evaluation_set, filename)\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.tkg_negative_sampler.TKGNegativeEdgeSampler","title":"<code>TKGNegativeEdgeSampler</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>tgb/linkproppred/tkg_negative_sampler.py</code> <pre><code>class TKGNegativeEdgeSampler(object):\n    def __init__(\n        self,\n        dataset_name: str,\n        first_dst_id: int,\n        last_dst_id: int,\n        strategy: str = \"time-filtered\",\n        partial_path: str = PROJ_DIR + \"/data/processed\",\n    ) -&gt; None:\n        r\"\"\"\n        Negative Edge Sampler\n            Loads and query the negative batches based on the positive batches provided.\n        constructor for the negative edge sampler class\n\n        Parameters:\n            dataset_name: name of the dataset\n            first_dst_id: identity of the first destination node\n            last_dst_id: indentity of the last destination node\n            strategy: will always load the pre-generated negatives\n            partial_path: the path to the directory where the negative edges are stored\n\n        Returns:\n            None\n        \"\"\"\n        self.dataset_name = dataset_name\n        self.eval_set = {}\n        self.first_dst_id = first_dst_id\n        self.last_dst_id = last_dst_id\n        self.strategy = strategy\n        self.dst_dict = None\n\n    def load_eval_set(\n        self,\n        fname: str,\n        split_mode: str = \"val\",\n    ) -&gt; None:\n        r\"\"\"\n        Load the evaluation set from disk, can be either val or test set ns samples\n        Parameters:\n            fname: the file name of the evaluation ns on disk\n            split_mode: the split mode of the evaluation set, can be either `val` or `test`\n\n        Returns:\n            None\n        \"\"\"\n        assert split_mode in [\n            \"val\",\n            \"test\",\n        ], \"Invalid split-mode! It should be `val`, `test`\"\n        if not os.path.exists(fname):\n            raise FileNotFoundError(f\"File not found at {fname}\")\n        self.eval_set[split_mode] = load_pkl(fname)\n\n    def query_batch(self, \n                    pos_src: Union[Tensor, np.ndarray], \n                    pos_dst: Union[Tensor, np.ndarray], \n                    pos_timestamp: Union[Tensor, np.ndarray], \n                    edge_type: Union[Tensor, np.ndarray],\n                    split_mode: str = \"test\") -&gt; list:\n        r\"\"\"\n        For each positive edge in the `pos_batch`, return a list of negative edges\n        `split_mode` specifies whether the valiation or test evaluation set should be retrieved.\n        modify now to include edge type argument\n\n        Parameters:\n            pos_src: list of positive source nodes\n            pos_dst: list of positive destination nodes\n            pos_timestamp: list of timestamps of the positive edges\n            split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n\n        Returns:\n            neg_samples: list of numpy array; each array contains the set of negative edges that\n                        should be evaluated against each positive edge.\n        \"\"\"\n        assert split_mode in [\n            \"val\",\n            \"test\",\n        ], \"Invalid split-mode! It should be `val`, `test`!\"\n        if self.eval_set[split_mode] == None:\n            raise ValueError(\n                f\"Evaluation set is None! You should load the {split_mode} evaluation set first!\"\n            )\n\n        # check the argument types...\n        if torch is not None and isinstance(pos_src, torch.Tensor):\n            pos_src = pos_src.detach().cpu().numpy()\n        if torch is not None and isinstance(pos_dst, torch.Tensor):\n            pos_dst = pos_dst.detach().cpu().numpy()\n        if torch is not None and isinstance(pos_timestamp, torch.Tensor):\n            pos_timestamp = pos_timestamp.detach().cpu().numpy()\n        if torch is not None and isinstance(edge_type, torch.Tensor):\n            edge_type = edge_type.detach().cpu().numpy()\n\n        if not isinstance(pos_src, np.ndarray) or not isinstance(pos_dst, np.ndarray) or not(pos_timestamp, np.ndarray) or not(edge_type, np.ndarray):\n            raise RuntimeError(\n                \"pos_src, pos_dst, and pos_timestamp need to be either numpy ndarray or torch tensor!\"\n                )\n\n        if self.strategy == \"time-filtered\":\n            neg_samples = []\n            for pos_s, pos_d, pos_t, e_type in zip(pos_src, pos_dst, pos_timestamp, edge_type):\n                if (pos_t, pos_s, e_type) not in self.eval_set[split_mode]:\n                    raise ValueError(\n                        f\"The edge ({pos_s}, {pos_d}, {pos_t}, {e_type}) is not in the '{split_mode}' evaluation set! Please check the implementation.\"\n                    )\n                else:\n                    conflict_dict = self.eval_set[split_mode]\n                    conflict_set = conflict_dict[(pos_t, pos_s, e_type)]\n                    all_dst = np.arange(self.first_dst_id, self.last_dst_id + 1)\n                    filtered_all_dst = np.delete(all_dst, conflict_set, axis=0)\n\n                    #! always using all possible destinations for evaluation\n                    neg_d_arr = filtered_all_dst\n\n                    #! this is very slow\n                    neg_samples.append(\n                            neg_d_arr\n                        )\n        elif self.strategy == \"dst-time-filtered\":\n            neg_samples = []\n            for pos_s, pos_d, pos_t, e_type in zip(pos_src, pos_dst, pos_timestamp, edge_type):\n                if (pos_t, pos_s, e_type) not in self.eval_set[split_mode]:\n                    raise ValueError(\n                        f\"The edge ({pos_s}, {pos_d}, {pos_t}, {e_type}) is not in the '{split_mode}' evaluation set! Please check the implementation.\"\n                    )\n                else:\n                    filtered_dst = self.eval_set[split_mode]\n                    neg_d_arr = filtered_dst[(pos_t, pos_s, e_type)]\n                    neg_samples.append(\n                            neg_d_arr\n                        )\n        #? can't convert to numpy array due to different lengths of negative samples\n        return neg_samples\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.tkg_negative_sampler.TKGNegativeEdgeSampler.__init__","title":"<code>__init__(dataset_name, first_dst_id, last_dst_id, strategy='time-filtered', partial_path=PROJ_DIR + '/data/processed')</code>","text":"<p>Negative Edge Sampler     Loads and query the negative batches based on the positive batches provided. constructor for the negative edge sampler class</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>name of the dataset</p> required <code>first_dst_id</code> <code>int</code> <p>identity of the first destination node</p> required <code>last_dst_id</code> <code>int</code> <p>indentity of the last destination node</p> required <code>strategy</code> <code>str</code> <p>will always load the pre-generated negatives</p> <code>'time-filtered'</code> <code>partial_path</code> <code>str</code> <p>the path to the directory where the negative edges are stored</p> <code>PROJ_DIR + '/data/processed'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>tgb/linkproppred/tkg_negative_sampler.py</code> <pre><code>def __init__(\n    self,\n    dataset_name: str,\n    first_dst_id: int,\n    last_dst_id: int,\n    strategy: str = \"time-filtered\",\n    partial_path: str = PROJ_DIR + \"/data/processed\",\n) -&gt; None:\n    r\"\"\"\n    Negative Edge Sampler\n        Loads and query the negative batches based on the positive batches provided.\n    constructor for the negative edge sampler class\n\n    Parameters:\n        dataset_name: name of the dataset\n        first_dst_id: identity of the first destination node\n        last_dst_id: indentity of the last destination node\n        strategy: will always load the pre-generated negatives\n        partial_path: the path to the directory where the negative edges are stored\n\n    Returns:\n        None\n    \"\"\"\n    self.dataset_name = dataset_name\n    self.eval_set = {}\n    self.first_dst_id = first_dst_id\n    self.last_dst_id = last_dst_id\n    self.strategy = strategy\n    self.dst_dict = None\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.tkg_negative_sampler.TKGNegativeEdgeSampler.load_eval_set","title":"<code>load_eval_set(fname, split_mode='val')</code>","text":"<p>Load the evaluation set from disk, can be either val or test set ns samples Parameters:     fname: the file name of the evaluation ns on disk     split_mode: the split mode of the evaluation set, can be either <code>val</code> or <code>test</code></p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>tgb/linkproppred/tkg_negative_sampler.py</code> <pre><code>def load_eval_set(\n    self,\n    fname: str,\n    split_mode: str = \"val\",\n) -&gt; None:\n    r\"\"\"\n    Load the evaluation set from disk, can be either val or test set ns samples\n    Parameters:\n        fname: the file name of the evaluation ns on disk\n        split_mode: the split mode of the evaluation set, can be either `val` or `test`\n\n    Returns:\n        None\n    \"\"\"\n    assert split_mode in [\n        \"val\",\n        \"test\",\n    ], \"Invalid split-mode! It should be `val`, `test`\"\n    if not os.path.exists(fname):\n        raise FileNotFoundError(f\"File not found at {fname}\")\n    self.eval_set[split_mode] = load_pkl(fname)\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.tkg_negative_sampler.TKGNegativeEdgeSampler.query_batch","title":"<code>query_batch(pos_src, pos_dst, pos_timestamp, edge_type, split_mode='test')</code>","text":"<p>For each positive edge in the <code>pos_batch</code>, return a list of negative edges <code>split_mode</code> specifies whether the valiation or test evaluation set should be retrieved. modify now to include edge type argument</p> <p>Parameters:</p> Name Type Description Default <code>pos_src</code> <code>Union[Tensor, ndarray]</code> <p>list of positive source nodes</p> required <code>pos_dst</code> <code>Union[Tensor, ndarray]</code> <p>list of positive destination nodes</p> required <code>pos_timestamp</code> <code>Union[Tensor, ndarray]</code> <p>list of timestamps of the positive edges</p> required <code>split_mode</code> <code>str</code> <p>specifies whether to generate negative edges for 'validation' or 'test' splits</p> <code>'test'</code> <p>Returns:</p> Name Type Description <code>neg_samples</code> <code>list</code> <p>list of numpy array; each array contains the set of negative edges that         should be evaluated against each positive edge.</p> Source code in <code>tgb/linkproppred/tkg_negative_sampler.py</code> <pre><code>def query_batch(self, \n                pos_src: Union[Tensor, np.ndarray], \n                pos_dst: Union[Tensor, np.ndarray], \n                pos_timestamp: Union[Tensor, np.ndarray], \n                edge_type: Union[Tensor, np.ndarray],\n                split_mode: str = \"test\") -&gt; list:\n    r\"\"\"\n    For each positive edge in the `pos_batch`, return a list of negative edges\n    `split_mode` specifies whether the valiation or test evaluation set should be retrieved.\n    modify now to include edge type argument\n\n    Parameters:\n        pos_src: list of positive source nodes\n        pos_dst: list of positive destination nodes\n        pos_timestamp: list of timestamps of the positive edges\n        split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n\n    Returns:\n        neg_samples: list of numpy array; each array contains the set of negative edges that\n                    should be evaluated against each positive edge.\n    \"\"\"\n    assert split_mode in [\n        \"val\",\n        \"test\",\n    ], \"Invalid split-mode! It should be `val`, `test`!\"\n    if self.eval_set[split_mode] == None:\n        raise ValueError(\n            f\"Evaluation set is None! You should load the {split_mode} evaluation set first!\"\n        )\n\n    # check the argument types...\n    if torch is not None and isinstance(pos_src, torch.Tensor):\n        pos_src = pos_src.detach().cpu().numpy()\n    if torch is not None and isinstance(pos_dst, torch.Tensor):\n        pos_dst = pos_dst.detach().cpu().numpy()\n    if torch is not None and isinstance(pos_timestamp, torch.Tensor):\n        pos_timestamp = pos_timestamp.detach().cpu().numpy()\n    if torch is not None and isinstance(edge_type, torch.Tensor):\n        edge_type = edge_type.detach().cpu().numpy()\n\n    if not isinstance(pos_src, np.ndarray) or not isinstance(pos_dst, np.ndarray) or not(pos_timestamp, np.ndarray) or not(edge_type, np.ndarray):\n        raise RuntimeError(\n            \"pos_src, pos_dst, and pos_timestamp need to be either numpy ndarray or torch tensor!\"\n            )\n\n    if self.strategy == \"time-filtered\":\n        neg_samples = []\n        for pos_s, pos_d, pos_t, e_type in zip(pos_src, pos_dst, pos_timestamp, edge_type):\n            if (pos_t, pos_s, e_type) not in self.eval_set[split_mode]:\n                raise ValueError(\n                    f\"The edge ({pos_s}, {pos_d}, {pos_t}, {e_type}) is not in the '{split_mode}' evaluation set! Please check the implementation.\"\n                )\n            else:\n                conflict_dict = self.eval_set[split_mode]\n                conflict_set = conflict_dict[(pos_t, pos_s, e_type)]\n                all_dst = np.arange(self.first_dst_id, self.last_dst_id + 1)\n                filtered_all_dst = np.delete(all_dst, conflict_set, axis=0)\n\n                #! always using all possible destinations for evaluation\n                neg_d_arr = filtered_all_dst\n\n                #! this is very slow\n                neg_samples.append(\n                        neg_d_arr\n                    )\n    elif self.strategy == \"dst-time-filtered\":\n        neg_samples = []\n        for pos_s, pos_d, pos_t, e_type in zip(pos_src, pos_dst, pos_timestamp, edge_type):\n            if (pos_t, pos_s, e_type) not in self.eval_set[split_mode]:\n                raise ValueError(\n                    f\"The edge ({pos_s}, {pos_d}, {pos_t}, {e_type}) is not in the '{split_mode}' evaluation set! Please check the implementation.\"\n                )\n            else:\n                filtered_dst = self.eval_set[split_mode]\n                neg_d_arr = filtered_dst[(pos_t, pos_s, e_type)]\n                neg_samples.append(\n                        neg_d_arr\n                    )\n    #? can't convert to numpy array due to different lengths of negative samples\n    return neg_samples\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.thg_negative_generator.THGNegativeEdgeGenerator","title":"<code>THGNegativeEdgeGenerator</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>tgb/linkproppred/thg_negative_generator.py</code> <pre><code>class THGNegativeEdgeGenerator(object):\n    def __init__(\n        self,\n        dataset_name: str,\n        first_node_id: int,\n        last_node_id: int,\n        node_type: Union[np.ndarray, torch.Tensor],\n        strategy: str = \"node-type-filtered\",\n        num_neg_e: int = -1,  # -1 means generate all possible negatives\n        rnd_seed: int = 1,\n        edge_data: TemporalData = None,\n    ) -&gt; None:\n        r\"\"\"\n        Negative Edge Generator class for Temporal Heterogeneous Graphs\n        this is a class for generating negative samples for a specific datasets\n        the set of the positive samples are provided, the negative samples are generated with specific strategies \n        and are saved for consistent evaluation across different methods\n\n        Parameters:\n            dataset_name: name of the dataset\n            first_node_id: the first node id\n            last_node_id: the last node id\n            node_type: the node type of each node\n            strategy: the strategy to generate negative samples\n            num_neg_e: number of negative samples to generate\n            rnd_seed: random seed\n            edge_data: the edge data object containing the positive edges\n        Returns:\n            None\n        \"\"\"\n        self.rnd_seed = rnd_seed\n        np.random.seed(self.rnd_seed)\n        self.dataset_name = dataset_name\n        self.first_node_id = first_node_id\n        self.last_node_id = last_node_id\n        if isinstance(node_type, torch.Tensor):\n            node_type = node_type.cpu().numpy()\n        self.node_type = node_type\n        self.node_type_dict = self.get_destinations_based_on_node_type(first_node_id, last_node_id, self.node_type) # {node_type: {nid:1}}\n        assert isinstance(self.node_type, np.ndarray), \"node_type should be a numpy array\"\n        self.num_neg_e = num_neg_e  #-1 means generate all \n\n        assert strategy in [\n            \"node-type-filtered\",\n            \"random\",\n        ], \"The supported strategies are `node-type-filtered`\"\n        self.strategy = strategy\n        self.edge_data = edge_data\n\n    def get_destinations_based_on_node_type(self, \n                                            first_node_id: int,\n                                            last_node_id: int,\n                                            node_type: np.ndarray) -&gt; dict:\n        r\"\"\"\n        get the destination node id arrays based on the node type\n        Parameters:\n            first_node_id: the first node id\n            last_node_id: the last node id\n            node_type: the node type of each node\n\n        Returns:\n            node_type_dict: a dictionary containing the destination node ids for each node type\n        \"\"\"\n        node_type_store = {}\n        assert first_node_id &lt;= last_node_id, \"Invalid destination node ids!\"\n        assert len(node_type) == (last_node_id - first_node_id + 1), \"node type array must match the indices\"\n        for k in range(len(node_type)):\n            nt = int(node_type[k]) #node type must be ints\n            nid = k + first_node_id\n            if nt not in node_type_store:\n                node_type_store[nt] = {nid:1}\n            else:\n                node_type_store[nt][nid] = 1\n        node_type_dict = {}\n        for ntype in node_type_store:\n            node_type_dict[ntype] = np.array(list(node_type_store[ntype].keys()))\n            assert np.all(np.diff(node_type_dict[ntype]) &gt;= 0), \"Destination node ids for a given type must be sorted\"\n            assert np.all(node_type_dict[ntype] &lt;= last_node_id), \"Destination node ids must be less than or equal to the last destination id\"\n        return node_type_dict\n\n    def generate_negative_samples(self, \n                                  pos_edges: TemporalData,\n                                  split_mode: str, \n                                  partial_path: str,\n                                  ) -&gt; None:\n        r\"\"\"\n        Generate negative samples\n\n        Parameters:\n            pos_edges: positive edges to generate the negatives for\n            split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n            partial_path: in which directory save the generated negatives\n        \"\"\"\n        # file name for saving or loading...\n        filename = (\n            partial_path\n            + \"/\"\n            + self.dataset_name\n            + \"_\"\n            + split_mode\n            + \"_\"\n            + \"ns\"\n            + \".pkl\"\n        )\n\n        if self.strategy == \"node-type-filtered\":\n            self.generate_negative_samples_nt(pos_edges, split_mode, filename)\n        elif self.strategy == \"random\":\n            self.generate_negative_samples_random(pos_edges, split_mode, filename)\n        else:\n            raise ValueError(\"Unsupported negative sample generation strategy!\")\n\n    def generate_negative_samples_nt(self, \n                                      data: TemporalData, \n                                      split_mode: str, \n                                      filename: str,\n                                      ) -&gt; None:\n        r\"\"\"\n        now we consider (s, d, t, edge_type) as a unique edge, also adding the node type info for the destination node for convenience so (s, d, t, edge_type): (conflict_set, d_node_type)\n        Generate negative samples based on the random strategy:\n            - for each positive edge, retrieve all possible destinations based on the node type of the destination node\n            - filter actual positive edges at the same timestamp with the same edge type\n\n        Parameters:\n            data: an object containing positive edges information\n            split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n            filename: name of the file containing the generated negative edges\n        \"\"\"\n        print(\n            f\"INFO: Negative Sampling Strategy: {self.strategy}, Data Split: {split_mode}\"\n        )\n        assert split_mode in [\n            \"val\",\n            \"test\",\n        ], \"Invalid split-mode! It should be `val` or `test`!\"\n\n        if os.path.exists(filename):\n            print(\n                f\"INFO: negative samples for '{split_mode}' evaluation are already generated!\"\n            )\n        else:\n            print(f\"INFO: Generating negative samples for '{split_mode}' evaluation!\")\n            # retrieve the information from the batch\n            pos_src, pos_dst, pos_timestamp, edge_type = (\n                data.src.cpu().numpy(),\n                data.dst.cpu().numpy(),\n                data.t.cpu().numpy(),\n                data.edge_type.cpu().numpy(),\n            )\n\n            # generate a list of negative destinations for each positive edge\n            pos_edge_tqdm = tqdm(\n                zip(pos_src, pos_dst, pos_timestamp, edge_type), total=len(pos_src)\n            )\n\n            edge_t_dict = {} # {(t, u, edge_type): {v_1, v_2, ..} }\n            #! iterate once to put all edges into a dictionary for reference\n            for (\n                pos_s,\n                pos_d,\n                pos_t,\n                edge_type,\n            ) in pos_edge_tqdm:\n                if (pos_t, pos_s, edge_type) not in edge_t_dict:\n                    edge_t_dict[(pos_t, pos_s, edge_type)] = {pos_d:1}\n                else:\n                    edge_t_dict[(pos_t, pos_s, edge_type)][pos_d] = 1\n\n            out_dict = {}\n            for key in tqdm(edge_t_dict):\n                conflict_set = np.array(list(edge_t_dict[key].keys()))\n                pos_d = conflict_set[0]\n                #* retieve the node type of the destination node as well \n                #! assumption, same edge type = same destination node type\n                d_node_type = int(self.node_type[pos_d - self.first_node_id])\n                all_dst = self.node_type_dict[d_node_type]\n                if (self.num_neg_e == -1):\n                    filtered_all_dst = np.setdiff1d(all_dst, conflict_set)\n                else:\n                    #* lazy sampling\n                    neg_d_arr = np.random.choice(\n                        all_dst, self.num_neg_e, replace=False) #never replace negatives\n                    if len(np.setdiff1d(neg_d_arr, conflict_set)) &lt; self.num_neg_e:\n                        neg_d_arr = np.random.choice(\n                            np.setdiff1d(all_dst, conflict_set), self.num_neg_e, replace=False)\n                    filtered_all_dst = neg_d_arr\n                out_dict[key] = filtered_all_dst\n            print (\"ns samples for \", len(out_dict), \" positive edges are generated\")\n            # save the generated evaluation set to disk\n            save_pkl(out_dict, filename)\n\n    def generate_negative_samples_random(self, \n                                      data: TemporalData, \n                                      split_mode: str, \n                                      filename: str,\n                                      ) -&gt; None:\n        r\"\"\"\n        generate random negative edges for ablation study\n\n        Parameters:\n            data: an object containing positive edges information\n            split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n            filename: name of the file containing the generated negative edges\n        \"\"\"\n        print(\n            f\"INFO: Negative Sampling Strategy: {self.strategy}, Data Split: {split_mode}\"\n        )\n        assert split_mode in [\n            \"val\",\n            \"test\",\n        ], \"Invalid split-mode! It should be `val` or `test`!\"\n\n        if os.path.exists(filename):\n            print(\n                f\"INFO: negative samples for '{split_mode}' evaluation are already generated!\"\n            )\n        else:\n            print(f\"INFO: Generating negative samples for '{split_mode}' evaluation!\")\n            # retrieve the information from the batch\n            pos_src, pos_dst, pos_timestamp, edge_type = (\n                data.src.cpu().numpy(),\n                data.dst.cpu().numpy(),\n                data.t.cpu().numpy(),\n                data.edge_type.cpu().numpy(),\n            )\n            first_dst_id = self.edge_data.dst.min()\n            last_dst_id = self.edge_data.dst.max()\n            all_dst = np.arange(first_dst_id, last_dst_id + 1)\n            evaluation_set = {}\n            # generate a list of negative destinations for each positive edge\n            pos_edge_tqdm = tqdm(\n                zip(pos_src, pos_dst, pos_timestamp, edge_type), total=len(pos_src)\n            )\n\n            for (\n                pos_s,\n                pos_d,\n                pos_t,\n                edge_type,\n            ) in pos_edge_tqdm:\n                t_mask = pos_timestamp == pos_t\n                src_mask = pos_src == pos_s\n                fn_mask = np.logical_and(t_mask, src_mask)\n                pos_e_dst_same_src = pos_dst[fn_mask]\n                filtered_all_dst = np.setdiff1d(all_dst, pos_e_dst_same_src)\n                if (self.num_neg_e &gt; len(filtered_all_dst)):\n                    neg_d_arr = filtered_all_dst\n                else:\n                    neg_d_arr = np.random.choice(\n                    filtered_all_dst, self.num_neg_e, replace=False) #never replace negatives\n                evaluation_set[(pos_t, pos_s, edge_type)] = neg_d_arr\n            save_pkl(evaluation_set, filename)\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.thg_negative_generator.THGNegativeEdgeGenerator.__init__","title":"<code>__init__(dataset_name, first_node_id, last_node_id, node_type, strategy='node-type-filtered', num_neg_e=-1, rnd_seed=1, edge_data=None)</code>","text":"<p>Negative Edge Generator class for Temporal Heterogeneous Graphs this is a class for generating negative samples for a specific datasets the set of the positive samples are provided, the negative samples are generated with specific strategies  and are saved for consistent evaluation across different methods</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>name of the dataset</p> required <code>first_node_id</code> <code>int</code> <p>the first node id</p> required <code>last_node_id</code> <code>int</code> <p>the last node id</p> required <code>node_type</code> <code>Union[ndarray, Tensor]</code> <p>the node type of each node</p> required <code>strategy</code> <code>str</code> <p>the strategy to generate negative samples</p> <code>'node-type-filtered'</code> <code>num_neg_e</code> <code>int</code> <p>number of negative samples to generate</p> <code>-1</code> <code>rnd_seed</code> <code>int</code> <p>random seed</p> <code>1</code> <code>edge_data</code> <code>TemporalData</code> <p>the edge data object containing the positive edges</p> <code>None</code> <p>Returns:     None</p> Source code in <code>tgb/linkproppred/thg_negative_generator.py</code> <pre><code>def __init__(\n    self,\n    dataset_name: str,\n    first_node_id: int,\n    last_node_id: int,\n    node_type: Union[np.ndarray, torch.Tensor],\n    strategy: str = \"node-type-filtered\",\n    num_neg_e: int = -1,  # -1 means generate all possible negatives\n    rnd_seed: int = 1,\n    edge_data: TemporalData = None,\n) -&gt; None:\n    r\"\"\"\n    Negative Edge Generator class for Temporal Heterogeneous Graphs\n    this is a class for generating negative samples for a specific datasets\n    the set of the positive samples are provided, the negative samples are generated with specific strategies \n    and are saved for consistent evaluation across different methods\n\n    Parameters:\n        dataset_name: name of the dataset\n        first_node_id: the first node id\n        last_node_id: the last node id\n        node_type: the node type of each node\n        strategy: the strategy to generate negative samples\n        num_neg_e: number of negative samples to generate\n        rnd_seed: random seed\n        edge_data: the edge data object containing the positive edges\n    Returns:\n        None\n    \"\"\"\n    self.rnd_seed = rnd_seed\n    np.random.seed(self.rnd_seed)\n    self.dataset_name = dataset_name\n    self.first_node_id = first_node_id\n    self.last_node_id = last_node_id\n    if isinstance(node_type, torch.Tensor):\n        node_type = node_type.cpu().numpy()\n    self.node_type = node_type\n    self.node_type_dict = self.get_destinations_based_on_node_type(first_node_id, last_node_id, self.node_type) # {node_type: {nid:1}}\n    assert isinstance(self.node_type, np.ndarray), \"node_type should be a numpy array\"\n    self.num_neg_e = num_neg_e  #-1 means generate all \n\n    assert strategy in [\n        \"node-type-filtered\",\n        \"random\",\n    ], \"The supported strategies are `node-type-filtered`\"\n    self.strategy = strategy\n    self.edge_data = edge_data\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.thg_negative_generator.THGNegativeEdgeGenerator.generate_negative_samples","title":"<code>generate_negative_samples(pos_edges, split_mode, partial_path)</code>","text":"<p>Generate negative samples</p> <p>Parameters:</p> Name Type Description Default <code>pos_edges</code> <code>TemporalData</code> <p>positive edges to generate the negatives for</p> required <code>split_mode</code> <code>str</code> <p>specifies whether to generate negative edges for 'validation' or 'test' splits</p> required <code>partial_path</code> <code>str</code> <p>in which directory save the generated negatives</p> required Source code in <code>tgb/linkproppred/thg_negative_generator.py</code> <pre><code>def generate_negative_samples(self, \n                              pos_edges: TemporalData,\n                              split_mode: str, \n                              partial_path: str,\n                              ) -&gt; None:\n    r\"\"\"\n    Generate negative samples\n\n    Parameters:\n        pos_edges: positive edges to generate the negatives for\n        split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n        partial_path: in which directory save the generated negatives\n    \"\"\"\n    # file name for saving or loading...\n    filename = (\n        partial_path\n        + \"/\"\n        + self.dataset_name\n        + \"_\"\n        + split_mode\n        + \"_\"\n        + \"ns\"\n        + \".pkl\"\n    )\n\n    if self.strategy == \"node-type-filtered\":\n        self.generate_negative_samples_nt(pos_edges, split_mode, filename)\n    elif self.strategy == \"random\":\n        self.generate_negative_samples_random(pos_edges, split_mode, filename)\n    else:\n        raise ValueError(\"Unsupported negative sample generation strategy!\")\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.thg_negative_generator.THGNegativeEdgeGenerator.generate_negative_samples_nt","title":"<code>generate_negative_samples_nt(data, split_mode, filename)</code>","text":"<p>now we consider (s, d, t, edge_type) as a unique edge, also adding the node type info for the destination node for convenience so (s, d, t, edge_type): (conflict_set, d_node_type) Generate negative samples based on the random strategy:     - for each positive edge, retrieve all possible destinations based on the node type of the destination node     - filter actual positive edges at the same timestamp with the same edge type</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TemporalData</code> <p>an object containing positive edges information</p> required <code>split_mode</code> <code>str</code> <p>specifies whether to generate negative edges for 'validation' or 'test' splits</p> required <code>filename</code> <code>str</code> <p>name of the file containing the generated negative edges</p> required Source code in <code>tgb/linkproppred/thg_negative_generator.py</code> <pre><code>def generate_negative_samples_nt(self, \n                                  data: TemporalData, \n                                  split_mode: str, \n                                  filename: str,\n                                  ) -&gt; None:\n    r\"\"\"\n    now we consider (s, d, t, edge_type) as a unique edge, also adding the node type info for the destination node for convenience so (s, d, t, edge_type): (conflict_set, d_node_type)\n    Generate negative samples based on the random strategy:\n        - for each positive edge, retrieve all possible destinations based on the node type of the destination node\n        - filter actual positive edges at the same timestamp with the same edge type\n\n    Parameters:\n        data: an object containing positive edges information\n        split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n        filename: name of the file containing the generated negative edges\n    \"\"\"\n    print(\n        f\"INFO: Negative Sampling Strategy: {self.strategy}, Data Split: {split_mode}\"\n    )\n    assert split_mode in [\n        \"val\",\n        \"test\",\n    ], \"Invalid split-mode! It should be `val` or `test`!\"\n\n    if os.path.exists(filename):\n        print(\n            f\"INFO: negative samples for '{split_mode}' evaluation are already generated!\"\n        )\n    else:\n        print(f\"INFO: Generating negative samples for '{split_mode}' evaluation!\")\n        # retrieve the information from the batch\n        pos_src, pos_dst, pos_timestamp, edge_type = (\n            data.src.cpu().numpy(),\n            data.dst.cpu().numpy(),\n            data.t.cpu().numpy(),\n            data.edge_type.cpu().numpy(),\n        )\n\n        # generate a list of negative destinations for each positive edge\n        pos_edge_tqdm = tqdm(\n            zip(pos_src, pos_dst, pos_timestamp, edge_type), total=len(pos_src)\n        )\n\n        edge_t_dict = {} # {(t, u, edge_type): {v_1, v_2, ..} }\n        #! iterate once to put all edges into a dictionary for reference\n        for (\n            pos_s,\n            pos_d,\n            pos_t,\n            edge_type,\n        ) in pos_edge_tqdm:\n            if (pos_t, pos_s, edge_type) not in edge_t_dict:\n                edge_t_dict[(pos_t, pos_s, edge_type)] = {pos_d:1}\n            else:\n                edge_t_dict[(pos_t, pos_s, edge_type)][pos_d] = 1\n\n        out_dict = {}\n        for key in tqdm(edge_t_dict):\n            conflict_set = np.array(list(edge_t_dict[key].keys()))\n            pos_d = conflict_set[0]\n            #* retieve the node type of the destination node as well \n            #! assumption, same edge type = same destination node type\n            d_node_type = int(self.node_type[pos_d - self.first_node_id])\n            all_dst = self.node_type_dict[d_node_type]\n            if (self.num_neg_e == -1):\n                filtered_all_dst = np.setdiff1d(all_dst, conflict_set)\n            else:\n                #* lazy sampling\n                neg_d_arr = np.random.choice(\n                    all_dst, self.num_neg_e, replace=False) #never replace negatives\n                if len(np.setdiff1d(neg_d_arr, conflict_set)) &lt; self.num_neg_e:\n                    neg_d_arr = np.random.choice(\n                        np.setdiff1d(all_dst, conflict_set), self.num_neg_e, replace=False)\n                filtered_all_dst = neg_d_arr\n            out_dict[key] = filtered_all_dst\n        print (\"ns samples for \", len(out_dict), \" positive edges are generated\")\n        # save the generated evaluation set to disk\n        save_pkl(out_dict, filename)\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.thg_negative_generator.THGNegativeEdgeGenerator.generate_negative_samples_random","title":"<code>generate_negative_samples_random(data, split_mode, filename)</code>","text":"<p>generate random negative edges for ablation study</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>TemporalData</code> <p>an object containing positive edges information</p> required <code>split_mode</code> <code>str</code> <p>specifies whether to generate negative edges for 'validation' or 'test' splits</p> required <code>filename</code> <code>str</code> <p>name of the file containing the generated negative edges</p> required Source code in <code>tgb/linkproppred/thg_negative_generator.py</code> <pre><code>def generate_negative_samples_random(self, \n                                  data: TemporalData, \n                                  split_mode: str, \n                                  filename: str,\n                                  ) -&gt; None:\n    r\"\"\"\n    generate random negative edges for ablation study\n\n    Parameters:\n        data: an object containing positive edges information\n        split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n        filename: name of the file containing the generated negative edges\n    \"\"\"\n    print(\n        f\"INFO: Negative Sampling Strategy: {self.strategy}, Data Split: {split_mode}\"\n    )\n    assert split_mode in [\n        \"val\",\n        \"test\",\n    ], \"Invalid split-mode! It should be `val` or `test`!\"\n\n    if os.path.exists(filename):\n        print(\n            f\"INFO: negative samples for '{split_mode}' evaluation are already generated!\"\n        )\n    else:\n        print(f\"INFO: Generating negative samples for '{split_mode}' evaluation!\")\n        # retrieve the information from the batch\n        pos_src, pos_dst, pos_timestamp, edge_type = (\n            data.src.cpu().numpy(),\n            data.dst.cpu().numpy(),\n            data.t.cpu().numpy(),\n            data.edge_type.cpu().numpy(),\n        )\n        first_dst_id = self.edge_data.dst.min()\n        last_dst_id = self.edge_data.dst.max()\n        all_dst = np.arange(first_dst_id, last_dst_id + 1)\n        evaluation_set = {}\n        # generate a list of negative destinations for each positive edge\n        pos_edge_tqdm = tqdm(\n            zip(pos_src, pos_dst, pos_timestamp, edge_type), total=len(pos_src)\n        )\n\n        for (\n            pos_s,\n            pos_d,\n            pos_t,\n            edge_type,\n        ) in pos_edge_tqdm:\n            t_mask = pos_timestamp == pos_t\n            src_mask = pos_src == pos_s\n            fn_mask = np.logical_and(t_mask, src_mask)\n            pos_e_dst_same_src = pos_dst[fn_mask]\n            filtered_all_dst = np.setdiff1d(all_dst, pos_e_dst_same_src)\n            if (self.num_neg_e &gt; len(filtered_all_dst)):\n                neg_d_arr = filtered_all_dst\n            else:\n                neg_d_arr = np.random.choice(\n                filtered_all_dst, self.num_neg_e, replace=False) #never replace negatives\n            evaluation_set[(pos_t, pos_s, edge_type)] = neg_d_arr\n        save_pkl(evaluation_set, filename)\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.thg_negative_generator.THGNegativeEdgeGenerator.get_destinations_based_on_node_type","title":"<code>get_destinations_based_on_node_type(first_node_id, last_node_id, node_type)</code>","text":"<p>get the destination node id arrays based on the node type Parameters:     first_node_id: the first node id     last_node_id: the last node id     node_type: the node type of each node</p> <p>Returns:</p> Name Type Description <code>node_type_dict</code> <code>dict</code> <p>a dictionary containing the destination node ids for each node type</p> Source code in <code>tgb/linkproppred/thg_negative_generator.py</code> <pre><code>def get_destinations_based_on_node_type(self, \n                                        first_node_id: int,\n                                        last_node_id: int,\n                                        node_type: np.ndarray) -&gt; dict:\n    r\"\"\"\n    get the destination node id arrays based on the node type\n    Parameters:\n        first_node_id: the first node id\n        last_node_id: the last node id\n        node_type: the node type of each node\n\n    Returns:\n        node_type_dict: a dictionary containing the destination node ids for each node type\n    \"\"\"\n    node_type_store = {}\n    assert first_node_id &lt;= last_node_id, \"Invalid destination node ids!\"\n    assert len(node_type) == (last_node_id - first_node_id + 1), \"node type array must match the indices\"\n    for k in range(len(node_type)):\n        nt = int(node_type[k]) #node type must be ints\n        nid = k + first_node_id\n        if nt not in node_type_store:\n            node_type_store[nt] = {nid:1}\n        else:\n            node_type_store[nt][nid] = 1\n    node_type_dict = {}\n    for ntype in node_type_store:\n        node_type_dict[ntype] = np.array(list(node_type_store[ntype].keys()))\n        assert np.all(np.diff(node_type_dict[ntype]) &gt;= 0), \"Destination node ids for a given type must be sorted\"\n        assert np.all(node_type_dict[ntype] &lt;= last_node_id), \"Destination node ids must be less than or equal to the last destination id\"\n    return node_type_dict\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.thg_negative_sampler.THGNegativeEdgeSampler","title":"<code>THGNegativeEdgeSampler</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>tgb/linkproppred/thg_negative_sampler.py</code> <pre><code>class THGNegativeEdgeSampler(object):\n    def __init__(\n        self,\n        dataset_name: str,\n        first_node_id: int,\n        last_node_id: int,\n        node_type: np.ndarray,\n        strategy: str = \"node-type-filtered\",\n    ) -&gt; None:\n        r\"\"\"\n        Negative Edge Sampler\n            Loads and query the negative batches based on the positive batches provided.\n            constructor for the negative edge sampler class\n\n        Parameters:\n            dataset_name: name of the dataset\n            first_node_id: identity of the first node\n            last_node_id: indentity of the last destination node\n            node_type: the node type of each node\n            strategy: will always load the pre-generated negatives\n\n        Returns:\n            None\n        \"\"\"\n        self.dataset_name = dataset_name\n        self.eval_set = {}\n        self.first_node_id = first_node_id\n        self.last_node_id = last_node_id\n        self.node_type = node_type\n        assert isinstance(self.node_type, np.ndarray), \"node_type should be a numpy array\"\n\n    def load_eval_set(\n        self,\n        fname: str,\n        split_mode: str = \"val\",\n    ) -&gt; None:\n        r\"\"\"\n        Load the evaluation set from disk, can be either val or test set ns samples\n        Parameters:\n            fname: the file name of the evaluation ns on disk\n            split_mode: the split mode of the evaluation set, can be either `val` or `test`\n\n        Returns:\n            None\n        \"\"\"\n        assert split_mode in [\n            \"val\",\n            \"test\",\n        ], \"Invalid split-mode! It should be `val`, `test`\"\n        if not os.path.exists(fname):\n            raise FileNotFoundError(f\"File not found at {fname}\")\n        self.eval_set[split_mode] = load_pkl(fname)\n\n    def query_batch(self, \n                    pos_src: Union[Tensor, np.ndarray], \n                    pos_dst: Union[Tensor, np.ndarray], \n                    pos_timestamp: Union[Tensor, np.ndarray], \n                    edge_type: Union[Tensor, np.ndarray],\n                    split_mode: str = \"test\") -&gt; list:\n        r\"\"\"\n        For each positive edge in the `pos_batch`, return a list of negative edges\n        `split_mode` specifies whether the valiation or test evaluation set should be retrieved.\n        modify now to include edge type argument\n\n        Parameters:\n            pos_src: list of positive source nodes\n            pos_dst: list of positive destination nodes\n            pos_timestamp: list of timestamps of the positive edges\n            split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n\n        Returns:\n            neg_samples: list of numpy array; each array contains the set of negative edges that\n                        should be evaluated against each positive edge.\n        \"\"\"\n        assert split_mode in [\n            \"val\",\n            \"test\",\n        ], \"Invalid split-mode! It should be `val`, `test`!\"\n        if self.eval_set[split_mode] == None:\n            raise ValueError(\n                f\"Evaluation set is None! You should load the {split_mode} evaluation set first!\"\n            )\n\n        # check the argument types...\n        if torch is not None and isinstance(pos_src, torch.Tensor):\n            pos_src = pos_src.detach().cpu().numpy()\n        if torch is not None and isinstance(pos_dst, torch.Tensor):\n            pos_dst = pos_dst.detach().cpu().numpy()\n        if torch is not None and isinstance(pos_timestamp, torch.Tensor):\n            pos_timestamp = pos_timestamp.detach().cpu().numpy()\n        if torch is not None and isinstance(edge_type, torch.Tensor):\n            edge_type = edge_type.detach().cpu().numpy()\n\n        if not isinstance(pos_src, np.ndarray) or not isinstance(pos_dst, np.ndarray) or not(pos_timestamp, np.ndarray) or not(edge_type, np.ndarray):\n            raise RuntimeError(\n                \"pos_src, pos_dst, and pos_timestamp need to be either numpy ndarray or torch tensor!\"\n                )\n\n        neg_samples = []\n        for pos_s, pos_d, pos_t, e_type in zip(pos_src, pos_dst, pos_timestamp, edge_type):\n            if (pos_t, pos_s, e_type) not in self.eval_set[split_mode]:\n                raise ValueError(\n                    f\"The edge ({pos_s}, {pos_d}, {pos_t}, {e_type}) is not in the '{split_mode}' evaluation set! Please check the implementation.\"\n                )\n            else:\n                filtered_dst = self.eval_set[split_mode]\n                neg_d_arr = filtered_dst[(pos_t, pos_s, e_type)]\n                neg_samples.append(\n                        neg_d_arr\n                    )\n\n        #? can't convert to numpy array due to different lengths of negative samples\n        return neg_samples\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.thg_negative_sampler.THGNegativeEdgeSampler.__init__","title":"<code>__init__(dataset_name, first_node_id, last_node_id, node_type, strategy='node-type-filtered')</code>","text":"<p>Negative Edge Sampler     Loads and query the negative batches based on the positive batches provided.     constructor for the negative edge sampler class</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>name of the dataset</p> required <code>first_node_id</code> <code>int</code> <p>identity of the first node</p> required <code>last_node_id</code> <code>int</code> <p>indentity of the last destination node</p> required <code>node_type</code> <code>ndarray</code> <p>the node type of each node</p> required <code>strategy</code> <code>str</code> <p>will always load the pre-generated negatives</p> <code>'node-type-filtered'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>tgb/linkproppred/thg_negative_sampler.py</code> <pre><code>def __init__(\n    self,\n    dataset_name: str,\n    first_node_id: int,\n    last_node_id: int,\n    node_type: np.ndarray,\n    strategy: str = \"node-type-filtered\",\n) -&gt; None:\n    r\"\"\"\n    Negative Edge Sampler\n        Loads and query the negative batches based on the positive batches provided.\n        constructor for the negative edge sampler class\n\n    Parameters:\n        dataset_name: name of the dataset\n        first_node_id: identity of the first node\n        last_node_id: indentity of the last destination node\n        node_type: the node type of each node\n        strategy: will always load the pre-generated negatives\n\n    Returns:\n        None\n    \"\"\"\n    self.dataset_name = dataset_name\n    self.eval_set = {}\n    self.first_node_id = first_node_id\n    self.last_node_id = last_node_id\n    self.node_type = node_type\n    assert isinstance(self.node_type, np.ndarray), \"node_type should be a numpy array\"\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.thg_negative_sampler.THGNegativeEdgeSampler.load_eval_set","title":"<code>load_eval_set(fname, split_mode='val')</code>","text":"<p>Load the evaluation set from disk, can be either val or test set ns samples Parameters:     fname: the file name of the evaluation ns on disk     split_mode: the split mode of the evaluation set, can be either <code>val</code> or <code>test</code></p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>tgb/linkproppred/thg_negative_sampler.py</code> <pre><code>def load_eval_set(\n    self,\n    fname: str,\n    split_mode: str = \"val\",\n) -&gt; None:\n    r\"\"\"\n    Load the evaluation set from disk, can be either val or test set ns samples\n    Parameters:\n        fname: the file name of the evaluation ns on disk\n        split_mode: the split mode of the evaluation set, can be either `val` or `test`\n\n    Returns:\n        None\n    \"\"\"\n    assert split_mode in [\n        \"val\",\n        \"test\",\n    ], \"Invalid split-mode! It should be `val`, `test`\"\n    if not os.path.exists(fname):\n        raise FileNotFoundError(f\"File not found at {fname}\")\n    self.eval_set[split_mode] = load_pkl(fname)\n</code></pre>"},{"location":"api/tgb.linkproppred/#tgb.linkproppred.thg_negative_sampler.THGNegativeEdgeSampler.query_batch","title":"<code>query_batch(pos_src, pos_dst, pos_timestamp, edge_type, split_mode='test')</code>","text":"<p>For each positive edge in the <code>pos_batch</code>, return a list of negative edges <code>split_mode</code> specifies whether the valiation or test evaluation set should be retrieved. modify now to include edge type argument</p> <p>Parameters:</p> Name Type Description Default <code>pos_src</code> <code>Union[Tensor, ndarray]</code> <p>list of positive source nodes</p> required <code>pos_dst</code> <code>Union[Tensor, ndarray]</code> <p>list of positive destination nodes</p> required <code>pos_timestamp</code> <code>Union[Tensor, ndarray]</code> <p>list of timestamps of the positive edges</p> required <code>split_mode</code> <code>str</code> <p>specifies whether to generate negative edges for 'validation' or 'test' splits</p> <code>'test'</code> <p>Returns:</p> Name Type Description <code>neg_samples</code> <code>list</code> <p>list of numpy array; each array contains the set of negative edges that         should be evaluated against each positive edge.</p> Source code in <code>tgb/linkproppred/thg_negative_sampler.py</code> <pre><code>def query_batch(self, \n                pos_src: Union[Tensor, np.ndarray], \n                pos_dst: Union[Tensor, np.ndarray], \n                pos_timestamp: Union[Tensor, np.ndarray], \n                edge_type: Union[Tensor, np.ndarray],\n                split_mode: str = \"test\") -&gt; list:\n    r\"\"\"\n    For each positive edge in the `pos_batch`, return a list of negative edges\n    `split_mode` specifies whether the valiation or test evaluation set should be retrieved.\n    modify now to include edge type argument\n\n    Parameters:\n        pos_src: list of positive source nodes\n        pos_dst: list of positive destination nodes\n        pos_timestamp: list of timestamps of the positive edges\n        split_mode: specifies whether to generate negative edges for 'validation' or 'test' splits\n\n    Returns:\n        neg_samples: list of numpy array; each array contains the set of negative edges that\n                    should be evaluated against each positive edge.\n    \"\"\"\n    assert split_mode in [\n        \"val\",\n        \"test\",\n    ], \"Invalid split-mode! It should be `val`, `test`!\"\n    if self.eval_set[split_mode] == None:\n        raise ValueError(\n            f\"Evaluation set is None! You should load the {split_mode} evaluation set first!\"\n        )\n\n    # check the argument types...\n    if torch is not None and isinstance(pos_src, torch.Tensor):\n        pos_src = pos_src.detach().cpu().numpy()\n    if torch is not None and isinstance(pos_dst, torch.Tensor):\n        pos_dst = pos_dst.detach().cpu().numpy()\n    if torch is not None and isinstance(pos_timestamp, torch.Tensor):\n        pos_timestamp = pos_timestamp.detach().cpu().numpy()\n    if torch is not None and isinstance(edge_type, torch.Tensor):\n        edge_type = edge_type.detach().cpu().numpy()\n\n    if not isinstance(pos_src, np.ndarray) or not isinstance(pos_dst, np.ndarray) or not(pos_timestamp, np.ndarray) or not(edge_type, np.ndarray):\n        raise RuntimeError(\n            \"pos_src, pos_dst, and pos_timestamp need to be either numpy ndarray or torch tensor!\"\n            )\n\n    neg_samples = []\n    for pos_s, pos_d, pos_t, e_type in zip(pos_src, pos_dst, pos_timestamp, edge_type):\n        if (pos_t, pos_s, e_type) not in self.eval_set[split_mode]:\n            raise ValueError(\n                f\"The edge ({pos_s}, {pos_d}, {pos_t}, {e_type}) is not in the '{split_mode}' evaluation set! Please check the implementation.\"\n            )\n        else:\n            filtered_dst = self.eval_set[split_mode]\n            neg_d_arr = filtered_dst[(pos_t, pos_s, e_type)]\n            neg_samples.append(\n                    neg_d_arr\n                )\n\n    #? can't convert to numpy array due to different lengths of negative samples\n    return neg_samples\n</code></pre>"},{"location":"api/tgb.nodeproppred/","title":"<code>tgb.nodeproppred</code>","text":""},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset","title":"<code>NodePropPredDataset</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>tgb/nodeproppred/dataset.py</code> <pre><code>class NodePropPredDataset(object):\n    def __init__(\n        self,\n        name: str,\n        root: Optional[str] = \"datasets\",\n        meta_dict: Optional[dict] = None,\n        preprocess: Optional[bool] = True,\n    ) -&gt; None:\n        r\"\"\"Dataset class for the node property prediction task. Stores meta information about each dataset such as evaluation metrics etc.\n        also automatically pre-processes the dataset.\n        [!] node property prediction datasets requires the following:\n        self.meta_dict[\"fname\"]: path to the edge list file\n        self.meta_dict[\"nodefile\"]: path to the node label file\n\n        Parameters:\n            name: name of the dataset\n            root: root directory to store the dataset folder\n            meta_dict: dictionary containing meta information about the dataset, should contain key 'dir_name' which is the name of the dataset folder\n            preprocess: whether to pre-process the dataset\n        Returns:\n            None\n        \"\"\"\n        self.name = name  ## original name\n        # check if dataset url exist\n        if self.name in DATA_URL_DICT:\n            self.url = DATA_URL_DICT[self.name]\n        else:\n            self.url = None\n            print(f\"Dataset {self.name} url not found, download not supported yet.\")\n\n        # check if the evaluatioin metric are specified\n        if self.name in DATA_EVAL_METRIC_DICT:\n            self.metric = DATA_EVAL_METRIC_DICT[self.name]\n        else:\n            self.metric = None\n            print(\n                f\"Dataset {self.name} default evaluation metric not found, it is not supported yet.\"\n            )\n\n        root = PROJ_DIR + root\n\n        if meta_dict is None:\n            self.dir_name = \"_\".join(name.split(\"-\"))  ## replace hyphen with underline\n            meta_dict = {\"dir_name\": self.dir_name}\n        else:\n            self.dir_name = meta_dict[\"dir_name\"]\n        self.root = osp.join(root, self.dir_name)\n        self.meta_dict = meta_dict\n        if \"fname\" not in self.meta_dict:\n            self.meta_dict[\"fname\"] = self.root + \"/\" + self.name + \"_edgelist.csv\"\n            self.meta_dict[\"nodefile\"] = self.root + \"/\" + self.name + \"_node_labels.csv\"\n\n         #! version check\n        self.version_passed = True\n        self._version_check()\n\n        self._num_classes = DATA_NUM_CLASSES[self.name]\n\n        # initialize\n        self._node_feat = None\n        self._edge_feat = None\n        self._full_data = None\n        self.download()\n        # check if the root directory exists, if not create it\n        if osp.isdir(self.root):\n            print(\"Dataset directory is \", self.root)\n        else:\n            raise FileNotFoundError(f\"Directory not found at {self.root}\")\n\n        if preprocess:\n            self.pre_process()\n\n        self.label_ts_idx = 0  # index for which node lables to return now\n\n    def _version_check(self) -&gt; None:\n        r\"\"\"Implement Version checks for dataset files\n        updates the file names based on the current version number\n        prompt the user to download the new version via self.version_passed variable\n        \"\"\"\n        if (self.name in DATA_VERSION_DICT):\n            version = DATA_VERSION_DICT[self.name]\n        else:\n            print(f\"Dataset {self.name} version number not found.\")\n            self.version_passed = False\n            return None\n\n        if (version &gt; 1):\n            #* check if current version is outdated\n            self.meta_dict[\"fname\"] = self.root + \"/\" + self.name + \"_edgelist_v\" + str(int(version)) + \".csv\"\n            self.meta_dict[\"nodefile\"] = self.root + \"/\" + self.name + \"_node_labels_v\" + str(int(version)) + \".csv\"\n\n            if (not osp.exists(self.meta_dict[\"fname\"])):\n                print(f\"Dataset {self.name} version {int(version)} not found.\")\n                print(f\"Please download the latest version of the dataset.\")\n                self.version_passed = False\n                return None\n\n    def download(self) -&gt; None:\n        r\"\"\"\n        downloads this dataset from url\n        check if files are already downloaded\n        Returns:\n            None\n        \"\"\"\n        # check if the file already exists\n        if osp.exists(self.meta_dict[\"fname\"]) and osp.exists(\n            self.meta_dict[\"nodefile\"]\n        ):\n            print(\"raw file found, skipping download\")\n            return\n\n        else:\n            inp = input(\n                \"Will you download the dataset(s) now? (y/N)\\n\"\n            ).lower()  # ask if the user wants to download the dataset\n            if inp == \"y\":\n                print(\n                    f\"{BColors.WARNING}Download started, this might take a while . . . {BColors.ENDC}\"\n                )\n                print(f\"Dataset title: {self.name}\")\n\n                if self.url is None:\n                    raise Exception(\n                        \"Dataset url not found, download not supported yet.\"\n                    )\n                else:\n                    r = requests.get(self.url, stream=True)\n                    if osp.isdir(self.root):\n                        print(\"Dataset directory is \", self.root)\n                    else:\n                        os.makedirs(self.root)\n\n                    path_download = self.root + \"/\" + self.name + \".zip\"\n                    with open(path_download, \"wb\") as f:\n                        total_length = int(r.headers.get(\"content-length\"))\n                        for chunk in progress.bar(\n                            r.iter_content(chunk_size=1024),\n                            expected_size=(total_length / 1024) + 1,\n                        ):\n                            if chunk:\n                                f.write(chunk)\n                                f.flush()\n                    # for unzipping the file\n                    with zipfile.ZipFile(path_download, \"r\") as zip_ref:\n                        zip_ref.extractall(self.root)\n                    print(f\"{BColors.OKGREEN}Download completed {BColors.ENDC}\")\n            else:\n                raise Exception(\n                    BColors.FAIL\n                    + \"Data not found error, download \"\n                    + self.name\n                    + \" failed\"\n                )\n\n    def generate_processed_files(\n        self,\n    ) -&gt; Tuple[pd.DataFrame, Dict[int, Dict[str, Any]]]:\n        r\"\"\"\n        returns an edge list of pandas data frame\n        Returns:\n            df: pandas data frame storing the temporal edge list\n            node_label_dict: dictionary with key as timestamp and item as dictionary of node labels\n        \"\"\"\n        OUT_DF = self.root + \"/\" + \"ml_{}.pkl\".format(self.name)\n        OUT_NODE_DF = self.root + \"/\" + \"ml_{}_node.pkl\".format(self.name)\n        OUT_LABEL_DF = self.root + \"/\" + \"ml_{}_label.pkl\".format(self.name)\n        OUT_EDGE_FEAT = self.root + \"/\" + \"ml_{}.pkl\".format(self.name + \"_edge\")\n\n        # * logic for large datasets, as node label file is too big to store on disc\n        if self.name == \"tgbn-reddit\" or self.name == \"tgbn-token\":\n            if osp.exists(OUT_DF) and osp.exists(OUT_NODE_DF) and osp.exists(OUT_EDGE_FEAT):\n                df = pd.read_pickle(OUT_DF)\n                edge_feat = load_pkl(OUT_EDGE_FEAT)\n                if (self.name == \"tgbn-token\"):\n                    #! taking log normalization for numerical stability\n                    print (\"applying log normalization for weights in tgbn-token\")\n                    edge_feat[:,0] = np.log(edge_feat[:,0])\n                node_ids = load_pkl(OUT_NODE_DF)\n                labels_dict = load_pkl(OUT_LABEL_DF)\n                node_label_dict = load_label_dict(\n                    self.meta_dict[\"nodefile\"], node_ids, labels_dict\n                )\n                return df, node_label_dict, edge_feat\n\n        # * load the preprocessed file if possible\n        if osp.exists(OUT_DF) and osp.exists(OUT_NODE_DF) and osp.exists(OUT_EDGE_FEAT):\n            print(\"loading processed file\")\n            df = pd.read_pickle(OUT_DF)\n            node_label_dict = load_pkl(OUT_NODE_DF)\n            edge_feat = load_pkl(OUT_EDGE_FEAT)\n        else:  # * process the file\n            print(\"file not processed, generating processed file\")\n            if self.name == \"tgbn-reddit\":\n                df, edge_feat, node_ids, labels_dict = load_edgelist_sr(\n                    self.meta_dict[\"fname\"], label_size=self._num_classes\n                )\n            elif self.name == \"tgbn-token\":\n                df, edge_feat, node_ids, labels_dict = load_edgelist_token(\n                    self.meta_dict[\"fname\"], label_size=self._num_classes\n                )\n            elif self.name == \"tgbn-genre\":\n                df, edge_feat, node_ids, labels_dict = load_edgelist_datetime(\n                    self.meta_dict[\"fname\"], label_size=self._num_classes\n                )\n            elif self.name == \"tgbn-trade\":\n                df, edge_feat, node_ids = load_edgelist_trade(\n                    self.meta_dict[\"fname\"], label_size=self._num_classes\n                )\n\n            df.to_pickle(OUT_DF)\n            save_pkl(edge_feat, OUT_EDGE_FEAT)\n\n            if self.name == \"tgbn-trade\":\n                node_label_dict = load_trade_label_dict(\n                    self.meta_dict[\"nodefile\"], node_ids\n                )\n            else:\n                node_label_dict = load_label_dict(\n                    self.meta_dict[\"nodefile\"], node_ids, labels_dict\n                )\n\n            if (\n                self.name != \"tgbn-reddit\" and self.name != \"tgbn-token\"\n            ):  # don't save subreddits on disc, the node label file is too big\n                save_pkl(node_label_dict, OUT_NODE_DF)\n            else:\n                save_pkl(node_ids, OUT_NODE_DF)\n                save_pkl(labels_dict, OUT_LABEL_DF)\n\n            print(\"file processed and saved\")\n        return df, node_label_dict, edge_feat\n\n    def pre_process(self) -&gt; None:\n        \"\"\"\n        Pre-process the dataset and generates the splits, must be run before dataset properties can be accessed\n        Returns:\n            None\n        \"\"\"\n        # first check if all files exist\n        if (\"fname\" not in self.meta_dict) or (\"nodefile\" not in self.meta_dict):\n            raise Exception(\"meta_dict does not contain all required filenames\")\n\n        df, node_label_dict, edge_feat = self.generate_processed_files()\n        sources = np.array(df[\"u\"])\n        destinations = np.array(df[\"i\"])\n        timestamps = np.array(df[\"ts\"])\n        edge_idxs = np.array(df[\"idx\"])\n        edge_label = np.ones(sources.shape[0])\n        #self._edge_feat = np.array(df[\"w\"])\n        self._edge_feat = edge_feat\n\n        full_data = {\n            \"sources\": sources,\n            \"destinations\": destinations,\n            \"timestamps\": timestamps,\n            \"edge_idxs\": edge_idxs,\n            \"edge_feat\": self._edge_feat,\n            \"edge_label\": edge_label,\n        }\n        self._full_data = full_data\n\n        # storing the split masks\n        _train_mask, _val_mask, _test_mask = self.generate_splits(full_data)\n\n        self._train_mask = _train_mask\n        self._val_mask = _val_mask\n        self._test_mask = _test_mask\n\n        self.label_dict = node_label_dict\n        self.label_ts = np.array(list(node_label_dict.keys()))\n        self.label_ts = np.sort(self.label_ts)\n\n    def generate_splits(\n        self,\n        full_data: Dict[str, Any],\n        val_ratio: float = 0.15,\n        test_ratio: float = 0.15,\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        r\"\"\"\n        Generates train, validation, and test splits from the full dataset\n        Parameters:\n            full_data: dictionary containing the full dataset\n            val_ratio: ratio of validation data\n            test_ratio: ratio of test data\n        Returns:\n            train_mask: boolean mask for training data\n            val_mask: boolean mask for validation data\n            test_mask: boolean mask for test data\n        \"\"\"\n        val_time, test_time = list(\n            np.quantile(\n                full_data[\"timestamps\"],\n                [(1 - val_ratio - test_ratio), (1 - test_ratio)],\n            )\n        )\n        timestamps = full_data[\"timestamps\"]\n        train_mask = timestamps &lt;= val_time\n        val_mask = np.logical_and(timestamps &lt;= test_time, timestamps &gt; val_time)\n        test_mask = timestamps &gt; test_time\n\n        return train_mask, val_mask, test_mask\n\n    def find_next_labels_batch(\n        self,\n        cur_t: int,\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        r\"\"\"\n        this returns the node labels closest to cur_t (for that given day)\n        Parameters:\n            cur_t: current timestamp of the batch of edges\n        Returns:\n            ts: timestamp of the node labels\n            source_idx: node ids\n            labels: the stacked label vectors\n        \"\"\"\n        if self.label_ts_idx &gt;= (self.label_ts.shape[0]):\n            # for query that are after the last batch of labels\n            return None\n        else:\n            ts = self.label_ts[self.label_ts_idx]\n\n        if cur_t &gt;= ts:\n            self.label_ts_idx += 1  # move to the next ts\n            # {ts: {node_id: label_vec}}\n            node_ids = np.array(list(self.label_dict[ts].keys()))\n\n            node_labels = []\n            for key in self.label_dict[ts]:\n                node_labels.append(np.array(self.label_dict[ts][key]))\n            node_labels = np.stack(node_labels, axis=0)\n            label_ts = np.full(node_ids.shape[0], ts, dtype=\"int\")\n            return (label_ts, node_ids, node_labels)\n        else:\n            return None\n\n    def reset_label_time(self) -&gt; None:\n        r\"\"\"\n        reset the pointer for node label once the entire dataset has been iterated once\n        Returns:\n            None\n        \"\"\"\n        self.label_ts_idx = 0\n\n    def return_label_ts(self) -&gt; int:\n        \"\"\"\n        return the current label timestamp that the pointer is at\n        Returns:\n            ts: int, the timestamp of the node labels\n        \"\"\"\n        if (self.label_ts_idx &gt;= self.label_ts.shape[0]):\n            return self.label_ts[-1]\n        else:\n            return self.label_ts[self.label_ts_idx]\n\n    @property\n    def num_classes(self) -&gt; int:\n        \"\"\"\n        number of classes in the node label\n        Returns:\n            num_classes: int, number of classes\n        \"\"\"\n        return self._num_classes\n\n    @property\n    def eval_metric(self) -&gt; str:\n        \"\"\"\n        the official evaluation metric for the dataset, loaded from info.py\n        Returns:\n            eval_metric: str, the evaluation metric\n        \"\"\"\n        return self.metric\n\n    # TODO not sure needed, to be removed\n    @property\n    def node_feat(self) -&gt; Optional[np.ndarray]:\n        r\"\"\"\n        Returns the node features of the dataset with dim [N, feat_dim]\n        Returns:\n            node_feat: np.ndarray, [N, feat_dim] or None if there is no node feature\n        \"\"\"\n        return self._node_feat\n\n    # TODO not sure needed, to be removed\n    @property\n    def edge_feat(self) -&gt; Optional[np.ndarray]:\n        r\"\"\"\n        Returns the edge features of the dataset with dim [E, feat_dim]\n        Returns:\n            edge_feat: np.ndarray, [E, feat_dim] or None if there is no edge feature\n        \"\"\"\n        return self._edge_feat\n\n    @property\n    def full_data(self) -&gt; Dict[str, Any]:\n        r\"\"\"\n        the full data of the dataset as a dictionary with keys: 'sources', 'destinations', 'timestamps', 'edge_idxs', 'edge_feat', 'w', 'edge_label',\n\n        Returns:\n            full_data: Dict[str, Any]\n        \"\"\"\n        if self._full_data is None:\n            raise ValueError(\n                \"dataset has not been processed yet, please call pre_process() first\"\n            )\n        return self._full_data\n\n    @property\n    def train_mask(self) -&gt; np.ndarray:\n        r\"\"\"\n        Returns the train mask of the dataset\n        Returns:\n            train_mask\n        \"\"\"\n        if self._train_mask is None:\n            raise ValueError(\"training split hasn't been loaded\")\n        return self._train_mask\n\n    @property\n    def val_mask(self) -&gt; np.ndarray:\n        r\"\"\"\n        Returns the validation mask of the dataset\n        Returns:\n            val_mask: Dict[str, Any]\n        \"\"\"\n        if self._val_mask is None:\n            raise ValueError(\"validation split hasn't been loaded\")\n\n        return self._val_mask\n\n    @property\n    def test_mask(self) -&gt; np.ndarray:\n        r\"\"\"\n        Returns the test mask of the dataset:\n        Returns:\n            test_mask: Dict[str, Any]\n        \"\"\"\n        if self._test_mask is None:\n            raise ValueError(\"test split hasn't been loaded\")\n\n        return self._test_mask\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.edge_feat","title":"<code>edge_feat: Optional[np.ndarray]</code>  <code>property</code>","text":"<p>Returns the edge features of the dataset with dim [E, feat_dim] Returns:     edge_feat: np.ndarray, [E, feat_dim] or None if there is no edge feature</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.eval_metric","title":"<code>eval_metric: str</code>  <code>property</code>","text":"<p>the official evaluation metric for the dataset, loaded from info.py Returns:     eval_metric: str, the evaluation metric</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.full_data","title":"<code>full_data: Dict[str, Any]</code>  <code>property</code>","text":"<p>the full data of the dataset as a dictionary with keys: 'sources', 'destinations', 'timestamps', 'edge_idxs', 'edge_feat', 'w', 'edge_label',</p> <p>Returns:</p> Name Type Description <code>full_data</code> <code>Dict[str, Any]</code> <p>Dict[str, Any]</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.node_feat","title":"<code>node_feat: Optional[np.ndarray]</code>  <code>property</code>","text":"<p>Returns the node features of the dataset with dim [N, feat_dim] Returns:     node_feat: np.ndarray, [N, feat_dim] or None if there is no node feature</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.num_classes","title":"<code>num_classes: int</code>  <code>property</code>","text":"<p>number of classes in the node label Returns:     num_classes: int, number of classes</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.test_mask","title":"<code>test_mask: np.ndarray</code>  <code>property</code>","text":"<p>Returns the test mask of the dataset: Returns:     test_mask: Dict[str, Any]</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.train_mask","title":"<code>train_mask: np.ndarray</code>  <code>property</code>","text":"<p>Returns the train mask of the dataset Returns:     train_mask</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.val_mask","title":"<code>val_mask: np.ndarray</code>  <code>property</code>","text":"<p>Returns the validation mask of the dataset Returns:     val_mask: Dict[str, Any]</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.__init__","title":"<code>__init__(name, root='datasets', meta_dict=None, preprocess=True)</code>","text":"<p>Dataset class for the node property prediction task. Stores meta information about each dataset such as evaluation metrics etc. also automatically pre-processes the dataset. [!] node property prediction datasets requires the following: self.meta_dict[\"fname\"]: path to the edge list file self.meta_dict[\"nodefile\"]: path to the node label file</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of the dataset</p> required <code>root</code> <code>Optional[str]</code> <p>root directory to store the dataset folder</p> <code>'datasets'</code> <code>meta_dict</code> <code>Optional[dict]</code> <p>dictionary containing meta information about the dataset, should contain key 'dir_name' which is the name of the dataset folder</p> <code>None</code> <code>preprocess</code> <code>Optional[bool]</code> <p>whether to pre-process the dataset</p> <code>True</code> <p>Returns:     None</p> Source code in <code>tgb/nodeproppred/dataset.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    root: Optional[str] = \"datasets\",\n    meta_dict: Optional[dict] = None,\n    preprocess: Optional[bool] = True,\n) -&gt; None:\n    r\"\"\"Dataset class for the node property prediction task. Stores meta information about each dataset such as evaluation metrics etc.\n    also automatically pre-processes the dataset.\n    [!] node property prediction datasets requires the following:\n    self.meta_dict[\"fname\"]: path to the edge list file\n    self.meta_dict[\"nodefile\"]: path to the node label file\n\n    Parameters:\n        name: name of the dataset\n        root: root directory to store the dataset folder\n        meta_dict: dictionary containing meta information about the dataset, should contain key 'dir_name' which is the name of the dataset folder\n        preprocess: whether to pre-process the dataset\n    Returns:\n        None\n    \"\"\"\n    self.name = name  ## original name\n    # check if dataset url exist\n    if self.name in DATA_URL_DICT:\n        self.url = DATA_URL_DICT[self.name]\n    else:\n        self.url = None\n        print(f\"Dataset {self.name} url not found, download not supported yet.\")\n\n    # check if the evaluatioin metric are specified\n    if self.name in DATA_EVAL_METRIC_DICT:\n        self.metric = DATA_EVAL_METRIC_DICT[self.name]\n    else:\n        self.metric = None\n        print(\n            f\"Dataset {self.name} default evaluation metric not found, it is not supported yet.\"\n        )\n\n    root = PROJ_DIR + root\n\n    if meta_dict is None:\n        self.dir_name = \"_\".join(name.split(\"-\"))  ## replace hyphen with underline\n        meta_dict = {\"dir_name\": self.dir_name}\n    else:\n        self.dir_name = meta_dict[\"dir_name\"]\n    self.root = osp.join(root, self.dir_name)\n    self.meta_dict = meta_dict\n    if \"fname\" not in self.meta_dict:\n        self.meta_dict[\"fname\"] = self.root + \"/\" + self.name + \"_edgelist.csv\"\n        self.meta_dict[\"nodefile\"] = self.root + \"/\" + self.name + \"_node_labels.csv\"\n\n     #! version check\n    self.version_passed = True\n    self._version_check()\n\n    self._num_classes = DATA_NUM_CLASSES[self.name]\n\n    # initialize\n    self._node_feat = None\n    self._edge_feat = None\n    self._full_data = None\n    self.download()\n    # check if the root directory exists, if not create it\n    if osp.isdir(self.root):\n        print(\"Dataset directory is \", self.root)\n    else:\n        raise FileNotFoundError(f\"Directory not found at {self.root}\")\n\n    if preprocess:\n        self.pre_process()\n\n    self.label_ts_idx = 0  # index for which node lables to return now\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.download","title":"<code>download()</code>","text":"<p>downloads this dataset from url check if files are already downloaded Returns:     None</p> Source code in <code>tgb/nodeproppred/dataset.py</code> <pre><code>def download(self) -&gt; None:\n    r\"\"\"\n    downloads this dataset from url\n    check if files are already downloaded\n    Returns:\n        None\n    \"\"\"\n    # check if the file already exists\n    if osp.exists(self.meta_dict[\"fname\"]) and osp.exists(\n        self.meta_dict[\"nodefile\"]\n    ):\n        print(\"raw file found, skipping download\")\n        return\n\n    else:\n        inp = input(\n            \"Will you download the dataset(s) now? (y/N)\\n\"\n        ).lower()  # ask if the user wants to download the dataset\n        if inp == \"y\":\n            print(\n                f\"{BColors.WARNING}Download started, this might take a while . . . {BColors.ENDC}\"\n            )\n            print(f\"Dataset title: {self.name}\")\n\n            if self.url is None:\n                raise Exception(\n                    \"Dataset url not found, download not supported yet.\"\n                )\n            else:\n                r = requests.get(self.url, stream=True)\n                if osp.isdir(self.root):\n                    print(\"Dataset directory is \", self.root)\n                else:\n                    os.makedirs(self.root)\n\n                path_download = self.root + \"/\" + self.name + \".zip\"\n                with open(path_download, \"wb\") as f:\n                    total_length = int(r.headers.get(\"content-length\"))\n                    for chunk in progress.bar(\n                        r.iter_content(chunk_size=1024),\n                        expected_size=(total_length / 1024) + 1,\n                    ):\n                        if chunk:\n                            f.write(chunk)\n                            f.flush()\n                # for unzipping the file\n                with zipfile.ZipFile(path_download, \"r\") as zip_ref:\n                    zip_ref.extractall(self.root)\n                print(f\"{BColors.OKGREEN}Download completed {BColors.ENDC}\")\n        else:\n            raise Exception(\n                BColors.FAIL\n                + \"Data not found error, download \"\n                + self.name\n                + \" failed\"\n            )\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.find_next_labels_batch","title":"<code>find_next_labels_batch(cur_t)</code>","text":"<p>this returns the node labels closest to cur_t (for that given day) Parameters:     cur_t: current timestamp of the batch of edges Returns:     ts: timestamp of the node labels     source_idx: node ids     labels: the stacked label vectors</p> Source code in <code>tgb/nodeproppred/dataset.py</code> <pre><code>def find_next_labels_batch(\n    self,\n    cur_t: int,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    r\"\"\"\n    this returns the node labels closest to cur_t (for that given day)\n    Parameters:\n        cur_t: current timestamp of the batch of edges\n    Returns:\n        ts: timestamp of the node labels\n        source_idx: node ids\n        labels: the stacked label vectors\n    \"\"\"\n    if self.label_ts_idx &gt;= (self.label_ts.shape[0]):\n        # for query that are after the last batch of labels\n        return None\n    else:\n        ts = self.label_ts[self.label_ts_idx]\n\n    if cur_t &gt;= ts:\n        self.label_ts_idx += 1  # move to the next ts\n        # {ts: {node_id: label_vec}}\n        node_ids = np.array(list(self.label_dict[ts].keys()))\n\n        node_labels = []\n        for key in self.label_dict[ts]:\n            node_labels.append(np.array(self.label_dict[ts][key]))\n        node_labels = np.stack(node_labels, axis=0)\n        label_ts = np.full(node_ids.shape[0], ts, dtype=\"int\")\n        return (label_ts, node_ids, node_labels)\n    else:\n        return None\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.generate_processed_files","title":"<code>generate_processed_files()</code>","text":"<p>returns an edge list of pandas data frame Returns:     df: pandas data frame storing the temporal edge list     node_label_dict: dictionary with key as timestamp and item as dictionary of node labels</p> Source code in <code>tgb/nodeproppred/dataset.py</code> <pre><code>def generate_processed_files(\n    self,\n) -&gt; Tuple[pd.DataFrame, Dict[int, Dict[str, Any]]]:\n    r\"\"\"\n    returns an edge list of pandas data frame\n    Returns:\n        df: pandas data frame storing the temporal edge list\n        node_label_dict: dictionary with key as timestamp and item as dictionary of node labels\n    \"\"\"\n    OUT_DF = self.root + \"/\" + \"ml_{}.pkl\".format(self.name)\n    OUT_NODE_DF = self.root + \"/\" + \"ml_{}_node.pkl\".format(self.name)\n    OUT_LABEL_DF = self.root + \"/\" + \"ml_{}_label.pkl\".format(self.name)\n    OUT_EDGE_FEAT = self.root + \"/\" + \"ml_{}.pkl\".format(self.name + \"_edge\")\n\n    # * logic for large datasets, as node label file is too big to store on disc\n    if self.name == \"tgbn-reddit\" or self.name == \"tgbn-token\":\n        if osp.exists(OUT_DF) and osp.exists(OUT_NODE_DF) and osp.exists(OUT_EDGE_FEAT):\n            df = pd.read_pickle(OUT_DF)\n            edge_feat = load_pkl(OUT_EDGE_FEAT)\n            if (self.name == \"tgbn-token\"):\n                #! taking log normalization for numerical stability\n                print (\"applying log normalization for weights in tgbn-token\")\n                edge_feat[:,0] = np.log(edge_feat[:,0])\n            node_ids = load_pkl(OUT_NODE_DF)\n            labels_dict = load_pkl(OUT_LABEL_DF)\n            node_label_dict = load_label_dict(\n                self.meta_dict[\"nodefile\"], node_ids, labels_dict\n            )\n            return df, node_label_dict, edge_feat\n\n    # * load the preprocessed file if possible\n    if osp.exists(OUT_DF) and osp.exists(OUT_NODE_DF) and osp.exists(OUT_EDGE_FEAT):\n        print(\"loading processed file\")\n        df = pd.read_pickle(OUT_DF)\n        node_label_dict = load_pkl(OUT_NODE_DF)\n        edge_feat = load_pkl(OUT_EDGE_FEAT)\n    else:  # * process the file\n        print(\"file not processed, generating processed file\")\n        if self.name == \"tgbn-reddit\":\n            df, edge_feat, node_ids, labels_dict = load_edgelist_sr(\n                self.meta_dict[\"fname\"], label_size=self._num_classes\n            )\n        elif self.name == \"tgbn-token\":\n            df, edge_feat, node_ids, labels_dict = load_edgelist_token(\n                self.meta_dict[\"fname\"], label_size=self._num_classes\n            )\n        elif self.name == \"tgbn-genre\":\n            df, edge_feat, node_ids, labels_dict = load_edgelist_datetime(\n                self.meta_dict[\"fname\"], label_size=self._num_classes\n            )\n        elif self.name == \"tgbn-trade\":\n            df, edge_feat, node_ids = load_edgelist_trade(\n                self.meta_dict[\"fname\"], label_size=self._num_classes\n            )\n\n        df.to_pickle(OUT_DF)\n        save_pkl(edge_feat, OUT_EDGE_FEAT)\n\n        if self.name == \"tgbn-trade\":\n            node_label_dict = load_trade_label_dict(\n                self.meta_dict[\"nodefile\"], node_ids\n            )\n        else:\n            node_label_dict = load_label_dict(\n                self.meta_dict[\"nodefile\"], node_ids, labels_dict\n            )\n\n        if (\n            self.name != \"tgbn-reddit\" and self.name != \"tgbn-token\"\n        ):  # don't save subreddits on disc, the node label file is too big\n            save_pkl(node_label_dict, OUT_NODE_DF)\n        else:\n            save_pkl(node_ids, OUT_NODE_DF)\n            save_pkl(labels_dict, OUT_LABEL_DF)\n\n        print(\"file processed and saved\")\n    return df, node_label_dict, edge_feat\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.generate_splits","title":"<code>generate_splits(full_data, val_ratio=0.15, test_ratio=0.15)</code>","text":"<p>Generates train, validation, and test splits from the full dataset Parameters:     full_data: dictionary containing the full dataset     val_ratio: ratio of validation data     test_ratio: ratio of test data Returns:     train_mask: boolean mask for training data     val_mask: boolean mask for validation data     test_mask: boolean mask for test data</p> Source code in <code>tgb/nodeproppred/dataset.py</code> <pre><code>def generate_splits(\n    self,\n    full_data: Dict[str, Any],\n    val_ratio: float = 0.15,\n    test_ratio: float = 0.15,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    r\"\"\"\n    Generates train, validation, and test splits from the full dataset\n    Parameters:\n        full_data: dictionary containing the full dataset\n        val_ratio: ratio of validation data\n        test_ratio: ratio of test data\n    Returns:\n        train_mask: boolean mask for training data\n        val_mask: boolean mask for validation data\n        test_mask: boolean mask for test data\n    \"\"\"\n    val_time, test_time = list(\n        np.quantile(\n            full_data[\"timestamps\"],\n            [(1 - val_ratio - test_ratio), (1 - test_ratio)],\n        )\n    )\n    timestamps = full_data[\"timestamps\"]\n    train_mask = timestamps &lt;= val_time\n    val_mask = np.logical_and(timestamps &lt;= test_time, timestamps &gt; val_time)\n    test_mask = timestamps &gt; test_time\n\n    return train_mask, val_mask, test_mask\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.pre_process","title":"<code>pre_process()</code>","text":"<p>Pre-process the dataset and generates the splits, must be run before dataset properties can be accessed Returns:     None</p> Source code in <code>tgb/nodeproppred/dataset.py</code> <pre><code>def pre_process(self) -&gt; None:\n    \"\"\"\n    Pre-process the dataset and generates the splits, must be run before dataset properties can be accessed\n    Returns:\n        None\n    \"\"\"\n    # first check if all files exist\n    if (\"fname\" not in self.meta_dict) or (\"nodefile\" not in self.meta_dict):\n        raise Exception(\"meta_dict does not contain all required filenames\")\n\n    df, node_label_dict, edge_feat = self.generate_processed_files()\n    sources = np.array(df[\"u\"])\n    destinations = np.array(df[\"i\"])\n    timestamps = np.array(df[\"ts\"])\n    edge_idxs = np.array(df[\"idx\"])\n    edge_label = np.ones(sources.shape[0])\n    #self._edge_feat = np.array(df[\"w\"])\n    self._edge_feat = edge_feat\n\n    full_data = {\n        \"sources\": sources,\n        \"destinations\": destinations,\n        \"timestamps\": timestamps,\n        \"edge_idxs\": edge_idxs,\n        \"edge_feat\": self._edge_feat,\n        \"edge_label\": edge_label,\n    }\n    self._full_data = full_data\n\n    # storing the split masks\n    _train_mask, _val_mask, _test_mask = self.generate_splits(full_data)\n\n    self._train_mask = _train_mask\n    self._val_mask = _val_mask\n    self._test_mask = _test_mask\n\n    self.label_dict = node_label_dict\n    self.label_ts = np.array(list(node_label_dict.keys()))\n    self.label_ts = np.sort(self.label_ts)\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.reset_label_time","title":"<code>reset_label_time()</code>","text":"<p>reset the pointer for node label once the entire dataset has been iterated once Returns:     None</p> Source code in <code>tgb/nodeproppred/dataset.py</code> <pre><code>def reset_label_time(self) -&gt; None:\n    r\"\"\"\n    reset the pointer for node label once the entire dataset has been iterated once\n    Returns:\n        None\n    \"\"\"\n    self.label_ts_idx = 0\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset.NodePropPredDataset.return_label_ts","title":"<code>return_label_ts()</code>","text":"<p>return the current label timestamp that the pointer is at Returns:     ts: int, the timestamp of the node labels</p> Source code in <code>tgb/nodeproppred/dataset.py</code> <pre><code>def return_label_ts(self) -&gt; int:\n    \"\"\"\n    return the current label timestamp that the pointer is at\n    Returns:\n        ts: int, the timestamp of the node labels\n    \"\"\"\n    if (self.label_ts_idx &gt;= self.label_ts.shape[0]):\n        return self.label_ts[-1]\n    else:\n        return self.label_ts[self.label_ts_idx]\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset","title":"<code>PyGNodePropPredDataset</code>","text":"<p>               Bases: <code>InMemoryDataset</code></p> <p>PyG wrapper for the NodePropPredDataset can return pytorch tensors for src,dst,t,msg,label can return Temporal Data object also query the node labels corresponding to a timestamp from edge batch Parameters:     name: name of the dataset, passed to <code>NodePropPredDataset</code>     root (string): Root directory where the dataset should be saved.     transform (callable, optional): A function/transform that takes in an     pre_transform (callable, optional): A function/transform that takes in</p> Source code in <code>tgb/nodeproppred/dataset_pyg.py</code> <pre><code>class PyGNodePropPredDataset(InMemoryDataset):\n    r\"\"\"\n    PyG wrapper for the NodePropPredDataset\n    can return pytorch tensors for src,dst,t,msg,label\n    can return Temporal Data object\n    also query the node labels corresponding to a timestamp from edge batch\n    Parameters:\n        name: name of the dataset, passed to `NodePropPredDataset`\n        root (string): Root directory where the dataset should be saved.\n        transform (callable, optional): A function/transform that takes in an\n        pre_transform (callable, optional): A function/transform that takes in\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        root: str,\n        transform: Optional[Callable] = None,\n        pre_transform: Optional[Callable] = None,\n    ):\n        self.name = name\n        self.root = root\n        self.dataset = NodePropPredDataset(name=name, root=root)\n        self._train_mask = torch.from_numpy(self.dataset.train_mask)\n        self._val_mask = torch.from_numpy(self.dataset.val_mask)\n        self._test_mask = torch.from_numpy(self.dataset.test_mask)\n        self.__num_classes = self.dataset.num_classes\n        super().__init__(root, transform, pre_transform)\n        self.process_data()\n\n    @property\n    def num_classes(self) -&gt; int:\n        \"\"\"\n        how many classes are in the node label\n        Returns:\n            num_classes: int\n        \"\"\"\n        return self.__num_classes\n\n    @property\n    def eval_metric(self) -&gt; str:\n        \"\"\"\n        the official evaluation metric for the dataset, loaded from info.py\n        Returns:\n            eval_metric: str, the evaluation metric\n        \"\"\"\n        return self.dataset.eval_metric\n\n    @property\n    def train_mask(self) -&gt; torch.Tensor:\n        r\"\"\"\n        Returns the train mask of the dataset\n        Returns:\n            train_mask: the mask for edges in the training set\n        \"\"\"\n        if self._train_mask is None:\n            raise ValueError(\"training split hasn't been loaded\")\n        return self._train_mask\n\n    @property\n    def val_mask(self) -&gt; torch.Tensor:\n        r\"\"\"\n        Returns the validation mask of the dataset\n        Returns:\n            val_mask: the mask for edges in the validation set\n        \"\"\"\n        if self._val_mask is None:\n            raise ValueError(\"validation split hasn't been loaded\")\n        return self._val_mask\n\n    @property\n    def test_mask(self) -&gt; torch.Tensor:\n        r\"\"\"\n        Returns the test mask of the dataset:\n        Returns:\n            test_mask: the mask for edges in the test set\n        \"\"\"\n        if self._test_mask is None:\n            raise ValueError(\"test split hasn't been loaded\")\n        return self._test_mask\n\n    @property\n    def src(self) -&gt; torch.Tensor:\n        r\"\"\"\n        Returns the source nodes of the dataset\n        Returns:\n            src: the idx of the source nodes\n        \"\"\"\n        return self._src\n\n    @property\n    def dst(self) -&gt; torch.Tensor:\n        r\"\"\"\n        Returns the destination nodes of the dataset\n        Returns:\n            dst: the idx of the destination nodes\n        \"\"\"\n        return self._dst\n\n    @property\n    def ts(self) -&gt; torch.Tensor:\n        r\"\"\"\n        Returns the timestamps of the dataset\n        Returns:\n            ts: the timestamps of the edges\n        \"\"\"\n        return self._ts\n\n    @property\n    def edge_feat(self) -&gt; torch.Tensor:\n        r\"\"\"\n        Returns the edge features of the dataset\n        Returns:\n            edge_feat: the edge features\n        \"\"\"\n        return self._edge_feat\n\n    @property\n    def edge_label(self) -&gt; torch.Tensor:\n        r\"\"\"\n        Returns the edge labels of the dataset\n        Returns:\n            edge_label: the labels of the edges (all one tensor)\n        \"\"\"\n        return self._edge_label\n\n    def process_data(self):\n        \"\"\"\n        convert data to pytorch tensors\n        \"\"\"\n        src = torch.from_numpy(self.dataset.full_data[\"sources\"])\n        dst = torch.from_numpy(self.dataset.full_data[\"destinations\"])\n        t = torch.from_numpy(self.dataset.full_data[\"timestamps\"])\n        edge_label = torch.from_numpy(self.dataset.full_data[\"edge_label\"])\n        msg = torch.from_numpy(self.dataset.full_data[\"edge_feat\"])\n        # msg = torch.from_numpy(self.dataset.full_data[\"edge_feat\"]).reshape(\n        #     [-1, 1]\n        # ) \n        # * check typing\n        if src.dtype != torch.int64:\n            src = src.long()\n\n        if dst.dtype != torch.int64:\n            dst = dst.long()\n\n        if t.dtype != torch.int64:\n            t = t.long()\n\n        if msg.dtype != torch.float32:\n            msg = msg.float()\n\n        self._src = src\n        self._dst = dst\n        self._ts = t\n        self._edge_label = edge_label\n        self._edge_feat = msg\n\n    def get_TemporalData(\n        self,\n    ) -&gt; TemporalData:\n        \"\"\"\n        return the TemporalData object for the entire dataset\n        Returns:\n            data: TemporalData object storing the edgelist\n        \"\"\"\n        data = TemporalData(\n            src=self._src,\n            dst=self._dst,\n            t=self._ts,\n            msg=self._edge_feat,\n            y=self._edge_label,\n        )\n        return data\n\n    def reset_label_time(self) -&gt; None:\n        \"\"\"\n        reset the pointer for the node labels, should be done per epoch\n        \"\"\"\n        self.dataset.reset_label_time()\n\n    def get_node_label(self, cur_t):\n        \"\"\"\n        return the node labels for the current timestamp\n        \"\"\"\n        label_tuple = self.dataset.find_next_labels_batch(cur_t)\n        if label_tuple is None:\n            return None\n        label_ts, label_srcs, labels = label_tuple[0], label_tuple[1], label_tuple[2]\n        label_ts = torch.from_numpy(label_ts).long()\n        label_srcs = torch.from_numpy(label_srcs).long()\n        labels = torch.from_numpy(labels).to(torch.float32)\n        return label_ts, label_srcs, labels\n\n    def get_label_time(self) -&gt; int:\n        \"\"\"\n        return the timestamps of the current node labels\n        Returns:\n            t: time of the current node labels\n        \"\"\"\n        return self.dataset.return_label_ts()\n\n    def len(self) -&gt; int:\n        \"\"\"\n        size of the dataset\n        Returns:\n            size: int\n        \"\"\"\n        return self._src.shape[0]\n\n    def get(self, idx: int) -&gt; TemporalData:\n        \"\"\"\n        construct temporal data object for a single edge\n        Parameters:\n            idx: index of the edge\n        Returns:\n            data: TemporalData object\n        \"\"\"\n        data = TemporalData(\n            src=self._src[idx],\n            dst=self._dst[idx],\n            t=self._ts[idx],\n            msg=self._edge_feat[idx],\n            y=self._edge_label[idx],\n        )\n        return data\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.name.capitalize()}()\"\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.dst","title":"<code>dst: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the destination nodes of the dataset Returns:     dst: the idx of the destination nodes</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.edge_feat","title":"<code>edge_feat: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the edge features of the dataset Returns:     edge_feat: the edge features</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.edge_label","title":"<code>edge_label: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the edge labels of the dataset Returns:     edge_label: the labels of the edges (all one tensor)</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.eval_metric","title":"<code>eval_metric: str</code>  <code>property</code>","text":"<p>the official evaluation metric for the dataset, loaded from info.py Returns:     eval_metric: str, the evaluation metric</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.num_classes","title":"<code>num_classes: int</code>  <code>property</code>","text":"<p>how many classes are in the node label Returns:     num_classes: int</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.src","title":"<code>src: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the source nodes of the dataset Returns:     src: the idx of the source nodes</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.test_mask","title":"<code>test_mask: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the test mask of the dataset: Returns:     test_mask: the mask for edges in the test set</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.train_mask","title":"<code>train_mask: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the train mask of the dataset Returns:     train_mask: the mask for edges in the training set</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.ts","title":"<code>ts: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the timestamps of the dataset Returns:     ts: the timestamps of the edges</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.val_mask","title":"<code>val_mask: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the validation mask of the dataset Returns:     val_mask: the mask for edges in the validation set</p>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.get","title":"<code>get(idx)</code>","text":"<p>construct temporal data object for a single edge Parameters:     idx: index of the edge Returns:     data: TemporalData object</p> Source code in <code>tgb/nodeproppred/dataset_pyg.py</code> <pre><code>def get(self, idx: int) -&gt; TemporalData:\n    \"\"\"\n    construct temporal data object for a single edge\n    Parameters:\n        idx: index of the edge\n    Returns:\n        data: TemporalData object\n    \"\"\"\n    data = TemporalData(\n        src=self._src[idx],\n        dst=self._dst[idx],\n        t=self._ts[idx],\n        msg=self._edge_feat[idx],\n        y=self._edge_label[idx],\n    )\n    return data\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.get_TemporalData","title":"<code>get_TemporalData()</code>","text":"<p>return the TemporalData object for the entire dataset Returns:     data: TemporalData object storing the edgelist</p> Source code in <code>tgb/nodeproppred/dataset_pyg.py</code> <pre><code>def get_TemporalData(\n    self,\n) -&gt; TemporalData:\n    \"\"\"\n    return the TemporalData object for the entire dataset\n    Returns:\n        data: TemporalData object storing the edgelist\n    \"\"\"\n    data = TemporalData(\n        src=self._src,\n        dst=self._dst,\n        t=self._ts,\n        msg=self._edge_feat,\n        y=self._edge_label,\n    )\n    return data\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.get_label_time","title":"<code>get_label_time()</code>","text":"<p>return the timestamps of the current node labels Returns:     t: time of the current node labels</p> Source code in <code>tgb/nodeproppred/dataset_pyg.py</code> <pre><code>def get_label_time(self) -&gt; int:\n    \"\"\"\n    return the timestamps of the current node labels\n    Returns:\n        t: time of the current node labels\n    \"\"\"\n    return self.dataset.return_label_ts()\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.get_node_label","title":"<code>get_node_label(cur_t)</code>","text":"<p>return the node labels for the current timestamp</p> Source code in <code>tgb/nodeproppred/dataset_pyg.py</code> <pre><code>def get_node_label(self, cur_t):\n    \"\"\"\n    return the node labels for the current timestamp\n    \"\"\"\n    label_tuple = self.dataset.find_next_labels_batch(cur_t)\n    if label_tuple is None:\n        return None\n    label_ts, label_srcs, labels = label_tuple[0], label_tuple[1], label_tuple[2]\n    label_ts = torch.from_numpy(label_ts).long()\n    label_srcs = torch.from_numpy(label_srcs).long()\n    labels = torch.from_numpy(labels).to(torch.float32)\n    return label_ts, label_srcs, labels\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.len","title":"<code>len()</code>","text":"<p>size of the dataset Returns:     size: int</p> Source code in <code>tgb/nodeproppred/dataset_pyg.py</code> <pre><code>def len(self) -&gt; int:\n    \"\"\"\n    size of the dataset\n    Returns:\n        size: int\n    \"\"\"\n    return self._src.shape[0]\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.process_data","title":"<code>process_data()</code>","text":"<p>convert data to pytorch tensors</p> Source code in <code>tgb/nodeproppred/dataset_pyg.py</code> <pre><code>def process_data(self):\n    \"\"\"\n    convert data to pytorch tensors\n    \"\"\"\n    src = torch.from_numpy(self.dataset.full_data[\"sources\"])\n    dst = torch.from_numpy(self.dataset.full_data[\"destinations\"])\n    t = torch.from_numpy(self.dataset.full_data[\"timestamps\"])\n    edge_label = torch.from_numpy(self.dataset.full_data[\"edge_label\"])\n    msg = torch.from_numpy(self.dataset.full_data[\"edge_feat\"])\n    # msg = torch.from_numpy(self.dataset.full_data[\"edge_feat\"]).reshape(\n    #     [-1, 1]\n    # ) \n    # * check typing\n    if src.dtype != torch.int64:\n        src = src.long()\n\n    if dst.dtype != torch.int64:\n        dst = dst.long()\n\n    if t.dtype != torch.int64:\n        t = t.long()\n\n    if msg.dtype != torch.float32:\n        msg = msg.float()\n\n    self._src = src\n    self._dst = dst\n    self._ts = t\n    self._edge_label = edge_label\n    self._edge_feat = msg\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset.reset_label_time","title":"<code>reset_label_time()</code>","text":"<p>reset the pointer for the node labels, should be done per epoch</p> Source code in <code>tgb/nodeproppred/dataset_pyg.py</code> <pre><code>def reset_label_time(self) -&gt; None:\n    \"\"\"\n    reset the pointer for the node labels, should be done per epoch\n    \"\"\"\n    self.dataset.reset_label_time()\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.evaluate.Evaluator","title":"<code>Evaluator</code>","text":"<p>               Bases: <code>object</code></p> <p>Evaluator for Node Property Prediction</p> Source code in <code>tgb/nodeproppred/evaluate.py</code> <pre><code>class Evaluator(object):\n    \"\"\"Evaluator for Node Property Prediction\"\"\"\n\n    def __init__(self, name: str):\n        r\"\"\"\n        Parameters:\n            name: name of the dataset\n        \"\"\"\n        self.name = name\n        self.valid_metric_list = [\"mse\", \"rmse\", \"ndcg\"]\n        if self.name not in DATA_EVAL_METRIC_DICT:\n            raise NotImplementedError(\"Dataset not supported\")\n\n    def _parse_and_check_input(self, input_dict):\n        \"\"\"\n        check whether the input has the required format\n        Parametrers:\n            -input_dict: a dictionary containing \"y_true\", \"y_pred\", and \"eval_metric\"\n\n            note: \"eval_metric\" should be a list including one or more of the followin metrics:\n                    [\"mse\"]\n        \"\"\"\n        # valid_metric_list = ['ap', 'au_roc_score', 'au_pr_score', 'acc', 'prec', 'rec', 'f1']\n\n        if \"eval_metric\" not in input_dict:\n            raise RuntimeError(\"Missing key of eval_metric\")\n\n        for eval_metric in input_dict[\"eval_metric\"]:\n            if eval_metric in self.valid_metric_list:\n                if \"y_true\" not in input_dict:\n                    raise RuntimeError(\"Missing key of y_true\")\n                if \"y_pred\" not in input_dict:\n                    raise RuntimeError(\"Missing key of y_pred\")\n\n                y_true, y_pred = input_dict[\"y_true\"], input_dict[\"y_pred\"]\n\n                # converting to numpy on cpu\n                if torch is not None and isinstance(y_true, torch.Tensor):\n                    y_true = y_true.detach().cpu().numpy()\n                if torch is not None and isinstance(y_pred, torch.Tensor):\n                    y_pred = y_pred.detach().cpu().numpy()\n\n                # check type and shape\n                if not isinstance(y_true, np.ndarray) or not isinstance(\n                    y_pred, np.ndarray\n                ):\n                    raise RuntimeError(\n                        \"Arguments to Evaluator need to be either numpy ndarray or torch tensor!\"\n                    )\n\n                if not y_true.shape == y_pred.shape:\n                    raise RuntimeError(\"Shape of y_true and y_pred must be the same!\")\n\n            else:\n                print(\n                    \"ERROR: The evaluation metric should be in:\", self.valid_metric_list\n                )\n                raise ValueError(\"Undefined eval metric %s \" % (eval_metric))\n        self.eval_metric = input_dict[\"eval_metric\"]\n\n        return y_true, y_pred\n\n    def _compute_metrics(self, y_true, y_pred):\n        \"\"\"\n        compute the performance metrics for the given true labels and prediction probabilities\n        Parameters:\n            -y_true: actual true labels\n            -y_pred: predicted probabilities\n        \"\"\"\n        perf_dict = {}\n        for eval_metric in self.eval_metric:\n            if eval_metric == \"mse\":\n                perf_dict = {\n                    \"mse\": mean_squared_error(y_true, y_pred),\n                    \"rmse\": math.sqrt(mean_squared_error(y_true, y_pred)),\n                }\n            elif eval_metric == \"ndcg\":\n                k = 10\n                perf_dict = {\"ndcg\": ndcg_score(y_true, y_pred, k=k)}\n        return perf_dict\n\n    def eval(self, input_dict, verbose=False):\n        \"\"\"\n        evaluation for edge regression task\n        \"\"\"\n        y_true, y_pred = self._parse_and_check_input(input_dict)\n        perf_dict = self._compute_metrics(y_true, y_pred)\n\n        if verbose:\n            print(\"INFO: Evaluation Results:\")\n            for eval_metric in input_dict[\"eval_metric\"]:\n                print(f\"\\t&gt;&gt;&gt; {eval_metric}: {perf_dict[eval_metric]:.4f}\")\n        return perf_dict\n\n    @property\n    def expected_input_format(self):\n        desc = \"==== Expected input format of Evaluator for {}\\n\".format(self.name)\n        if \"mse\" in self.valid_metric_list:\n            desc += \"{'y_pred': y_pred}\\n\"\n            desc += \"- y_pred: numpy ndarray or torch tensor of shape (num_edges, ). Torch tensor on GPU is recommended for efficiency.\\n\"\n            desc += \"y_pred is the predicted weight for edges.\\n\"\n        else:\n            raise ValueError(\"Undefined eval metric %s\" % (self.eval_metric))\n        return desc\n\n    @property\n    def expected_output_format(self):\n        desc = \"==== Expected output format of Evaluator for {}\\n\".format(self.name)\n        if \"mse\" in self.valid_metric_list:\n            desc += \"{'mse': mse\\n\"\n            desc += \"- mse (float): mse score\\n\"\n        else:\n            raise ValueError(\"Undefined eval metric %s\" % (self.eval_metric))\n        return desc\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.evaluate.Evaluator.__init__","title":"<code>__init__(name)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of the dataset</p> required Source code in <code>tgb/nodeproppred/evaluate.py</code> <pre><code>def __init__(self, name: str):\n    r\"\"\"\n    Parameters:\n        name: name of the dataset\n    \"\"\"\n    self.name = name\n    self.valid_metric_list = [\"mse\", \"rmse\", \"ndcg\"]\n    if self.name not in DATA_EVAL_METRIC_DICT:\n        raise NotImplementedError(\"Dataset not supported\")\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.evaluate.Evaluator.eval","title":"<code>eval(input_dict, verbose=False)</code>","text":"<p>evaluation for edge regression task</p> Source code in <code>tgb/nodeproppred/evaluate.py</code> <pre><code>def eval(self, input_dict, verbose=False):\n    \"\"\"\n    evaluation for edge regression task\n    \"\"\"\n    y_true, y_pred = self._parse_and_check_input(input_dict)\n    perf_dict = self._compute_metrics(y_true, y_pred)\n\n    if verbose:\n        print(\"INFO: Evaluation Results:\")\n        for eval_metric in input_dict[\"eval_metric\"]:\n            print(f\"\\t&gt;&gt;&gt; {eval_metric}: {perf_dict[eval_metric]:.4f}\")\n    return perf_dict\n</code></pre>"},{"location":"api/tgb.nodeproppred/#tgb.nodeproppred.evaluate.main","title":"<code>main()</code>","text":"<p>simple test for evaluator</p> Source code in <code>tgb/nodeproppred/evaluate.py</code> <pre><code>def main():\n    \"\"\"\n    simple test for evaluator\n    \"\"\"\n    name = \"tgbn-trade\"\n    evaluator = Evaluator(name=name)\n    print(evaluator.expected_input_format)\n    print(evaluator.expected_output_format)\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred, \"eval_metric\": [\"mse\"]}\n\n    result_dict = evaluator.eval(input_dict)\n    print(result_dict)\n</code></pre>"},{"location":"api/tgb.utils/","title":"<code>tgb.utils</code>","text":"<p>script for generating statistics from the dataset</p>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.clean_rows","title":"<code>clean_rows(fname, outname)</code>","text":"<p>clean the rows with comma in the name args:     fname: the path to the raw data     outname: the path to the cleaned data</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def clean_rows(\n    fname: str,\n    outname: str,\n):\n    r\"\"\"\n    clean the rows with comma in the name\n    args:\n        fname: the path to the raw data\n        outname: the path to the cleaned data\n    \"\"\"\n\n    outf = open(outname, \"w\")\n\n    with open(fname) as f:\n        s = next(f)\n        outf.write(s)\n        for idx, line in enumerate(f):\n            strs = [\"China, Taiwan Province of\", \"China, mainland\"]\n            for str in strs:\n                line = line.replace(\n                    \"China, Taiwan Province of\", \"Taiwan Province of China\"\n                )\n                line = line.replace(\"China, mainland\", \"China mainland\")\n                line = line.replace(\"China, Hong Kong SAR\", \"China Hong Kong SAR\")\n                line = line.replace(\"China, Macao SAR\", \"China Macao SAR\")\n                line = line.replace(\n                    \"Saint Helena, Ascension and Tristan da Cunha\",\n                    \"Saint Helena Ascension and Tristan da Cunha\",\n                )\n\n            e = line.strip().split(\",\")\n            if len(e) &gt; 4:\n                print(e)\n                raise ValueError(\"line has more than 4 elements\")\n            outf.write(line)\n\n    outf.close()\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.convert_str2int","title":"<code>convert_str2int(in_str)</code>","text":"<p>convert strings to vectors of integers based on individual character each letter is converted as follows, a=10, b=11 numbers are still int Parameters:     in_str: an input string to parse Returns:     out: a numpy integer array</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def convert_str2int(\n    in_str: str,\n) -&gt; np.ndarray:\n    \"\"\"\n    convert strings to vectors of integers based on individual character\n    each letter is converted as follows, a=10, b=11\n    numbers are still int\n    Parameters:\n        in_str: an input string to parse\n    Returns:\n        out: a numpy integer array\n    \"\"\"\n    out = []\n    for element in in_str:\n        if element.isnumeric():\n            out.append(element)\n        elif element == \"!\":\n            out.append(-1)\n        else:\n            out.append(ord(element.upper()) - 44 + 9)\n    out = np.array(out, dtype=np.float32)\n    return out\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.csv_to_forum_data","title":"<code>csv_to_forum_data(fname)</code>","text":"<p>used by thgl-forum dataset convert the raw .csv data to pandas dataframe and numpy array input .csv file format should be: timestamp, head, tail, relation type Args:     fname: the path to the raw data</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def csv_to_forum_data(\n    fname: str,\n) -&gt; pd.DataFrame:\n    r\"\"\"\n    used by thgl-forum dataset\n    convert the raw .csv data to pandas dataframe and numpy array\n    input .csv file format should be: timestamp, head, tail, relation type\n    Args:\n        fname: the path to the raw data\n    \"\"\"\n    feat_size = 2\n    num_lines = sum(1 for line in open(fname)) - 1\n    print(\"number of lines counted\", num_lines)\n    u_list = np.zeros(num_lines)\n    i_list = np.zeros(num_lines)\n    ts_list = np.zeros(num_lines)\n    label_list = np.zeros(num_lines)\n    edge_type = np.zeros(num_lines)\n    feat_l = np.zeros((num_lines, feat_size))\n    idx_list = np.zeros(num_lines)\n    w_list = np.zeros(num_lines)\n    node_ids = {}\n    unique_id = 0\n\n    word_max = 10000\n    score_max = 10000\n\n    with open(fname, \"r\") as csv_file:\n        csv_reader = csv.reader(csv_file, delimiter=\",\")\n        idx = 0\n        #timestamp, head, tail, relation type\n        for row in tqdm(csv_reader):\n            if idx == 0:\n                idx += 1\n                continue\n            else:\n                #! ts,src,dst,relation_type,num_words,score\n                ts = int(row[0]) #converted to UNIX timestamp already \n                src = int(row[1])\n                dst = int(row[2])\n                relation = int(row[3])\n                num_words = int(row[4])\n                score = int(row[5])\n                if src not in node_ids:\n                    node_ids[src] = unique_id\n                    unique_id += 1\n                if dst not in node_ids:\n                    node_ids[dst] = unique_id\n                    unique_id += 1\n                u = node_ids[src]\n                i = node_ids[dst]\n                u_list[idx - 1] = u\n                i_list[idx - 1] = i\n                ts_list[idx - 1] = ts\n                idx_list[idx - 1] = idx\n                w_list[idx - 1] = float(1)\n                edge_type[idx - 1] = relation\n                feat_l[idx - 1] = np.array([num_words/word_max, score/score_max])\n                idx += 1\n    return (\n        pd.DataFrame(\n            {\n                \"u\": u_list,\n                \"i\": i_list,\n                \"ts\": ts_list,\n                \"label\": label_list,\n                \"idx\": idx_list,\n                \"w\": w_list,\n                \"edge_type\": edge_type,\n            }\n        ),\n        feat_l,\n        node_ids,\n    )\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.csv_to_pd_data","title":"<code>csv_to_pd_data(fname)</code>","text":"<p>currently used by tgbl-flight dataset convert the raw .csv data to pandas dataframe and numpy array input .csv file format should be: timestamp, node u, node v, attributes Args:     fname: the path to the raw data</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def csv_to_pd_data(\n    fname: str,\n) -&gt; pd.DataFrame:\n    r\"\"\"\n    currently used by tgbl-flight dataset\n    convert the raw .csv data to pandas dataframe and numpy array\n    input .csv file format should be: timestamp, node u, node v, attributes\n    Args:\n        fname: the path to the raw data\n    \"\"\"\n    feat_size = 16\n    num_lines = sum(1 for line in open(fname)) - 1\n    print(\"number of lines counted\", num_lines)\n    u_list = np.zeros(num_lines)\n    i_list = np.zeros(num_lines)\n    ts_list = np.zeros(num_lines)\n    label_list = np.zeros(num_lines)\n    feat_l = np.zeros((num_lines, feat_size))\n    idx_list = np.zeros(num_lines)\n    w_list = np.zeros(num_lines)\n    print(\"numpy allocated\")\n    node_ids = {}\n    unique_id = 0\n    ts_format = None\n\n    with open(fname, \"r\") as csv_file:\n        csv_reader = csv.reader(csv_file, delimiter=\",\")\n        idx = 0\n        #'day','src','dst','callsign','typecode'\n        for row in tqdm(csv_reader):\n            if idx == 0:\n                idx += 1\n                continue\n            else:\n                ts = row[0]\n                if ts_format is None:\n                    if (ts.isdigit()):\n                        ts_format = True\n                    else:\n                        ts_format = False\n\n                if ts_format:\n                    ts = float(int(ts)) #unix timestamp already\n                else:\n                    #convert to unix timestamp\n                    TIME_FORMAT = \"%Y-%m-%d\"\n                    date_cur = datetime.datetime.strptime(ts, TIME_FORMAT)\n                    ts = float(date_cur.timestamp())\n                    # TIME_FORMAT = \"%Y-%m-%d\" # 2019-01-01\n                    # date_cur  = date.fromisoformat(ts)\n                    # dt = datetime.datetime.combine(date_cur, datetime.datetime.min.time())\n                    # dt = dt.replace(tzinfo=datetime.timezone.edt)\n                    # ts = float(dt.timestamp())\n\n\n                src = row[1]\n                dst = row[2]\n\n                # 'callsign' has max size 8, can be 4, 5, 6, or 7\n                # 'typecode' has max size 8\n                # use ! as padding\n\n                # pad row[3] to size 7\n                if len(row[3]) == 0:\n                    row[3] = \"!!!!!!!!\"\n                while len(row[3]) &lt; 8:\n                    row[3] += \"!\"\n\n                # pad row[4] to size 4\n                if len(row[4]) == 0:\n                    row[4] = \"!!!!!!!!\"\n                while len(row[4]) &lt; 8:\n                    row[4] += \"!\"\n                if len(row[4]) &gt; 8:\n                    row[4] = \"!!!!!!!!\"\n\n                feat_str = row[3] + row[4]\n\n                if src not in node_ids:\n                    node_ids[src] = unique_id\n                    unique_id += 1\n                if dst not in node_ids:\n                    node_ids[dst] = unique_id\n                    unique_id += 1\n                u = node_ids[src]\n                i = node_ids[dst]\n                u_list[idx - 1] = u\n                i_list[idx - 1] = i\n                ts_list[idx - 1] = ts\n                idx_list[idx - 1] = idx\n                w_list[idx - 1] = float(1)\n                feat_l[idx - 1] = convert_str2int(feat_str)\n                idx += 1\n    return (\n        pd.DataFrame(\n            {\n                \"u\": u_list,\n                \"i\": i_list,\n                \"ts\": ts_list,\n                \"label\": label_list,\n                \"idx\": idx_list,\n                \"w\": w_list,\n            }\n        ),\n        feat_l,\n        node_ids,\n    )\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.csv_to_pd_data_rc","title":"<code>csv_to_pd_data_rc(fname)</code>","text":"<p>currently used by redditcomments dataset convert the raw .csv data to pandas dataframe and numpy array input .csv file format should be: timestamp, node u, node v, attributes Args:     fname: the path to the raw data</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def csv_to_pd_data_rc(\n    fname: str,\n) -&gt; pd.DataFrame:\n    r\"\"\"\n    currently used by redditcomments dataset\n    convert the raw .csv data to pandas dataframe and numpy array\n    input .csv file format should be: timestamp, node u, node v, attributes\n    Args:\n        fname: the path to the raw data\n    \"\"\"\n    feat_size = 2  # 1 for subreddit, 1 for num words\n    num_lines = sum(1 for line in open(fname)) - 1\n    #print(\"number of lines counted\", num_lines)\n    print(\"there are \", num_lines, \" lines in the raw data\")\n    u_list = np.zeros(num_lines)\n    i_list = np.zeros(num_lines)\n    ts_list = np.zeros(num_lines)\n    label_list = np.zeros(num_lines)\n    feat_l = np.zeros((num_lines, feat_size))\n    idx_list = np.zeros(num_lines)\n    w_list = np.zeros(num_lines)\n    node_ids = {}\n\n    unique_id = 0\n    max_words = 5000  # counted form statistics\n\n    with open(fname, \"r\") as csv_file:\n        csv_reader = csv.reader(csv_file, delimiter=\",\")\n        idx = 0\n        # ['ts', 'src', 'dst', 'subreddit', 'num_words', 'score']\n        for row in tqdm(csv_reader):\n            if idx == 0:\n                idx += 1\n                continue\n            else:\n                ts = int(row[0])\n                src = row[1]\n                dst = row[2]\n                num_words = int(row[3]) / max_words  # int number, normalize to [0,1]\n                score = int(row[4])  # int number\n\n                # reindexing node and subreddits\n                if src not in node_ids:\n                    node_ids[src] = unique_id\n                    unique_id += 1\n                if dst not in node_ids:\n                    node_ids[dst] = unique_id\n                    unique_id += 1\n                w = float(score)\n                u = node_ids[src]\n                i = node_ids[dst]\n                u_list[idx - 1] = u\n                i_list[idx - 1] = i\n                ts_list[idx - 1] = ts\n                idx_list[idx - 1] = idx\n                w_list[idx - 1] = w\n                feat_l[idx - 1] = np.array([num_words])\n                idx += 1\n    print(\"there are \", len(node_ids), \" unique nodes\")\n\n    return (\n        pd.DataFrame(\n            {\n                \"u\": u_list,\n                \"i\": i_list,\n                \"ts\": ts_list,\n                \"label\": label_list,\n                \"idx\": idx_list,\n                \"w\": w_list,\n            }\n        ),\n        feat_l,\n        node_ids,\n    )\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.csv_to_pd_data_sc","title":"<code>csv_to_pd_data_sc(fname)</code>","text":"<p>currently used by stablecoin dataset convert the raw .csv data to pandas dataframe and numpy array input .csv file format should be: timestamp, node u, node v, attributes Parameters:     fname: the path to the raw data Returns:     df: a pandas dataframe containing the edgelist data     feat_l: a numpy array containing the node features     node_ids: a dictionary mapping node id to integer</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def csv_to_pd_data_sc(\n    fname: str,\n) -&gt; pd.DataFrame:\n    r\"\"\"\n    currently used by stablecoin dataset\n    convert the raw .csv data to pandas dataframe and numpy array\n    input .csv file format should be: timestamp, node u, node v, attributes\n    Parameters:\n        fname: the path to the raw data\n    Returns:\n        df: a pandas dataframe containing the edgelist data\n        feat_l: a numpy array containing the node features\n        node_ids: a dictionary mapping node id to integer\n    \"\"\"\n    feat_size = 1\n    num_lines = sum(1 for line in open(fname)) - 1\n    print(\"number of lines counted\", num_lines)\n    u_list = np.zeros(num_lines)\n    i_list = np.zeros(num_lines)\n    ts_list = np.zeros(num_lines)\n    label_list = np.zeros(num_lines)\n    feat_l = np.zeros((num_lines, feat_size))\n    idx_list = np.zeros(num_lines)\n    w_list = np.zeros(num_lines)\n    print(\"numpy allocated\")\n    node_ids = {}\n    unique_id = 0\n\n    with open(fname, \"r\") as csv_file:\n        csv_reader = csv.reader(csv_file, delimiter=\",\")\n        idx = 0\n        # time,src,dst,weight\n        # 1648811421,0x27cbb0e6885ccb1db2dab7c2314131c94795fbef,0x8426a27add8dca73548f012d92c7f8f4bbd42a3e,800.0\n        for row in tqdm(csv_reader):\n            if idx == 0:\n                idx += 1\n                continue\n            else:\n                ts = int(row[0])\n                src = row[1]\n                dst = row[2]\n\n                if src not in node_ids:\n                    node_ids[src] = unique_id\n                    unique_id += 1\n                if dst not in node_ids:\n                    node_ids[dst] = unique_id\n                    unique_id += 1\n\n                w = float(row[3])\n                if w == 0:\n                    w = 1\n\n                u = node_ids[src]\n                i = node_ids[dst]\n                u_list[idx - 1] = u\n                i_list[idx - 1] = i\n                ts_list[idx - 1] = ts\n                idx_list[idx - 1] = idx\n                w_list[idx - 1] = w\n                feat_l[idx - 1] = np.zeros(feat_size)\n                idx += 1\n\n    #! normalize by log 2 for stablecoin\n    w_list = np.log2(w_list)\n\n    return (\n        pd.DataFrame(\n            {\n                \"u\": u_list,\n                \"i\": i_list,\n                \"ts\": ts_list,\n                \"label\": label_list,\n                \"idx\": idx_list,\n                \"w\": w_list,\n            }\n        ),\n        feat_l,\n        node_ids,\n    )\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.csv_to_staticdata","title":"<code>csv_to_staticdata(fname, node_ids)</code>","text":"<p>used by tkgl-wikidata and tkgl-smallpedia convert the raw .csv data to pandas dataframe and numpy array for static knowledge edges input .csv file format should be: head, tail, relation type Args:     fname: the path to the raw data     node_ids: dictionary of node names mapped to integer node ids</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def csv_to_staticdata(\n    fname: str,\n    node_ids: dict,\n) -&gt; pd.DataFrame:\n    r\"\"\"\n    used by tkgl-wikidata and tkgl-smallpedia\n    convert the raw .csv data to pandas dataframe and numpy array for static knowledge edges\n    input .csv file format should be: head, tail, relation type\n    Args:\n        fname: the path to the raw data\n        node_ids: dictionary of node names mapped to integer node ids\n    \"\"\"\n    num_lines = sum(1 for line in open(fname)) - 1\n    print(\"number of lines counted\", num_lines)\n    u_list = np.zeros(num_lines)\n    i_list = np.zeros(num_lines)\n    edge_type = np.zeros(num_lines)\n    edge_type_ids = {}\n    out_dict = {}\n\n    with open(fname, \"r\") as csv_file:\n        csv_reader = csv.reader(csv_file, delimiter=\",\")\n        idx = 0\n        #timestamp, head, tail, relation type\n        for row in tqdm(csv_reader):\n            if idx == 0:\n                idx += 1\n                continue\n            else:\n                src = row[0]\n                dst = row[1]\n                relation = row[2]\n                if src not in node_ids:\n                    node_ids[src] = len(node_ids)\n                if dst not in node_ids:\n                    node_ids[dst] = len(node_ids)\n                if relation not in edge_type_ids:\n                    edge_type_ids[relation] = len(edge_type_ids)\n                u = node_ids[src]\n                i = node_ids[dst]\n                u_list[idx - 1] = u\n                i_list[idx - 1] = i                \n                edge_type[idx - 1] = edge_type_ids[relation]\n                idx += 1\n\n    out_dict[\"head\"] = u_list\n    out_dict[\"tail\"] = i_list\n    out_dict[\"edge_type\"] = edge_type\n    return out_dict, node_ids\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.csv_to_thg_data","title":"<code>csv_to_thg_data(fname)</code>","text":"<p>used by thgl-myket dataset convert the raw .csv data to pandas dataframe and numpy array input .csv file format should be: timestamp, head, tail, relation type Args:     fname: the path to the raw data</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def csv_to_thg_data(\n    fname: str,\n) -&gt; pd.DataFrame:\n    r\"\"\"\n    used by thgl-myket dataset\n    convert the raw .csv data to pandas dataframe and numpy array\n    input .csv file format should be: timestamp, head, tail, relation type\n    Args:\n        fname: the path to the raw data\n    \"\"\"\n    feat_size = 1\n    num_lines = sum(1 for line in open(fname)) - 1\n    print(\"number of lines counted\", num_lines)\n    u_list = np.zeros(num_lines)\n    i_list = np.zeros(num_lines)\n    ts_list = np.zeros(num_lines)\n    label_list = np.zeros(num_lines)\n    edge_type = np.zeros(num_lines)\n    feat_l = np.zeros((num_lines, feat_size))\n    idx_list = np.zeros(num_lines)\n    w_list = np.zeros(num_lines)\n    node_ids = {}\n    unique_id = 0\n\n    with open(fname, \"r\") as csv_file:\n        csv_reader = csv.reader(csv_file, delimiter=\",\")\n        idx = 0\n        #timestamp, head, tail, relation type\n        for row in tqdm(csv_reader):\n            if idx == 0:\n                idx += 1\n                continue\n            else:\n                ts = int(row[0]) #converted to UNIX timestamp already \n                src = int(row[1])\n                dst = int(row[2])\n                relation = int(row[3])\n                if src not in node_ids:\n                    node_ids[src] = unique_id\n                    unique_id += 1\n                if dst not in node_ids:\n                    node_ids[dst] = unique_id\n                    unique_id += 1\n                u = node_ids[src]\n                i = node_ids[dst]\n                u_list[idx - 1] = u\n                i_list[idx - 1] = i\n                ts_list[idx - 1] = ts\n                idx_list[idx - 1] = idx\n                w_list[idx - 1] = float(1)\n                edge_type[idx - 1] = relation\n                idx += 1\n    return (\n        pd.DataFrame(\n            {\n                \"u\": u_list,\n                \"i\": i_list,\n                \"ts\": ts_list,\n                \"label\": label_list,\n                \"idx\": idx_list,\n                \"w\": w_list,\n                \"edge_type\": edge_type,\n            }\n        ),\n        feat_l,\n        node_ids,\n    )\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.csv_to_tkg_data","title":"<code>csv_to_tkg_data(fname)</code>","text":"<p>used by tkgl-polecat convert the raw .csv data to pandas dataframe and numpy array input .csv file format should be: timestamp, head, tail, relation type Args:     fname: the path to the raw data</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def csv_to_tkg_data(\n    fname: str,\n) -&gt; pd.DataFrame:\n    r\"\"\"\n    used by tkgl-polecat\n    convert the raw .csv data to pandas dataframe and numpy array\n    input .csv file format should be: timestamp, head, tail, relation type\n    Args:\n        fname: the path to the raw data\n    \"\"\"\n    feat_size = 1\n    num_lines = sum(1 for line in open(fname)) - 1\n    print(\"number of lines counted\", num_lines)\n    u_list = np.zeros(num_lines)\n    i_list = np.zeros(num_lines)\n    ts_list = np.zeros(num_lines)\n    label_list = np.zeros(num_lines)\n    edge_type = np.zeros(num_lines)\n    feat_l = np.zeros((num_lines, feat_size))\n    idx_list = np.zeros(num_lines)\n    w_list = np.zeros(num_lines)\n    node_ids = {}\n    unique_id = 0\n\n    with open(fname, \"r\") as csv_file:\n        csv_reader = csv.reader(csv_file, delimiter=\",\")\n        idx = 0\n        #timestamp, head, tail, relation type\n        for row in tqdm(csv_reader):\n            if idx == 0:\n                idx += 1\n                continue\n            else:\n                ts = int(row[0]) #converted to UNIX timestamp already \n                src = int(row[1])\n                dst = int(row[2])\n                relation = int(row[3])\n                if src not in node_ids:\n                    node_ids[src] = unique_id\n                    unique_id += 1\n                if dst not in node_ids:\n                    node_ids[dst] = unique_id\n                    unique_id += 1\n                u = node_ids[src]\n                i = node_ids[dst]\n                u_list[idx - 1] = u\n                i_list[idx - 1] = i\n                ts_list[idx - 1] = ts\n                idx_list[idx - 1] = idx\n                w_list[idx - 1] = float(1)\n                edge_type[idx - 1] = relation\n                idx += 1\n    return (\n        pd.DataFrame(\n            {\n                \"u\": u_list,\n                \"i\": i_list,\n                \"ts\": ts_list,\n                \"label\": label_list,\n                \"idx\": idx_list,\n                \"w\": w_list,\n                \"edge_type\": edge_type,\n            }\n        ),\n        feat_l,\n        node_ids,\n    )\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.csv_to_wikidata","title":"<code>csv_to_wikidata(fname)</code>","text":"<p>used by tkgl-wikidata and tkgl-smallpedia convert the raw .csv data to pandas dataframe and numpy array input .csv file format should be: timestamp, head, tail, relation type Args:     fname: the path to the raw data</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def csv_to_wikidata(\n    fname: str,\n) -&gt; pd.DataFrame:\n    r\"\"\"\n    used by tkgl-wikidata and tkgl-smallpedia\n    convert the raw .csv data to pandas dataframe and numpy array\n    input .csv file format should be: timestamp, head, tail, relation type\n    Args:\n        fname: the path to the raw data\n    \"\"\"\n    feat_size = 1\n    num_lines = sum(1 for line in open(fname)) - 1\n    print(\"number of lines counted\", num_lines)\n    u_list = np.zeros(num_lines)\n    i_list = np.zeros(num_lines)\n    ts_list = np.zeros(num_lines)\n    label_list = np.zeros(num_lines)\n    edge_type = np.zeros(num_lines)\n    feat_l = np.zeros((num_lines, feat_size))\n    idx_list = np.zeros(num_lines)\n    w_list = np.zeros(num_lines)\n    node_ids = {}\n    edge_type_ids = {}\n    unique_id = 0\n    et_id = 0\n\n    with open(fname, \"r\") as csv_file:\n        csv_reader = csv.reader(csv_file, delimiter=\",\")\n        idx = 0\n        #timestamp, head, tail, relation type\n        for row in tqdm(csv_reader):\n            if idx == 0:\n                idx += 1\n                continue\n            else:\n                ts = int(row[0]) #converted to year already\n                src = row[1]\n                dst = row[2]\n                relation = row[3]\n                if src not in node_ids:\n                    node_ids[src] = unique_id\n                    unique_id += 1\n                if dst not in node_ids:\n                    node_ids[dst] = unique_id\n                    unique_id += 1\n                if relation not in edge_type_ids:\n                    edge_type_ids[relation] = et_id\n                    et_id += 1\n                u = node_ids[src]\n                i = node_ids[dst]\n                u_list[idx - 1] = u\n                i_list[idx - 1] = i\n                ts_list[idx - 1] = ts\n                idx_list[idx - 1] = idx\n                w_list[idx - 1] = float(1)\n                edge_type[idx - 1] = edge_type_ids[relation]\n                idx += 1\n    return (\n        pd.DataFrame(\n            {\n                \"u\": u_list,\n                \"i\": i_list,\n                \"ts\": ts_list,\n                \"label\": label_list,\n                \"idx\": idx_list,\n                \"w\": w_list,\n                \"edge_type\": edge_type,\n            }\n        ),\n        feat_l,\n        node_ids,\n    )\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.load_edgelist_datetime","title":"<code>load_edgelist_datetime(fname, label_size=514)</code>","text":"<p>load the edgelist into a pandas dataframe use numpy array instead of list for faster processing assume all edges are already sorted by time convert all time unit to unix time</p> <p>time, user_id, genre, weight</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def load_edgelist_datetime(fname, label_size=514):\n    \"\"\"\n    load the edgelist into a pandas dataframe\n    use numpy array instead of list for faster processing\n    assume all edges are already sorted by time\n    convert all time unit to unix time\n\n    time, user_id, genre, weight\n    \"\"\"\n    feat_size = 1\n    num_lines = sum(1 for line in open(fname)) - 1\n    print(\"number of lines counted\", num_lines)\n    u_list = np.zeros(num_lines)\n    i_list = np.zeros(num_lines)\n    ts_list = np.zeros(num_lines)\n    feat_l = np.zeros((num_lines, feat_size))\n    idx_list = np.zeros(num_lines)\n    w_list = np.zeros(num_lines)\n    #print(\"numpy allocated\")\n    node_ids = {}  # dictionary for node ids\n    label_ids = {}  # dictionary for label ids\n    node_uid = label_size  # node ids start after the genre nodes\n    label_uid = 0\n\n    with open(fname, \"r\") as csv_file:\n        csv_reader = csv.reader(csv_file, delimiter=\",\")\n        idx = 0\n        for row in tqdm(csv_reader):\n            if idx == 0:\n                idx += 1\n            else:\n                ts = int(row[0])\n                user_id = row[1]\n                genre = row[2]\n                w = float(row[3])\n\n                if user_id not in node_ids:\n                    node_ids[user_id] = node_uid\n                    node_uid += 1\n\n                if genre not in label_ids:\n                    label_ids[genre] = label_uid\n                    if label_uid &gt;= label_size:\n                        print(\"id overlap, terminate\")\n                    label_uid += 1\n\n                u = node_ids[user_id]\n                i = label_ids[genre]\n                u_list[idx - 1] = u\n                i_list[idx - 1] = i\n                ts_list[idx - 1] = ts\n                idx_list[idx - 1] = idx\n                w_list[idx - 1] = w\n                feat_l[idx - 1] = np.asarray([w])\n                idx += 1\n\n    return (\n        pd.DataFrame(\n            {\"u\": u_list, \"i\": i_list, \"ts\": ts_list, \"idx\": idx_list, \"w\": w_list}\n        ),\n        feat_l,\n        node_ids,\n        label_ids,\n    )\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.load_edgelist_sr","title":"<code>load_edgelist_sr(fname, label_size=2221)</code>","text":"<p>load the edgelist into pandas dataframe also outputs index for the user nodes and genre nodes Parameters:     fname: str, name of the input file     label_size: int, number of genres Returns:     df: a pandas dataframe containing the edgelist data</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def load_edgelist_sr(\n    fname: str,\n    label_size: int = 2221,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    load the edgelist into pandas dataframe\n    also outputs index for the user nodes and genre nodes\n    Parameters:\n        fname: str, name of the input file\n        label_size: int, number of genres\n    Returns:\n        df: a pandas dataframe containing the edgelist data\n    \"\"\"\n    feat_size = 1 #2\n    num_lines = sum(1 for line in open(fname)) - 1\n    #print(\"number of lines counted\", num_lines)\n    print(\"there are \", num_lines, \" lines in the raw data\")\n    u_list = np.zeros(num_lines)\n    i_list = np.zeros(num_lines)\n    ts_list = np.zeros(num_lines)\n    label_list = np.zeros(num_lines)\n    feat_l = np.zeros((num_lines, feat_size))\n    idx_list = np.zeros(num_lines)\n    w_list = np.zeros(num_lines)\n\n    node_ids = {}\n    rd_dict = {}\n    node_uid = label_size  # node ids start after all the genres\n    sr_uid = 0\n\n    with open(fname, \"r\") as csv_file:\n        csv_reader = csv.reader(csv_file, delimiter=\",\")\n        idx = 0\n        # ['ts', 'src', 'subreddit', 'num_words', 'score']\n        for row in tqdm(csv_reader):\n            if idx == 0:\n                idx += 1\n            else:\n                ts = row[0]\n                src = row[1]\n                subreddit = row[2]\n                #num_words = int(row[3])\n                score = int(row[4])\n                if src not in node_ids:\n                    node_ids[src] = node_uid\n                    node_uid += 1\n                if subreddit not in rd_dict:\n                    rd_dict[subreddit] = sr_uid\n                    sr_uid += 1\n                w = float(score)\n                u = node_ids[src]\n                i = rd_dict[subreddit]\n                u_list[idx - 1] = u\n                i_list[idx - 1] = i\n                ts_list[idx - 1] = ts\n                idx_list[idx - 1] = idx\n                w_list[idx - 1] = w\n                feat_l[idx - 1] = np.array([w])\n                idx += 1\n\n        return (\n            pd.DataFrame(\n                {\n                    \"u\": u_list,\n                    \"i\": i_list,\n                    \"ts\": ts_list,\n                    \"label\": label_list,\n                    \"idx\": idx_list,\n                    \"w\": w_list,\n                }\n            ),\n            feat_l,\n            node_ids,\n            rd_dict,\n        )\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.load_edgelist_token","title":"<code>load_edgelist_token(fname, label_size=1001)</code>","text":"<p>load the edgelist into pandas dataframe also outputs index for the user nodes and genre nodes Parameters:     fname: str, name of the input file     label_size: int, number of genres Returns:     df: a pandas dataframe containing the edgelist data</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def load_edgelist_token(\n    fname: str,\n    label_size: int = 1001,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    load the edgelist into pandas dataframe\n    also outputs index for the user nodes and genre nodes\n    Parameters:\n        fname: str, name of the input file\n        label_size: int, number of genres\n    Returns:\n        df: a pandas dataframe containing the edgelist data\n    \"\"\"\n    feat_size = 2\n    num_lines = sum(1 for line in open(fname)) - 1\n    #print(\"number of lines counted\", num_lines)\n    print(\"there are \", num_lines, \" lines in the raw data\")\n    u_list = np.zeros(num_lines)\n    i_list = np.zeros(num_lines)\n    ts_list = np.zeros(num_lines)\n    label_list = np.zeros(num_lines)\n    feat_l = np.zeros((num_lines, feat_size))\n    idx_list = np.zeros(num_lines)\n    w_list = np.zeros(num_lines)\n\n    node_ids = {}\n    rd_dict = {}\n    node_uid = label_size  # node ids start after all the genres\n    sr_uid = 0\n\n    with open(fname, \"r\") as csv_file:\n        csv_reader = csv.reader(csv_file, delimiter=\",\")\n        idx = 0\n        # [timestamp,user_address,token_address,value,IsSender]\n        for row in tqdm(csv_reader):\n            if idx == 0:\n                idx += 1\n            else:\n                ts = row[0]\n                src = row[1]\n                token = row[2]\n                w = float(row[3])\n                attr = float(row[4])\n                if src not in node_ids:\n                    node_ids[src] = node_uid\n                    node_uid += 1\n                if token not in rd_dict:\n                    rd_dict[token] = sr_uid\n                    sr_uid += 1\n                u = node_ids[src]\n                i = rd_dict[token]\n                u_list[idx - 1] = u\n                i_list[idx - 1] = i\n                ts_list[idx - 1] = ts\n                idx_list[idx - 1] = idx\n                w_list[idx - 1] = w\n                feat_l[idx - 1] = np.array([w,attr])\n                idx += 1\n\n        return (\n            pd.DataFrame(\n                {\n                    \"u\": u_list,\n                    \"i\": i_list,\n                    \"ts\": ts_list,\n                    \"label\": label_list,\n                    \"idx\": idx_list,\n                    \"w\": w_list,\n                }\n            ),\n            feat_l,\n            node_ids,\n            rd_dict,\n        )\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.load_edgelist_trade","title":"<code>load_edgelist_trade(fname, label_size=255)</code>","text":"<p>load the edgelist into pandas dataframe</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def load_edgelist_trade(fname: str, label_size=255):\n    \"\"\"\n    load the edgelist into pandas dataframe\n    \"\"\"\n    feat_size = 1\n    num_lines = sum(1 for line in open(fname)) - 1\n    print(\"number of lines counted\", num_lines)\n    u_list = np.zeros(num_lines)\n    i_list = np.zeros(num_lines)\n    ts_list = np.zeros(num_lines)\n    feat_l = np.zeros((num_lines, feat_size))\n    idx_list = np.zeros(num_lines)\n    w_list = np.zeros(num_lines)\n    #print(\"numpy allocated\")\n    node_ids = {}  # dictionary for node ids\n    node_uid = 0\n\n    with open(fname, \"r\") as csv_file:\n        csv_reader = csv.reader(csv_file, delimiter=\",\")\n        idx = 0\n        for row in tqdm(csv_reader):\n            if idx == 0:\n                idx += 1\n            else:\n                ts = int(row[0])\n                u = row[1]\n                v = row[2]\n                w = float(row[3])\n                if u not in node_ids:\n                    node_ids[u] = node_uid\n                    node_uid += 1\n\n                if v not in node_ids:\n                    node_ids[v] = node_uid\n                    node_uid += 1\n\n                u = node_ids[u]\n                i = node_ids[v]\n                u_list[idx - 1] = u\n                i_list[idx - 1] = i\n                ts_list[idx - 1] = ts\n                idx_list[idx - 1] = idx\n                w_list[idx - 1] = w\n                feat_l[idx - 1] = np.array([w])\n                idx += 1\n\n    return (\n        pd.DataFrame(\n            {\"u\": u_list, \"i\": i_list, \"ts\": ts_list, \"idx\": idx_list, \"w\": w_list}\n        ),\n        feat_l,\n        node_ids,\n    )\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.load_edgelist_wiki","title":"<code>load_edgelist_wiki(fname)</code>","text":"<p>loading wikipedia dataset into pandas dataframe similar processing to https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/datasets/jodie.html</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>str, name of the input file</p> required <p>Returns:     df: a pandas dataframe containing the edgelist data</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def load_edgelist_wiki(fname: str) -&gt; pd.DataFrame:\n    \"\"\"\n    loading wikipedia dataset into pandas dataframe\n    similar processing to\n    https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/datasets/jodie.html\n\n    Parameters:\n        fname: str, name of the input file\n    Returns:\n        df: a pandas dataframe containing the edgelist data\n    \"\"\"\n    df = pd.read_csv(fname, skiprows=1, header=None)\n    src = df.iloc[:, 0].values\n    dst = df.iloc[:, 1].values\n    dst += int(src.max()) + 1\n    t = df.iloc[:, 2].values\n    msg = df.iloc[:, 4:].values\n    idx = np.arange(t.shape[0])\n    w = np.ones(t.shape[0])\n\n    return pd.DataFrame({\"u\": src, \"i\": dst, \"ts\": t, \"idx\": idx, \"w\": w}), msg, None\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.load_genre_list","title":"<code>load_genre_list(fname)</code>","text":"<p>load the list of genres</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def load_genre_list(fname):\n    \"\"\"\n    load the list of genres\n    \"\"\"\n    if not osp.exists(fname):\n        raise FileNotFoundError(f\"File not found at {fname}\")\n\n    edgelist = open(fname, \"r\")\n    lines = list(edgelist.readlines())\n    edgelist.close()\n\n    genre_index = {}\n    ctr = 0\n    for i in range(1, len(lines)):\n        vals = lines[i].split(\",\")\n        genre = vals[0]\n        if genre not in genre_index:\n            genre_index[genre] = ctr\n            ctr += 1\n        else:\n            raise ValueError(\"duplicate in genre_index\")\n    return genre_index\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.load_label_dict","title":"<code>load_label_dict(fname, node_ids, rd_dict)</code>","text":"<p>load node labels into a nested dictionary instead of pandas dataobject {ts: {node_id: label_vec}} Parameters:     fname: str, name of the input file     node_ids: dictionary of user names mapped to integer node ids     rd_dict: dictionary of subreddit names mapped to integer node ids</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def load_label_dict(fname: str, node_ids: dict, rd_dict: dict) -&gt; dict:\n    \"\"\"\n    load node labels into a nested dictionary instead of pandas dataobject\n    {ts: {node_id: label_vec}}\n    Parameters:\n        fname: str, name of the input file\n        node_ids: dictionary of user names mapped to integer node ids\n        rd_dict: dictionary of subreddit names mapped to integer node ids\n    \"\"\"\n    if not osp.exists(fname):\n        raise FileNotFoundError(f\"File not found at {fname}\")\n\n    # day, user_idx, label_vec\n    label_size = len(rd_dict)\n    node_label_dict = {}  # {ts: {node_id: label_vec}}\n\n    with open(fname, \"r\") as csv_file:\n        csv_reader = csv.reader(csv_file, delimiter=\",\")\n        idx = 0\n        # ['ts', 'src', 'dst', 'w']\n        for row in tqdm(csv_reader):\n            if idx == 0:\n                idx += 1\n            else:\n                u = node_ids[row[1]]\n                ts = int(row[0])\n                v = int(rd_dict[row[2]])\n                weight = float(row[3])\n                if (ts not in node_label_dict):\n                    node_label_dict[ts] = {u:np.zeros(label_size)}\n\n                if (u not in node_label_dict[ts]):\n                    node_label_dict[ts][u] = np.zeros(label_size)\n\n                node_label_dict[ts][u][v] = weight\n                idx += 1\n        return node_label_dict\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.load_labels_sr","title":"<code>load_labels_sr(fname, node_ids, rd_dict)</code>","text":"<p>load the node labels for subreddit dataset</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def load_labels_sr(\n    fname,\n    node_ids,\n    rd_dict,\n):\n    \"\"\"\n    load the node labels for subreddit dataset\n    \"\"\"\n    if not osp.exists(fname):\n        raise FileNotFoundError(f\"File not found at {fname}\")\n\n    # day, user_idx, label_vec\n    label_size = len(rd_dict)\n    label_vec = np.zeros(label_size)\n    ts_prev = 0\n    prev_user = 0\n\n    ts_list = []\n    node_id_list = []\n    y_list = []\n\n    with open(fname, \"r\") as csv_file:\n        csv_reader = csv.reader(csv_file, delimiter=\",\")\n        idx = 0\n        # ['ts', 'src', 'subreddit', 'num_words', 'score']\n        for row in tqdm(csv_reader):\n            if idx == 0:\n                idx += 1\n            else:\n                user_id = node_ids[int(row[1])]\n                ts = int(row[0])\n                sr_id = int(rd_dict[row[2]])\n                weight = float(row[3])\n                if idx == 1:\n                    ts_prev = ts\n                    prev_user = user_id\n                # the next day\n                if ts != ts_prev:\n                    ts_list.append(ts_prev)\n                    node_id_list.append(prev_user)\n                    y_list.append(label_vec)\n                    label_vec = np.zeros(label_size)\n                    ts_prev = ts\n                    prev_user = user_id\n                else:\n                    label_vec[sr_id] = weight\n\n                if user_id != prev_user:\n                    ts_list.append(ts_prev)\n                    node_id_list.append(prev_user)\n                    y_list.append(label_vec)\n                    prev_user = user_id\n                    label_vec = np.zeros(label_size)\n                idx += 1\n        return pd.DataFrame({\"ts\": ts_list, \"node_id\": node_id_list, \"y\": y_list})\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.load_trade_label_dict","title":"<code>load_trade_label_dict(fname, node_ids)</code>","text":"<p>load node labels into a nested dictionary instead of pandas dataobject {ts: {node_id: label_vec}} Parameters:     fname: str, name of the input file     node_ids: dictionary of user names mapped to integer node ids Returns:     node_label_dict: a nested dictionary of node labels</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def load_trade_label_dict(\n    fname: str,\n    node_ids: dict,\n) -&gt; dict:\n    \"\"\"\n    load node labels into a nested dictionary instead of pandas dataobject\n    {ts: {node_id: label_vec}}\n    Parameters:\n        fname: str, name of the input file\n        node_ids: dictionary of user names mapped to integer node ids\n    Returns:\n        node_label_dict: a nested dictionary of node labels\n    \"\"\"\n    if not osp.exists(fname):\n        raise FileNotFoundError(f\"File not found at {fname}\")\n\n    label_size = len(node_ids)\n    #label_vec = np.zeros(label_size)\n\n    node_label_dict = {}  # {ts: {node_id: label_vec}}\n\n    with open(fname, \"r\") as csv_file:\n        csv_reader = csv.reader(csv_file, delimiter=\",\")\n        idx = 0\n        for row in tqdm(csv_reader):\n            if idx == 0:\n                idx += 1\n            else:\n                ts = int(row[0])\n                u = node_ids[row[1]]\n                v = node_ids[row[2]]\n                weight = float(row[3])\n\n                if (ts not in node_label_dict):\n                    node_label_dict[ts] = {u:np.zeros(label_size)}\n\n                if (u not in node_label_dict[ts]):\n                    node_label_dict[ts][u] = np.zeros(label_size)\n\n                node_label_dict[ts][u][v] = weight\n                idx += 1\n        return node_label_dict\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.process_node_feat","title":"<code>process_node_feat(fname, node_ids)</code>","text":"<ol> <li>need to have the same node id as csv_to_pd_data</li> <li>process the various node features into a vector</li> <li>return a numpy array of node features with index corresponding to node id</li> </ol> <p>airport_code,type,continent,iso_region,longitude,latitude type: onehot encoding continent: onehot encoding iso_region: alphabet encoding same as edge feat longitude: float divide by 180 latitude: float divide by 90</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def process_node_feat(\n    fname: str,\n    node_ids,\n):\n    \"\"\"\n    1. need to have the same node id as csv_to_pd_data\n    2. process the various node features into a vector\n    3. return a numpy array of node features with index corresponding to node id\n\n    airport_code,type,continent,iso_region,longitude,latitude\n    type: onehot encoding\n    continent: onehot encoding\n    iso_region: alphabet encoding same as edge feat\n    longitude: float divide by 180\n    latitude: float divide by 90\n    \"\"\"\n    feat_size = 20\n    node_feat = np.zeros((len(node_ids), feat_size))\n    type_dict = {}\n    type_idx = 0\n    continent_dict = {}\n    cont_idx = 0\n\n    with open(fname, \"r\") as csv_file:\n        csv_reader = csv.reader(csv_file, delimiter=\",\")\n        idx = 0\n        # airport_code,type,continent,iso_region,longitude,latitude\n        for row in tqdm(csv_reader):\n            if idx == 0:\n                idx += 1\n                continue\n            else:\n                code = row[0]\n                if code not in node_ids:\n                    continue\n                else:\n                    node_id = node_ids[code]\n                    airport_type = row[1]\n                    if airport_type not in type_dict:\n                        type_dict[airport_type] = type_idx\n                        type_idx += 1\n                    continent = row[2]\n                    if continent not in continent_dict:\n                        continent_dict[continent] = cont_idx\n                        cont_idx += 1\n\n    with open(fname, \"r\") as csv_file:\n        csv_reader = csv.reader(csv_file, delimiter=\",\")\n        idx = 0\n        # airport_code,type,continent,iso_region,longitude,latitude\n        for row in tqdm(csv_reader):\n            if idx == 0:\n                idx += 1\n                continue\n            else:\n                code = row[0]\n                if code not in node_ids:\n                    continue\n                else:\n                    node_id = node_ids[code]\n                    airport_type = type_dict[row[1]]\n                    type_vec = np.zeros(type_idx)\n                    type_vec[airport_type] = 1\n                    continent = continent_dict[row[2]]\n                    cont_vec = np.zeros(cont_idx)\n                    cont_vec[continent] = 1\n                    while len(row[3]) &lt; 7:\n                        row[3] += \"!\"\n                    iso_region = convert_str2int(row[3])  # numpy float array\n                    lng = float(row[4])\n                    lat = float(row[5])\n                    coor_vec = np.array([lng, lat])\n                    final = np.concatenate(\n                        (type_vec, cont_vec, iso_region, coor_vec), axis=0\n                    )\n                    node_feat[node_id] = final\n    return node_feat\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.process_node_type","title":"<code>process_node_type(fname, node_ids)</code>","text":"<ol> <li>process the node type into integer</li> <li>return a numpy array of node types with index corresponding to node id</li> </ol> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def process_node_type(\n    fname: str,\n    node_ids,\n):\n    \"\"\"\n    1. process the node type into integer\n    3. return a numpy array of node types with index corresponding to node id\n    \"\"\"\n    node_feat = np.zeros(len(node_ids))\n    with open(fname, \"r\") as csv_file:\n        csv_reader = csv.reader(csv_file, delimiter=\",\")\n        idx = 0\n        # node_id,type\n        for row in tqdm(csv_reader):\n            if idx == 0:\n                idx += 1\n                continue\n            else:\n                nid = int(row[0])\n                try:\n                    node_type = int(row[1])\n                except:\n                    raise ValueError(row[1], \" is not an integer thus can't be a node type for thg dataset\")\n                try:\n                    node_id = node_ids[nid]\n                except:\n                    raise ValueError(nid, \" is not a valid node id\")\n                node_feat[node_id] = node_type\n    return node_feat\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.pre_process.reindex","title":"<code>reindex(df, bipartite=False)</code>","text":"<p>reindex the nodes especially if the node ids are not integers Args:     df: the pandas dataframe containing the graph     bipartite: whether the graph is bipartite</p> Source code in <code>tgb/utils/pre_process.py</code> <pre><code>def reindex(\n    df: pd.DataFrame,\n    bipartite: Optional[bool] = False,\n):\n    r\"\"\"\n    reindex the nodes especially if the node ids are not integers\n    Args:\n        df: the pandas dataframe containing the graph\n        bipartite: whether the graph is bipartite\n    \"\"\"\n    new_df = df.copy()\n    if bipartite:\n        assert df.u.max() - df.u.min() + 1 == len(df.u.unique())\n        assert df.i.max() - df.i.min() + 1 == len(df.i.unique())\n\n        upper_u = df.u.max() + 1\n        new_i = df.i + upper_u\n\n        new_df.i = new_i\n        new_df.u += 1\n        new_df.i += 1\n        new_df.idx += 1\n    else:\n        new_df.u += 1\n        new_df.i += 1\n        new_df.idx += 1\n\n    return new_df\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.utils.add_inverse_quadruples","title":"<code>add_inverse_quadruples(df)</code>","text":"<p>adds the inverse relations required for the model to the dataframe</p> Source code in <code>tgb/utils/utils.py</code> <pre><code>def add_inverse_quadruples(df: pd.DataFrame) -&gt; pd.DataFrame:\n    r\"\"\"\n    adds the inverse relations required for the model to the dataframe\n    \"\"\"\n    if (\"edge_type\" not in df):\n        raise ValueError(\"edge_type is required to invert relation in TKG\")\n\n    sources = np.array(df[\"u\"])\n    destinations = np.array(df[\"i\"])\n    timestamps = np.array(df[\"ts\"])\n    edge_idxs = np.array(df[\"idx\"])\n    weights = np.array(df[\"w\"])\n    edge_type = np.array(df[\"edge_type\"])\n\n    num_rels = np.unique(edge_type).shape[0]\n    inv_edge_type = edge_type + num_rels\n\n    all_sources = np.concatenate([sources, destinations])\n    all_destinations = np.concatenate([destinations, sources])\n    all_timestamps = np.concatenate([timestamps, timestamps])\n    all_edge_idxs = np.concatenate([edge_idxs, edge_idxs+edge_idxs.max()+1])\n    all_weights = np.concatenate([weights, weights])\n    all_edge_types = np.concatenate([edge_type, inv_edge_type])\n\n    return pd.DataFrame(\n            {\n                \"u\": all_sources,\n                \"i\": all_destinations,\n                \"ts\": all_timestamps,\n                \"label\": np.ones(all_timestamps.shape[0]),\n                \"idx\": all_edge_idxs,\n                \"w\": all_weights,\n                \"edge_type\": all_edge_types,\n            }\n        )\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.utils.add_inverse_quadruples_np","title":"<code>add_inverse_quadruples_np(quadruples, num_rels)</code>","text":"<p>creates an inverse quadruple for each quadruple in quadruples. inverse quadruple swaps subject and objsect, and increases  relation id by num_rels :param quadruples: [np.array] dataset quadruples, [src, relation_id, dst, timestamp ] :param num_rels: [int] number of relations that we have originally returns all_quadruples: [np.array] quadruples including inverse quadruples</p> Source code in <code>tgb/utils/utils.py</code> <pre><code>def add_inverse_quadruples_np(quadruples: np.array, \n                              num_rels:int) -&gt; np.array:\n    \"\"\"\n    creates an inverse quadruple for each quadruple in quadruples. inverse quadruple swaps subject and objsect, and increases \n    relation id by num_rels\n    :param quadruples: [np.array] dataset quadruples, [src, relation_id, dst, timestamp ]\n    :param num_rels: [int] number of relations that we have originally\n    returns all_quadruples: [np.array] quadruples including inverse quadruples\n    \"\"\"\n    inverse_quadruples = quadruples[:, [2, 1, 0, 3]]\n    inverse_quadruples[:, 1] = inverse_quadruples[:, 1] + num_rels  # we also need inverse quadruples\n    all_quadruples = np.concatenate((quadruples[:,0:4], inverse_quadruples))\n    return all_quadruples\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.utils.add_inverse_quadruples_pyg","title":"<code>add_inverse_quadruples_pyg(data, num_rels=-1)</code>","text":"<p>creates an inverse quadruple from PyG TemporalData object, returns both the original and inverse quadruples</p> Source code in <code>tgb/utils/utils.py</code> <pre><code>def add_inverse_quadruples_pyg(data: TemporalData, num_rels:int=-1) -&gt; list:\n    r\"\"\"\n    creates an inverse quadruple from PyG TemporalData object, returns both the original and inverse quadruples\n    \"\"\"\n    timestamp = data.t\n    head = data.src\n    tail = data.dst\n    msg = data.msg\n    edge_type = data.edge_type #relation\n    num_rels = torch.max(edge_type).item() + 1\n    inv_type = edge_type + num_rels\n    all_data = TemporalData(src=torch.cat([head, tail]), \n                            dst=torch.cat([tail, head]), \n                            t=torch.cat([timestamp, timestamp.clone()]), \n                            edge_type=torch.cat([edge_type, inv_type]), \n                            msg=torch.cat([msg, msg.clone()]),\n                            y = torch.cat([data.y, data.y.clone()]),)\n    return all_data\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.utils.load_pkl","title":"<code>load_pkl(fname)</code>","text":"<p>load a python object from a pickle file</p> Source code in <code>tgb/utils/utils.py</code> <pre><code>def load_pkl(fname: str) -&gt; Any:\n    r\"\"\"\n    load a python object from a pickle file\n    \"\"\"\n    with open(fname, \"rb\") as handle:\n        return pickle.load(handle)\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.utils.save_pkl","title":"<code>save_pkl(obj, fname)</code>","text":"<p>save a python object as a pickle file</p> Source code in <code>tgb/utils/utils.py</code> <pre><code>def save_pkl(obj: Any, fname: str) -&gt; None:\n    r\"\"\"\n    save a python object as a pickle file\n    \"\"\"\n    with open(fname, \"wb\") as handle:\n        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.utils.save_results","title":"<code>save_results(new_results, filename)</code>","text":"<p>save (new) results into a json file :param: new_results (dictionary): a dictionary of new results to be saved :filename: the name of the file to save the (new) results</p> Source code in <code>tgb/utils/utils.py</code> <pre><code>def save_results(new_results: dict, filename: str):\n    r\"\"\"\n    save (new) results into a json file\n    :param: new_results (dictionary): a dictionary of new results to be saved\n    :filename: the name of the file to save the (new) results\n    \"\"\"\n    if os.path.isfile(filename):\n        # append to the file\n        with open(filename, 'r+') as json_file:\n            file_data = json.load(json_file)\n            # convert file_data to list if not\n            if type(file_data) is dict:\n                file_data = [file_data]\n            file_data.append(new_results)\n            json_file.seek(0)\n            json.dump(file_data, json_file, indent=4)\n    else:\n        # dump the results\n        with open(filename, 'w') as json_file:\n            json.dump(new_results, json_file, indent=4)\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.utils.set_random_seed","title":"<code>set_random_seed(random_seed)</code>","text":"<p>set random seed for reproducibility Args:     random_seed (int): random seed</p> Source code in <code>tgb/utils/utils.py</code> <pre><code>def set_random_seed(random_seed: int):\n    r\"\"\"\n    set random seed for reproducibility\n    Args:\n        random_seed (int): random seed\n    \"\"\"\n    random.seed(random_seed)\n    np.random.seed(random_seed)\n    torch.manual_seed(random_seed)\n    torch.cuda.manual_seed(random_seed)\n    torch.cuda.manual_seed_all(random_seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n    print(f'INFO: fixed random seed: {random_seed}')\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.utils.split_by_time","title":"<code>split_by_time(data)</code>","text":"<p>https://github.com/Lee-zix/CEN/blob/main/rgcn/utils.py create list where each entry has an entry with all triples for this timestep</p> Source code in <code>tgb/utils/utils.py</code> <pre><code>def split_by_time(data):\n    \"\"\"\n    https://github.com/Lee-zix/CEN/blob/main/rgcn/utils.py\n    create list where each entry has an entry with all triples for this timestep\n    \"\"\"\n    timesteps = list(set(data[:,3]))\n    timesteps.sort()\n    snapshot_list = [None] * len(timesteps)\n\n    for index, ts in enumerate(timesteps):\n        mask = np.where(data[:, 3] == ts)[0]\n        snapshot_list[index] = data[mask,:3]\n\n    return snapshot_list\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.info.BColors","title":"<code>BColors</code>","text":"<p>A class to change the colors of the strings.</p> Source code in <code>tgb/utils/info.py</code> <pre><code>class BColors:\n    \"\"\"\n    A class to change the colors of the strings.\n    \"\"\"\n\n    HEADER = \"\\033[95m\"\n    OKBLUE = \"\\033[94m\"\n    OKCYAN = \"\\033[96m\"\n    OKGREEN = \"\\033[92m\"\n    WARNING = \"\\033[93m\"\n    FAIL = \"\\033[91m\"\n    ENDC = \"\\033[0m\"\n    BOLD = \"\\033[1m\"\n    UNDERLINE = \"\\033[4m\"\n</code></pre>"},{"location":"api/tgb.utils/#tgb.utils.stats.plot_curve","title":"<code>plot_curve(y, outname)</code>","text":"<p>plot the training curve given y Parameters:     y: np.ndarray, the training curve     outname: str, the output name</p> Source code in <code>tgb/utils/stats.py</code> <pre><code>def plot_curve(y: np.ndarray, outname: str) -&gt; None:\n    \"\"\"\n    plot the training curve given y\n    Parameters:\n        y: np.ndarray, the training curve\n        outname: str, the output name\n    \"\"\"\n    plt.plot(y, color=\"#fc4e2a\")\n    plt.savefig(outname + \".pdf\")\n    plt.close()\n</code></pre>"},{"location":"tutorials/Edge_data_numpy/","title":"Access edge data as numpy arrays","text":"In\u00a0[1]: Copied! <pre>from tgb.linkproppred.dataset import LinkPropPredDataset\n</pre> from tgb.linkproppred.dataset import LinkPropPredDataset <p>specifying the name of the dataset</p> In\u00a0[2]: Copied! <pre>name = \"tgbl-wiki\"\n</pre> name = \"tgbl-wiki\"  In\u00a0[3]: Copied! <pre>dataset = LinkPropPredDataset(name=name, root=\"datasets\", preprocess=True)\ntype(dataset)\n</pre> dataset = LinkPropPredDataset(name=name, root=\"datasets\", preprocess=True) type(dataset) <pre>Will you download the dataset(s) now? (y/N)\ny\nDownload started, this might take a while . . . \nDataset title: tgbl-wiki\nDownload completed \nDataset directory is  /mnt/f/code/TGB/tgb/datasets/tgbl_wiki\nfile not processed, generating processed file\n</pre> Out[3]: <pre>tgb.linkproppred.dataset.LinkPropPredDataset</pre> In\u00a0[4]: Copied! <pre>data = dataset.full_data  #a dictionary stores all the edge data\ntype(data)\n</pre> data = dataset.full_data  #a dictionary stores all the edge data type(data)  Out[4]: <pre>dict</pre> In\u00a0[5]: Copied! <pre>type(data['sources'])\ntype(data['destinations'])\ntype(data['timestamps'])\ntype(data['edge_feat'])\ntype(data['w'])\ntype(data['edge_label']) #just all one array as all edges in the dataset are positive edges\ntype(data['edge_idxs']) #just index of the edges increment by 1 for each edge\n</pre> type(data['sources']) type(data['destinations']) type(data['timestamps']) type(data['edge_feat']) type(data['w']) type(data['edge_label']) #just all one array as all edges in the dataset are positive edges type(data['edge_idxs']) #just index of the edges increment by 1 for each edge Out[5]: <pre>numpy.ndarray</pre> In\u00a0[6]: Copied! <pre>train_mask = dataset.train_mask\nval_mask = dataset.val_mask\ntest_mask = dataset.test_mask\n\ntype(train_mask)\ntype(val_mask)\ntype(test_mask)\n</pre> train_mask = dataset.train_mask val_mask = dataset.val_mask test_mask = dataset.test_mask  type(train_mask) type(val_mask) type(test_mask) Out[6]: <pre>numpy.ndarray</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/Edge_data_numpy/#access-edge-data-as-numpy-arrays","title":"Access edge data as numpy arrays\u00b6","text":"<p>This tutorial will show you how to access various datasets and their corresponding edgelists in <code>tgb</code></p> <p>You can directly retrieve the edge data as <code>numpy</code> arrays, <code>PyG</code> and <code>Pytorch</code> dependencies are not necessary</p> <p>The logic is implemented in <code>dataset.py</code> under <code>tgb/linkproppred/</code> and <code>tgb/nodeproppred/</code> folders respectively</p>"},{"location":"tutorials/Edge_data_numpy/#process-and-loading-the-dataset","title":"process and loading the dataset\u00b6","text":"<p>if the dataset has been processed, it will be loaded from disc for fast access</p> <p>if the dataset has not been downloaded, it will be processed automatically</p>"},{"location":"tutorials/Edge_data_numpy/#accessing-the-edge-data","title":"Accessing the edge data\u00b6","text":"<p>the edge data can be easily accessed via the property of the method as <code>numpy</code> arrays</p>"},{"location":"tutorials/Edge_data_numpy/#accessing-the-train-test-val-split","title":"Accessing the train, test, val split\u00b6","text":"<p>the masks for training, validation, and test split can be accessed directly from the <code>dataset</code> as well</p>"},{"location":"tutorials/Edge_data_pyg/","title":"Access edge data in Pytorch Geometric","text":"In\u00a0[1]: Copied! <pre>from tgb.linkproppred.dataset_pyg import PyGLinkPropPredDataset\n</pre> from tgb.linkproppred.dataset_pyg import PyGLinkPropPredDataset <p>specifying the name of the dataset</p> In\u00a0[2]: Copied! <pre>name = \"tgbl-wiki\"\n</pre> name = \"tgbl-wiki\" In\u00a0[3]: Copied! <pre>dataset = PyGLinkPropPredDataset(name=name, root=\"datasets\")\ntype(dataset)\n</pre> dataset = PyGLinkPropPredDataset(name=name, root=\"datasets\") type(dataset) <pre>file found, skipping download\nDataset directory is  /mnt/f/code/TGB/tgb/datasets/tgbl_wiki\nloading processed file\n</pre> Out[3]: <pre>tgb.linkproppred.dataset_pyg.PyGLinkPropPredDataset</pre> In\u00a0[4]: Copied! <pre>data = dataset.get_TemporalData()\ntype(data)\n</pre> data = dataset.get_TemporalData() type(data) Out[4]: <pre>torch_geometric.data.temporal.TemporalData</pre> In\u00a0[5]: Copied! <pre>type(data.src)\ntype(data.dst)\ntype(data.t)\ntype(data.msg)\n</pre> type(data.src) type(data.dst) type(data.t) type(data.msg) Out[5]: <pre>torch.Tensor</pre> In\u00a0[6]: Copied! <pre>type(dataset.src)  #same as src from above\ntype(dataset.dst)  #same as dst\ntype(dataset.ts)  #same as t\ntype(dataset.edge_feat) #same as msg\ntype(dataset.edge_label) #same as label used in tgn\n</pre> type(dataset.src)  #same as src from above type(dataset.dst)  #same as dst type(dataset.ts)  #same as t type(dataset.edge_feat) #same as msg type(dataset.edge_label) #same as label used in tgn Out[6]: <pre>torch.Tensor</pre> In\u00a0[7]: Copied! <pre>train_mask = dataset.train_mask\nval_mask = dataset.val_mask\ntest_mask = dataset.test_mask\n\ntype(train_mask)\ntype(val_mask)\ntype(test_mask)\n</pre> train_mask = dataset.train_mask val_mask = dataset.val_mask test_mask = dataset.test_mask  type(train_mask) type(val_mask) type(test_mask) Out[7]: <pre>torch.Tensor</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/Edge_data_pyg/#access-edge-data-in-pytorch-geometric","title":"Access edge data in Pytorch Geometric\u00b6","text":"<p>This tutorial will show you how to access various datasets and their corresponding edgelists in <code>tgb</code></p> <p>The logic for PyG data is stored in <code>dataset_pyg.py</code> in <code>tgb/linkproppred</code> and <code>tgb/nodeproppred</code> folders</p> <p>This tutorial requires <code>Pytorch</code> and <code>PyG</code>, refer to <code>README.md</code> for installation instructions</p>"},{"location":"tutorials/Edge_data_pyg/#process-and-load-the-dataset","title":"Process and load the dataset\u00b6","text":"<p>if the dataset has been processed, it will be loaded from disc for fast access</p> <p>if the dataset has not been downloaded, it will be processed automatically</p>"},{"location":"tutorials/Edge_data_pyg/#access-edge-data-from-temporaldata-object","title":"Access edge data from TemporalData object\u00b6","text":"<p>You can retrieve <code>torch_geometric.data.temporal.TemporalData</code> directly from <code>PyGLinkPropPredDataset</code></p>"},{"location":"tutorials/Edge_data_pyg/#directly-access-edge-data-as-pytorch-tensors","title":"Directly access edge data as Pytorch tensors\u00b6","text":"<p>the edge data can be easily accessed via the property of the method, these are converted into pytorch tensors (from <code>PyGLinkPropPredDataset</code>)</p>"},{"location":"tutorials/Edge_data_pyg/#accessing-the-train-test-val-split","title":"Accessing the train, test, val split\u00b6","text":"<p>the masks for training, validation, and test split can be accessed directly from the <code>dataset</code> as well</p>"},{"location":"tutorials/Node_label_tutorial/","title":"Access node labels for Dynamic Node Property Prediction","text":"In\u00a0[1]: Copied! <pre>from tgb.nodeproppred.dataset_pyg import PyGNodePropPredDataset\nfrom torch_geometric.loader import TemporalDataLoader\n</pre> from tgb.nodeproppred.dataset_pyg import PyGNodePropPredDataset from torch_geometric.loader import TemporalDataLoader <p>specifying the name of the dataset</p> In\u00a0[2]: Copied! <pre>name = \"tgbn-genre\"\n</pre> name = \"tgbn-genre\" In\u00a0[3]: Copied! <pre>dataset = PyGNodePropPredDataset(name=name, root=\"datasets\")\ntype(dataset)\n</pre> dataset = PyGNodePropPredDataset(name=name, root=\"datasets\") type(dataset) <pre>file found, skipping download\nDataset directory is  /mnt/f/code/TGB/tgb/datasets/tgbn_genre\nloading processed file\n</pre> Out[3]: <pre>tgb.nodeproppred.dataset_pyg.PyGNodePropPredDataset</pre> In\u00a0[4]: Copied! <pre>train_mask = dataset.train_mask\nval_mask = dataset.val_mask\ntest_mask = dataset.test_mask\n\n\ndata = dataset.get_TemporalData()\n\ntrain_data = data[train_mask]\nval_data = data[val_mask]\ntest_data = data[test_mask]\n\nbatch_size = 200\ntrain_loader = TemporalDataLoader(train_data, batch_size=batch_size)\nval_loader = TemporalDataLoader(val_data, batch_size=batch_size)\ntest_loader = TemporalDataLoader(test_data, batch_size=batch_size)\n</pre> train_mask = dataset.train_mask val_mask = dataset.val_mask test_mask = dataset.test_mask   data = dataset.get_TemporalData()  train_data = data[train_mask] val_data = data[val_mask] test_data = data[test_mask]  batch_size = 200 train_loader = TemporalDataLoader(train_data, batch_size=batch_size) val_loader = TemporalDataLoader(val_data, batch_size=batch_size) test_loader = TemporalDataLoader(test_data, batch_size=batch_size)  In\u00a0[5]: Copied! <pre>#query the timestamps for the first node labels\nlabel_t = dataset.get_label_time()\n\nfor batch in train_loader:\n    #access the edges in this batch\n    src, dst, t, msg = batch.src, batch.dst, batch.t, batch.msg\n    query_t = batch.t[-1]\n    # check if this batch moves to the next day\n    if query_t &gt; label_t:\n        # find the node labels from the past day\n        label_tuple = dataset.get_node_label(query_t)\n        # node labels are structured as a tuple with (timestamps, source node, label) format, label is a vector\n        label_ts, label_srcs, labels = (\n            label_tuple[0],\n            label_tuple[1],\n            label_tuple[2],\n        )\n        label_t = dataset.get_label_time()\n\n        #insert your code for backproping with node labels here\n</pre> #query the timestamps for the first node labels label_t = dataset.get_label_time()  for batch in train_loader:     #access the edges in this batch     src, dst, t, msg = batch.src, batch.dst, batch.t, batch.msg     query_t = batch.t[-1]     # check if this batch moves to the next day     if query_t &gt; label_t:         # find the node labels from the past day         label_tuple = dataset.get_node_label(query_t)         # node labels are structured as a tuple with (timestamps, source node, label) format, label is a vector         label_ts, label_srcs, labels = (             label_tuple[0],             label_tuple[1],             label_tuple[2],         )         label_t = dataset.get_label_time()          #insert your code for backproping with node labels here"},{"location":"tutorials/Node_label_tutorial/#access-node-labels-for-dynamic-node-property-prediction","title":"Access node labels for Dynamic Node Property Prediction\u00b6","text":"<p>This tutorial will show you how to access node labels and edge data for the node property prediction datasets in <code>tgb</code>.</p> <p>The source code is stored in <code>dataset_pyg.py</code> in <code>tgb/nodeproppred</code> folder</p> <p>This tutorial requires <code>Pytorch</code> and <code>PyG</code>, refer to <code>README.md</code> for installation instructions</p> <p>This tutorial uses <code>PyG TemporalData</code> object, however it is possible to use <code>numpy</code> arrays as well.</p> <p>see examples in <code>examples/nodeproppred</code> folder for more details.</p>"},{"location":"tutorials/Node_label_tutorial/#process-and-load-the-dataset","title":"Process and load the dataset\u00b6","text":"<p>if the dataset has been processed, it will be loaded from disc for fast access</p> <p>if the dataset has not been downloaded, it will be processed automatically</p>"},{"location":"tutorials/Node_label_tutorial/#train-validation-and-test-splits-with-dataloaders","title":"Train, Validation and Test splits with dataloaders\u00b6","text":"<p>splitting the edges into train, val, test sets and construct dataloader for each</p>"},{"location":"tutorials/Node_label_tutorial/#access-node-label-data","title":"Access node label data\u00b6","text":"<p>In <code>tgb</code>, the node label data are queried based on the nearest edge observed so far and retrieves the node label data for the corresponding day.</p> <p>Note that this is because the node labels often have different timestamps from the edges thus should be processed at the correct time in the edge stream.</p> <p>In the example below, we show how to iterate through the edges and retrieve the node labels of the corresponding time.</p>"}]}